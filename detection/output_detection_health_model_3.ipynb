{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833e5d48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:14.525704Z",
     "iopub.status.busy": "2023-12-13T07:53:14.525704Z",
     "iopub.status.idle": "2023-12-13T07:53:19.485454Z",
     "shell.execute_reply": "2023-12-13T07:53:19.485454Z"
    },
    "papermill": {
     "duration": 4.964765,
     "end_time": "2023-12-13T07:53:19.486468",
     "exception": false,
     "start_time": "2023-12-13T07:53:14.521703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.26.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\user\\anaconda3\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\user\\anaconda3\\lib\\site-packages (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: datetime in c:\\users\\user\\anaconda3\\lib\\site-packages (5.3)\n",
      "Requirement already satisfied: imblearn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\anaconda3\\lib\\site-packages (0.1.99)\n",
      "Requirement already satisfied: papermill in c:\\users\\user\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: protobuf in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.25.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (6.25.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (8.16.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (5.10.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\user\\anaconda3\\lib\\site-packages (from datetime) (5.4.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from imblearn) (0.11.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (8.1.7)\n",
      "Requirement already satisfied: nbformat>=5.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (5.9.2)\n",
      "Requirement already satisfied: nbclient>=0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (0.8.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (0.4)\n",
      "Requirement already satisfied: tenacity>=5.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (8.2.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (8.3.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.8)\n",
      "Requirement already satisfied: pyzmq>=20 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\user\\anaconda3\\lib\\site-packages (from nbformat>=5.1.2->papermill) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nbformat>=5.1.2->papermill) (4.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from zope.interface->datetime) (68.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (306)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy transformers pandas torch scikit-learn pyarrow accelerate transformers[torch] transformers[sentencepiece] ipywidgets tqdm datetime imblearn sentencepiece papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bb9966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:19.493468Z",
     "iopub.status.busy": "2023-12-13T07:53:19.493468Z",
     "iopub.status.idle": "2023-12-13T07:53:22.335951Z",
     "shell.execute_reply": "2023-12-13T07:53:22.335951Z"
    },
    "papermill": {
     "duration": 2.848491,
     "end_time": "2023-12-13T07:53:22.337959",
     "exception": false,
     "start_time": "2023-12-13T07:53:19.489468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification ,AlbertTokenizer, AlbertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8be3134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:22.344959Z",
     "iopub.status.busy": "2023-12-13T07:53:22.343960Z",
     "iopub.status.idle": "2023-12-13T07:53:22.347643Z",
     "shell.execute_reply": "2023-12-13T07:53:22.347643Z"
    },
    "papermill": {
     "duration": 0.007701,
     "end_time": "2023-12-13T07:53:22.348658",
     "exception": false,
     "start_time": "2023-12-13T07:53:22.340957",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Default parameters, Papermill will overwrite these\n",
    "categories = 'science'\n",
    "select_model = 0\n",
    "freeze_layers_up_to = 0\n",
    "weight_for_class_0 = 0.45\n",
    "weight_for_class_1 = 0.55\n",
    "learning_rate = 0.00001\n",
    "min_acc = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "566a0aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:22.354658Z",
     "iopub.status.busy": "2023-12-13T07:53:22.354658Z",
     "iopub.status.idle": "2023-12-13T07:53:22.357743Z",
     "shell.execute_reply": "2023-12-13T07:53:22.357743Z"
    },
    "papermill": {
     "duration": 0.008079,
     "end_time": "2023-12-13T07:53:22.358759",
     "exception": false,
     "start_time": "2023-12-13T07:53:22.350680",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "freeze_layers_up_to = 0\n",
    "weight_for_class_0 = 0.45\n",
    "weight_for_class_1 = 0.55\n",
    "learning_rate = 1e-05\n",
    "min_acc = 0.8\n",
    "select_model = 3\n",
    "categories = \"health\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9636e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:22.364757Z",
     "iopub.status.busy": "2023-12-13T07:53:22.364757Z",
     "iopub.status.idle": "2023-12-13T07:53:22.367757Z",
     "shell.execute_reply": "2023-12-13T07:53:22.367757Z"
    },
    "papermill": {
     "duration": 0.008003,
     "end_time": "2023-12-13T07:53:22.368762",
     "exception": false,
     "start_time": "2023-12-13T07:53:22.360759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_random_string(length=10):\n",
    "    letters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "run_id = generate_random_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b733ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:22.375762Z",
     "iopub.status.busy": "2023-12-13T07:53:22.375762Z",
     "iopub.status.idle": "2023-12-13T07:53:22.379439Z",
     "shell.execute_reply": "2023-12-13T07:53:22.379439Z"
    },
    "papermill": {
     "duration": 0.008662,
     "end_time": "2023-12-13T07:53:22.380444",
     "exception": false,
     "start_time": "2023-12-13T07:53:22.371782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir_mapping = {\n",
    "  'crime': './results/crime',\n",
    "  'science': './results/science',\n",
    "  'health': './results/health',\n",
    "  'politics': './results/politics',\n",
    "  'social_media': './results/social_media'\n",
    "}\n",
    "\n",
    "log_dir = log_dir_mapping.get(categories, './results')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "log_filename = f'training_log_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "log_filepath = os.path.join(log_dir, log_filename)\n",
    "\n",
    "logging.basicConfig(filename=log_filepath, filemode='w', format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logging.info(f'\\nStarting detection model - {run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1bcc1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:22.387445Z",
     "iopub.status.busy": "2023-12-13T07:53:22.386444Z",
     "iopub.status.idle": "2023-12-13T07:53:22.445616Z",
     "shell.execute_reply": "2023-12-13T07:53:22.445616Z"
    },
    "papermill": {
     "duration": 0.063178,
     "end_time": "2023-12-13T07:53:22.446621",
     "exception": false,
     "start_time": "2023-12-13T07:53:22.383443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label  \\\n",
      "0     circulating on social networks video that show...      1   \n",
      "1     internet sensation and the world cutest baby a...      1   \n",
      "2     a video has been viewed hundreds of thousands ...      1   \n",
      "3     a facebook post claiming that the national tre...      1   \n",
      "4     moves on facebook and twitter video showing po...      1   \n",
      "...                                                 ...    ...   \n",
      "5935  Quarantine advised for 80 persons across the S...      0   \n",
      "5936  South Sudan has begun screening all travelers ...      0   \n",
      "5937  Coronavirus (2019-nCoV) is a new respiratory i...      0   \n",
      "5938  Characterizing Patients Hospitalized With COVI...      1   \n",
      "5939  Just something to be prepared for… but then it...      1   \n",
      "\n",
      "                                                  title  \\\n",
      "0      A video shows a fortune teller predicting the...   \n",
      "1     Internet sensation and the worldâ€™s cutest ba...   \n",
      "2     A video has been viewed hundreds of thousands ...   \n",
      "3     Treasury is depositing Kshs 45, 000 to the mob...   \n",
      "4     Hunagrian authorities are capturing men 50 or ...   \n",
      "...                                                 ...   \n",
      "5935              2019-nCoV: Health dept. on full alert   \n",
      "5936  Screening machine for corona virus launched at...   \n",
      "5937                            Coronavirus (2019-nCoV)   \n",
      "5938  Characterizing Patients Hospitalized With COVI...   \n",
      "5939                               Corona Virus WARNING   \n",
      "\n",
      "                                               metadata  \n",
      "0     {'URL': 'https://observador.pt/factchecks/fact...  \n",
      "1     {'URL': 'https://www.newschecker.in/article/ne...  \n",
      "2     {'URL': 'https://factcheck.afp.com/video-shows...  \n",
      "3     {'URL': 'https://pesacheck.org/false-treasury-...  \n",
      "4     {'URL': 'https://www.animalpolitico.com/elsabu...  \n",
      "...                                                 ...  \n",
      "5935                {'label': 1, 'subcategory': 'true'}  \n",
      "5936                {'label': 1, 'subcategory': 'true'}  \n",
      "5937                {'label': 1, 'subcategory': 'true'}  \n",
      "5938          {'label': 0, 'subcategory': 'false news'}  \n",
      "5939          {'label': 0, 'subcategory': 'false news'}  \n",
      "\n",
      "[5932 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "root_dir = '../data'\n",
    "\n",
    "def load_data_from_category(category):\n",
    "    files = os.listdir(os.path.join(root_dir, category))\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        if file.endswith('.feather'):\n",
    "            df = pd.read_feather(os.path.join(root_dir, category, file))\n",
    "            dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Use this one when running multiple categories\n",
    "#combined_dataframes = [load_data_from_category(category) for category in categories] \n",
    "# This one is for single category\n",
    "combined_dataframes = [load_data_from_category(categories)]\n",
    "combined_df = pd.concat(combined_dataframes, ignore_index=True)\n",
    "combined_df.dropna(inplace=True)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925832a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:22.454620Z",
     "iopub.status.busy": "2023-12-13T07:53:22.453620Z",
     "iopub.status.idle": "2023-12-13T07:53:22.459639Z",
     "shell.execute_reply": "2023-12-13T07:53:22.459639Z"
    },
    "papermill": {
     "duration": 0.010032,
     "end_time": "2023-12-13T07:53:22.460653",
     "exception": false,
     "start_time": "2023-12-13T07:53:22.450621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_data_from_category(category, filenames):\\n    dataframes = []\\n    for file_name in filenames:\\n        file_path = os.path.join(root_dir, category, file_name)\\n        if os.path.exists(file_path):\\n            df = pd.read_feather(file_path)\\n            dataframes.append(df)\\n        else:\\n            print(f\"The file \\'{file_name}\\' in the \\'{category}\\' category does not exist.\")\\n    return pd.concat(dataframes, ignore_index=True)\\n\\nroot_dir = \\'../data\\'\\ncategory = \\'health\\'\\n\\n# List of filenames to include in combined_df\\n#included_filenames = [\\'isot_dataset.feather\\', \\'fake_news_dataset.feather\\', \\'pheme.feather\\', \\'liar_dataset.feather\\', \\'politifact_dataset.feather\\']\\nincluded_filenames = [\\'covid_claims.feather\\', \\'covid_fake_news_dataset.feather\\', \\'covid_FNIR.feather\\']\\ncombined_df = load_data_from_category(category, included_filenames)\\n\\n# Drop NaN values\\ncombined_df.dropna(inplace=True)\\n\\n# Print the resulting DataFrame\\nprint(combined_df)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def load_data_from_category(category, filenames):\n",
    "    dataframes = []\n",
    "    for file_name in filenames:\n",
    "        file_path = os.path.join(root_dir, category, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_feather(file_path)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"The file '{file_name}' in the '{category}' category does not exist.\")\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "root_dir = '../data'\n",
    "category = 'health'\n",
    "\n",
    "# List of filenames to include in combined_df\n",
    "#included_filenames = ['isot_dataset.feather', 'fake_news_dataset.feather', 'pheme.feather', 'liar_dataset.feather', 'politifact_dataset.feather']\n",
    "included_filenames = ['covid_claims.feather', 'covid_fake_news_dataset.feather', 'covid_FNIR.feather']\n",
    "combined_df = load_data_from_category(category, included_filenames)\n",
    "\n",
    "# Drop NaN values\n",
    "combined_df.dropna(inplace=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(combined_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7dd705d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:22.468654Z",
     "iopub.status.busy": "2023-12-13T07:53:22.467653Z",
     "iopub.status.idle": "2023-12-13T07:53:22.472816Z",
     "shell.execute_reply": "2023-12-13T07:53:22.472816Z"
    },
    "papermill": {
     "duration": 0.010168,
     "end_time": "2023-12-13T07:53:22.473821",
     "exception": false,
     "start_time": "2023-12-13T07:53:22.463653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['circulating on social networks video that shows an excerpt from spanish television show supposedly issued december in which it appears woman who claims to be psychic to make predictions in this video the woman describes set of events that have been interpreted as detailed forecast of covid pandemic that has hit the world it is however fake video at least as regards the date of issue the video has been being disseminated on the internet with date and not tampered with the real '\n",
      " 'internet sensation and the world cutest baby anahita hashemzadeh is suffering covid video is doing the rounds on various social media platforms and messaging apps little girl can be seen coughing and crying in the video according to one of the viral claims this girl is internet sensation anahita hashemzadeh who got infected with covid this claim is widely shared onyoutubeno internet sensation anahita hashemzadeh is not suffering from covid pm apr preeti chauhan claim internet sensation and the world cutest baby anahita hashemzadeh is suffering covid world cutest baby anahita suffering corona it very dangerous so stay home stay safe cronavirusindia coronaupdatesindia covid india covid beastauro akshaykumar akshaymanish aarush for akki pic twitter com mnt msihk rohan rn rnsrksrider march verification video is doing the rounds on various social media platforms and messaging apps little girl can be seen coughing and crying in the video according to one of the viral claims this girl is internet sensation anahita hashemzadeh who got infected with covid this claim is widely shared on twitter facebook youtube and sharechat we first did google search about anahita hashemzadeh and found out that the girl hails from iran and is an internet sensation with over one million followers on instagram her images are being shared along with the video of girl who seems to be unwell and admitted to the hospital but to verify whether the girl in the video is anahita we carefully studied the video we noticed that the eye color of the girl in the video and that of anahita is different we then searched anahita instagram account which is managed by her mother for more information in one of the pictures many users asked about the rumors going around of anahita being diagnosed with covid to this her mother replied she anahita is healthy she also posted video on her instagram story clarifying anahita is healthy and safe the video can be seen below it is clear with all the evidence we found that anahita is not suffering from covid and she is not the girl in the video we tried to find out more about the viral video with the help of google reverse image search the video was first shared on facebook page by the name of mascula city on march though there was no information about the girl given in the description we can say whether the girl is suffering from coronavirus infection or not as we are still trying to identify the girl in the video tools used google search youtube search invid google reverse image search result false if you would like us to fact check claim give feedback or lodge complaint whatsapp us at you can also visit the contact us page and fill the form video is doing the rounds on various social media platforms and messaging apps little girl can be seen coughing and crying in the video according to one of the viral claims this girl is internet sensation anahita hashemzadeh who got infected with covid this claim is widely shared on twitter facebook youtube and sharechat facebooktwitter'\n",
      " 'a video has been viewed hundreds of thousands of times on facebook and youtube in march and april alongside claim it shows koran recitation during us senate meeting attended by president donald trump during the covid pandemic the claim is false the video actually shows trump attending an interfaith prayer service at church after his presidential inauguration in january the video caption states president donald trump and vice president mike pence attend the national prayer service at the washington national cathedral an inaugural tradition that dates back to george washington as of april the novel coronavirus disease or covid has killed at least people in the us and infected more than others according to this data from us centers for disease control and prevention cdc the post indonesian language caption translates to english as holy god almighty us broadcast channel span also reported on the event on january here saying president donald trump and vice president mike pence and their families attended the th presidential inauguration national prayer service at the national cathedral in washington c the interfaith service traditionally held the morning after the inauguration featured prayers and readings from religious leaders of many faiths along with musical performances the service marked the end of the official inaugural schedule the koran recitation starts at the minute four second mark of the video the same clip was also shared here on march with an english language claim that it shows trump and pence listening to koran recitation at us senate meeting during the coronavirus pandemic google reverse image search using keyframe generated by invid weverify video verification tool and subsequent keyword searches found this video published on the facebook page for abc news on january it also has been watched more than times after it was shared on facebook here here and here with similar claims and comments in arabic the video was viewed more than times and shared more than times after it was published in this facebook post on april below is screenshot of the misleading post the three minute eight second video shows us president donald trump sitting next to first lady melania trump vice president mike pence and his wife karen pence the video title which is almost an hour and minutes in length reads trump pence attend national prayer service the white house asked for an imam islamic scholar to pray so they are kept away from the corona outbreak from this their heart actually acknowledge that islam is true religion and may allah give them the guidance to islam trump was inaugurated as the th us president on january one day before the interfaith prayer event below is screenshot comparison of the clip in the misleading post and the abc news video the white text across the bottom of the video reads praise be to allah the first time in history the us president and senate opened meeting with the koran the video has been viewed more than times after it was posted here on facebook and here on youtube with similar claim about koran recitation at the us senate with comments about the covid outbreak the washington national cathedral lists the january event here on its website while the full schedule of the event can be read here these claims are false the video shows trump attending an interfaith prayer service at church in washington dc after his inauguration as us president in '\n",
      " 'a facebook post claiming that the national treasury will deposit ksh to the mobile wallets of nairobi residents is false and although president uhuru kenyatta set up the covid emergency response fund board to support the government efforts in assisting vulnerable communities with their immediate needs such as food the distribution approach which also includes cash transfers does not require beneficiaries to get in touch with the national treasury directly pesacheck has looked into claim that the national treasury is depositing upkeep money to the mobile wallets of nairobi residents during covid and finds it to be false according to the guidelines assessment of beneficiaries is done through local county and sub county offices and network of beneficiary welfare committees bwcs in each location additional support is provided by the local administration including chiefs their assistants and community members if this were true it would have made headlines in major news publications in the country no news outlet has covered this by partnering with facebook and similar social media platforms third party fact checking organisations like pesacheck are helping to sort fact from fiction we do this by giving the public deeper insight and context to posts they see in their social media feeds pesacheck is east africa first public finance fact checking initiative it was co founded by catherine gicheru and justin arenstein and is being incubated by the continent largest civic technology and data journalism accelerator code for africa it seeks to help the public separate fact from fiction in public pronouncements about the numbers that shape our world with special emphasis on pronouncements about public finances that shape government delivery of sustainable development goals sdg public services such as healthcare rural development and access to water sanitation pesacheck also tests the accuracy of media reportage to find out more about the project visit pesacheck org however there has been no communication of the sort by the ministry either on its website or through its facebook and twitter accounts the post urges residents to confirm their mpesa details with the treasury in order to receive the funds apparently meant to assist them to cater for their needs while they stay at home due to the covid crisis there has been no such communication from the national treasury and the government monetary assistance is based on defined national needs assessment matrix rather it is based on defined national needs assessment matrix and provided for under the ministry of labour and social protection treasury cabinet secretary ukur yattani has however said the government is realigning its budget to make funds available to mitigate the effects of the covid pandemic this fact check was written by fact checker simon muli and edited by news editor enock nyariki pesacheck is joint initiative of code for africa through its innovateafrica fund with additional funding support from the international budget partnership kenya and twaweza in partnership with coalition of local media organisations and the international center for journalists icfj have you spotted what you think is fake news or false information on facebook here how you can report and here more information on pesacheck methodology for fact checking questionable content this post is part of an ongoing series of pesacheck fact checks examining content marked as potential misinformation on facebook and other social media platforms '\n",
      " 'moves on facebook and twitter video showing police using force against citizens on public roads the video was protests in azerbaijan in but viral text says is violent measures in hungary for pandemic covid the last april orban announced the unlimited extension of quarantine in hungr aunque it is true that the hungarian government is taking advantage of the quarantine to legislate against human rights it is false that is arresting men years or older to avoid contagios la translation of the title of the original video is images police and insults protesters violence government anti which were held in baku in late la description of the original video says it refers to events with the description this is the situation in hungary are picking up men age and over the streets throwing them into police vans as animals to be quarantined in government centers video for nearly five minutes showing arrests and use of force by police elements shared but the video is not in hungary or developed during the outbreak of covid no public record exists or media to confirm that the hungarian authorities captured men aged and over to quarantine in government facilities such as ensuring viral el publication october month before the first case of covid was registered in china china the average meydantv of azerbaijan published the video polisl rin sad zorak q aksiya lar hqir etm r nt ri same which resumed publication in viral and contextualized falsely hungr consulta also our microsite with all the checks we have done so far on the covid sin but even when this kind of police violence was reported the government hungarian captured the attention of defense agencies humanos el sabueso rights is one of the more than events verifiers countries that make up to lianza coordinated by the international red verification of facts related to debunk false information with the coronavirus find the checks in this international alliance with the hashtag coronavirusfacts and datoscoronavirus or visit this gina en an article last april human rights watch warns that the hungarian parliament adopted an emergency law that allows the first viktor orban minister to suspend laws which may jeopardize the rights of citizens at risk ']\n",
      "[1 1 1 1 1]\n",
      "Number of unique classes: 2\n",
      "Class 0: 3284 instances\n",
      "Class 1: 2648 instances\n"
     ]
    }
   ],
   "source": [
    "# May need to include more columns to process metadata\n",
    "texts = combined_df['text'].values\n",
    "labels = combined_df['label'].values\n",
    "num_classes = combined_df['label'].nunique()\n",
    "\n",
    "print(texts[:5])\n",
    "print(labels[:5])\n",
    "print(f\"Number of unique classes: {num_classes}\")\n",
    "\n",
    "unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "for class_label, count in zip(unique_classes, class_counts):\n",
    "  print(f\"Class {class_label}: {count} instances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c12cc34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:22.480821Z",
     "iopub.status.busy": "2023-12-13T07:53:22.480821Z",
     "iopub.status.idle": "2023-12-13T07:53:22.484274Z",
     "shell.execute_reply": "2023-12-13T07:53:22.484274Z"
    },
    "papermill": {
     "duration": 0.008456,
     "end_time": "2023-12-13T07:53:22.485279",
     "exception": false,
     "start_time": "2023-12-13T07:53:22.476823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=7623)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07935fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:22.492279Z",
     "iopub.status.busy": "2023-12-13T07:53:22.492279Z",
     "iopub.status.idle": "2023-12-13T07:53:23.915673Z",
     "shell.execute_reply": "2023-12-13T07:53:23.915673Z"
    },
    "papermill": {
     "duration": 1.429401,
     "end_time": "2023-12-13T07:53:23.917679",
     "exception": false,
     "start_time": "2023-12-13T07:53:22.488278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT (or BERT variation) model and tokenizer\n",
    "model_mapping = {\n",
    "    0: (BertTokenizer, BertForSequenceClassification, 'bert-base-uncased'),\n",
    "    1: (BertTokenizer, BertForSequenceClassification, 'bert-base-cased'),\n",
    "    2: (DistilBertTokenizer, DistilBertForSequenceClassification, 'distilbert-base-uncased-finetuned-sst-2-english'),\n",
    "    3: (DistilBertTokenizer, DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
    "    4: (RobertaTokenizer, RobertaForSequenceClassification, 'roberta-base'),\n",
    "    5: (AlbertTokenizer, AlbertForSequenceClassification, 'albert-base-v2')\n",
    "}\n",
    "\n",
    "tokenizer_class, model_class, model_name = model_mapping.get(select_model, (None, None, None))\n",
    "\n",
    "if tokenizer_class and model_class and model_name:\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name, num_labels=2)\n",
    "else:\n",
    "    logging.error(f\"Invalid model selection: {select_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ce1657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:23.925680Z",
     "iopub.status.busy": "2023-12-13T07:53:23.925680Z",
     "iopub.status.idle": "2023-12-13T07:53:23.929960Z",
     "shell.execute_reply": "2023-12-13T07:53:23.929960Z"
    },
    "papermill": {
     "duration": 0.010287,
     "end_time": "2023-12-13T07:53:23.930965",
     "exception": false,
     "start_time": "2023-12-13T07:53:23.920678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Freeze the layers up to the specified layer\n",
    "if freeze_layers_up_to > 0:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if select_model == 0 or select_model == 1:\n",
    "        print(\"Layers: \"+str(len(model.bert.encoder.layer)))\n",
    "        for param in model.bert.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif select_model == 2 or select_model == 3:\n",
    "        print(\"Layers: \"+str(len(model.distilbert.transformer.layer)))\n",
    "        for param in model.distilbert.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif select_model == 4:\n",
    "        print(\"Layers: \"+str(len(model.roberta.encoder.layer)))\n",
    "        for param in model.roberta.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif select_model == 5:\n",
    "        print(\"Layers: \"+str(len(model.albert.encoder.albert_layer_groups)))\n",
    "        for param in model.albert.embeddings.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e341a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:23.938970Z",
     "iopub.status.busy": "2023-12-13T07:53:23.938970Z",
     "iopub.status.idle": "2023-12-13T07:53:49.456453Z",
     "shell.execute_reply": "2023-12-13T07:53:49.455449Z"
    },
    "papermill": {
     "duration": 25.523466,
     "end_time": "2023-12-13T07:53:49.457454",
     "exception": false,
     "start_time": "2023-12-13T07:53:23.933988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f573441e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:49.466454Z",
     "iopub.status.busy": "2023-12-13T07:53:49.465454Z",
     "iopub.status.idle": "2023-12-13T07:53:49.941721Z",
     "shell.execute_reply": "2023-12-13T07:53:49.941721Z"
    },
    "papermill": {
     "duration": 0.482254,
     "end_time": "2023-12-13T07:53:49.942727",
     "exception": false,
     "start_time": "2023-12-13T07:53:49.460473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the encodings to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create a DataLoader for training and testing\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=8)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff4d023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:49.951726Z",
     "iopub.status.busy": "2023-12-13T07:53:49.951726Z",
     "iopub.status.idle": "2023-12-13T07:53:49.954459Z",
     "shell.execute_reply": "2023-12-13T07:53:49.954459Z"
    },
    "papermill": {
     "duration": 0.008738,
     "end_time": "2023-12-13T07:53:49.955464",
     "exception": false,
     "start_time": "2023-12-13T07:53:49.946726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97332f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:53:49.962469Z",
     "iopub.status.busy": "2023-12-13T07:53:49.962469Z",
     "iopub.status.idle": "2023-12-13T07:55:45.082489Z",
     "shell.execute_reply": "2023-12-13T07:55:45.082489Z"
    },
    "papermill": {
     "duration": 115.126104,
     "end_time": "2023-12-13T07:55:45.084593",
     "exception": false,
     "start_time": "2023-12-13T07:53:49.958489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Move the model and data to the GPU\n",
    "model.to(device)\n",
    "train_inputs, train_masks, train_labels = train_inputs.to(device), train_masks.to(device), train_labels.to(device)\n",
    "test_inputs, test_masks, test_labels = test_inputs.to(device), test_masks.to(device), test_labels.to(device)\n",
    "\n",
    "# Define class weights based on class imbalance\n",
    "class_weights = [weight_for_class_0, weight_for_class_1]\n",
    "\n",
    "# Define the loss function with class weights\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device))\n",
    "\n",
    "# Define the optimizer with a learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Fine-tune the pre-trained BERT model\n",
    "train_start_time = time.time()\n",
    "model.train()\n",
    "model.to(device)\n",
    "for batch in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
    "    outputs = model(**inputs)\n",
    "    loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "train_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fa228cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:55:45.093594Z",
     "iopub.status.busy": "2023-12-13T07:55:45.092596Z",
     "iopub.status.idle": "2023-12-13T07:55:54.414187Z",
     "shell.execute_reply": "2023-12-13T07:55:54.414187Z"
    },
    "papermill": {
     "duration": 9.32761,
     "end_time": "2023-12-13T07:55:54.416213",
     "exception": false,
     "start_time": "2023-12-13T07:55:45.088603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_start_time = time.time()\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': None}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "eval_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a42297c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:55:54.425195Z",
     "iopub.status.busy": "2023-12-13T07:55:54.424194Z",
     "iopub.status.idle": "2023-12-13T07:55:54.433108Z",
     "shell.execute_reply": "2023-12-13T07:55:54.433108Z"
    },
    "papermill": {
     "duration": 0.01593,
     "end_time": "2023-12-13T07:55:54.435122",
     "exception": false,
     "start_time": "2023-12-13T07:55:54.419192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "precision = precision_score(test_labels.cpu(), predictions)\n",
    "recall = recall_score(test_labels.cpu(), predictions)\n",
    "f1 = f1_score(test_labels.cpu(), predictions)\n",
    "accuracy = accuracy_score(test_labels.cpu(), predictions)\n",
    "g_mean = (recall*accuracy)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6667d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:55:54.443122Z",
     "iopub.status.busy": "2023-12-13T07:55:54.442142Z",
     "iopub.status.idle": "2023-12-13T07:55:54.813480Z",
     "shell.execute_reply": "2023-12-13T07:55:54.813480Z"
    },
    "papermill": {
     "duration": 0.376346,
     "end_time": "2023-12-13T07:55:54.814503",
     "exception": false,
     "start_time": "2023-12-13T07:55:54.438157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dir = './models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "saved_model_name = f\"bert_model_{run_id}.pt\"\n",
    "model_path = os.path.join(save_dir, saved_model_name)\n",
    "\n",
    "# Only save model if accuracy meets minimum threshold\n",
    "if accuracy > min_acc:\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb25463c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T07:55:54.824502Z",
     "iopub.status.busy": "2023-12-13T07:55:54.824502Z",
     "iopub.status.idle": "2023-12-13T07:55:54.829799Z",
     "shell.execute_reply": "2023-12-13T07:55:54.829799Z"
    },
    "papermill": {
     "duration": 0.012304,
     "end_time": "2023-12-13T07:55:54.831805",
     "exception": false,
     "start_time": "2023-12-13T07:55:54.819501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Training time: 113.7515377998352 seconds\n",
      "Inference time: 9.317203760147095 seconds\n",
      "Precision: 0.8847058823529412\n",
      "Recall: 0.7445544554455445\n",
      "F-score: 0.8086021505376344\n",
      "Accuracy: 0.8500421229991575\n",
      "G-mean: 0.7955517896374895\n"
     ]
    }
   ],
   "source": [
    "# Log the results\n",
    "logging.info(\"Evaluation Results\")\n",
    "logging.info(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "logging.info(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F-score: {f1}\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"G-mean: {g_mean}\")\n",
    "logging.info(\"Additional Info\")\n",
    "logging.info(f\"Model name: {model_name}\")\n",
    "logging.info(f\"Datasets list: {categories}\")\n",
    "logging.info(f\"Layers frozen: {freeze_layers_up_to}\")\n",
    "logging.info(f\"Learning rate: {learning_rate}\")\n",
    "logging.info(f\"Class weights: {class_weights}\")\n",
    "if accuracy > min_acc:\n",
    "  logging.info(f\"Model saved to: {model_path}\")\n",
    "else:\n",
    "  logging.info(\"Model not saved, didn't meet minimum accuracy threshold\")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "print(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"G-mean: {g_mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 162.616613,
   "end_time": "2023-12-13T07:55:55.839160",
   "environment_variables": {},
   "exception": null,
   "input_path": "detection.ipynb",
   "output_path": "output_detection_health_model_3.ipynb",
   "parameters": {
    "categories": "health",
    "freeze_layers_up_to": 0,
    "learning_rate": 1e-05,
    "min_acc": 0.8,
    "select_model": 3,
    "weight_for_class_0": 0.45,
    "weight_for_class_1": 0.55
   },
   "start_time": "2023-12-13T07:53:13.222547",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}