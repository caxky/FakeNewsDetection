2023-12-22 19:37:58,699 - 
Starting detection model - GzZ4rC3zMm
2023-12-22 19:38:57,187 - Epoch 1 average train loss: 0.0
2023-12-22 19:39:29,909 - Epoch 2 average train loss: 0.0
2023-12-22 19:40:02,618 - Epoch 3 average train loss: 0.0
2023-12-22 19:40:35,331 - Epoch 4 average train loss: 0.0
2023-12-22 19:41:08,046 - Epoch 5 average train loss: 0.0
2023-12-22 19:41:11,042 - Evaluation Results
2023-12-22 19:41:11,042 - Training time: 163.95419907569885 seconds
2023-12-22 19:41:11,042 - Inference time: 2.5303738117218018 seconds
2023-12-22 19:41:11,042 - Precision: 0.8611670020120724
2023-12-22 19:41:11,042 - Recall: 0.8475247524752475
2023-12-22 19:41:11,042 - F-score: 0.8542914171656686
2023-12-22 19:41:11,042 - Accuracy: 0.8770008424599831
2023-12-22 19:41:11,042 - G-mean: 0.8621368348043602
2023-12-22 19:41:11,042 - Additional Info
2023-12-22 19:41:11,042 - Model name: distilbert-base-uncased
2023-12-22 19:41:11,042 - Datasets list: health
2023-12-22 19:41:11,042 - Dataset size: 5932
2023-12-22 19:41:11,043 - Layers frozen: 0
2023-12-22 19:41:11,043 - Learning rate: 1e-05
2023-12-22 19:41:11,043 - Class weights: Manual weights - [0.45, 0.55]
2023-12-22 19:41:11,043 - Model saved to: ./models/bert_model_GzZ4rC3zMm.pt
