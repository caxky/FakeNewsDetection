2023-12-22 08:42:23,211 - 
Starting detection model - F27pUjX2O3
2023-12-22 08:42:43,684 - Epoch 1 average train loss: 0.0
2023-12-22 08:42:44,773 - Evaluation Results
2023-12-22 08:42:44,774 - Training time: 14.99517560005188 seconds
2023-12-22 08:42:44,774 - Inference time: 0.6696686744689941 seconds
2023-12-22 08:42:44,774 - Precision: 1.0
2023-12-22 08:42:44,774 - Recall: 0.10526315789473684
2023-12-22 08:42:44,774 - F-score: 0.1904761904761905
2023-12-22 08:42:44,774 - Accuracy: 0.8015564202334631
2023-12-22 08:42:44,774 - G-mean: 0.2904726493571728
2023-12-22 08:42:44,774 - Additional Info
2023-12-22 08:42:44,774 - Model name: distilbert-base-uncased-finetuned-sst-2-english
2023-12-22 08:42:44,775 - Datasets list: politics
2023-12-22 08:42:44,775 - Dataset size: 6424
2023-12-22 08:42:44,775 - Layers frozen: 0
2023-12-22 08:42:44,775 - Learning rate: 1e-05
2023-12-22 08:42:44,775 - Class weights: Default weights - [0.7921855541718555, 0.20781444582814446]
2023-12-22 08:42:44,775 - Model saved to: ./models/bert_model_F27pUjX2O3.pt
