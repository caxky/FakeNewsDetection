{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_mapping = {\n",
    "    'crime': './results/crime',\n",
    "    'science': './results/science',\n",
    "    'health': './results/health',\n",
    "    'politics': './results/politics',\n",
    "    'social_media': './results/social_media'\n",
    "}\n",
    "\n",
    "dataset_mapping = {\n",
    "  'fa-kes': '../data/crime/FA-KES-Dataset.feather',  \n",
    "  'snope': '../data/crime/snope.feather',  \n",
    "  'covid_claims': '../data/health/covid_claims.feather',  \n",
    "  'covid_fake_news': '../data/health/covid_fake_news_dataset.feather',  \n",
    "  'covid_FNIR': '../data/health/covid_FNIR.feather',  \n",
    "  'fake_news': '../data/politics/fake_news_dataset.feather',  \n",
    "  'isot_dataset': '../data/politics/isot_dataset.feather',  \n",
    "  'liar_dataset': '../data/politics/liar_dataset.feather',  \n",
    "  'pheme': '../data/politics/pheme.feather',  \n",
    "  'politifact': '../data/politics/politifact_dataset.feather',  \n",
    "  'climate': '../data/science/climate_dataset.feather',  \n",
    "  'gossipcop': '../data/social_media/gossipcop.feather',  \n",
    "  'isot_social': '../data/social_media/isot_dataset.feather',  \n",
    "  'isot_multipurpose': '../data/isot_multipurpose_small.feather'\n",
    "}\n",
    "\n",
    "combined_dataset_mappings = {\n",
    "  'crime': ['fa-kes', 'snope'],\n",
    "  'health': ['covid_claims', 'covid_fake_news', 'covid_FNIR'],\n",
    "  'politics': ['fake_news', 'isot_dataset', 'liar_dataset', 'pheme', 'politifact'],\n",
    "  'science': ['climate', 'isot_multipurpose'],\n",
    "  'social_media': ['gossipcop', 'isot_social']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = []\n",
    "\n",
    "for category, directory in log_dir_mapping.items():\n",
    "    if os.path.exists(directory):\n",
    "        log_files = [f for f in os.listdir(directory) if f.endswith('.log')]\n",
    "        \n",
    "        if log_files:\n",
    "            for log_file in log_files:\n",
    "                with open(os.path.join(directory, log_file), 'r') as file:\n",
    "                    lines = file.readlines()\n",
    "                    log_info = {}\n",
    "                    skip_file = False\n",
    "                    datasets_detailed = \"\"\n",
    "                    datasets_list = \"\"\n",
    "                    for line in lines:\n",
    "                        parts = line.split(' - ')\n",
    "                        if len(parts) > 1:\n",
    "                            key_value = parts[1].split(': ', maxsplit=1)\n",
    "                            if len(key_value) == 2:\n",
    "                                key, value = key_value\n",
    "                                log_info[key.strip()] = value.strip()\n",
    "                                # Check for zero values in specific keys\n",
    "                                if key.strip() in ['Precision', 'Recall', 'F-score', 'Accuracy', 'G-mean']:\n",
    "                                    if float(value.strip()) == 0.0:\n",
    "                                        skip_file = True\n",
    "                                # Capture category\n",
    "                                if key.strip() == 'Datasets list':\n",
    "                                    datasets_match = re.match(r'(.+)$', value.strip())\n",
    "                                    datasets_match_more = re.match(r'(.*?)\\b(Train: \\[.*?\\]), (Test: \\[.*?\\])', value.strip())\n",
    "                                    if datasets_match_more:\n",
    "                                        datasets_list = datasets_match_more.group(1).strip()\n",
    "                                        train_dataset_name = datasets_match_more.group(2)\n",
    "                                        test_dataset_name = datasets_match_more.group(3)\n",
    "                                        if train_dataset_name and test_dataset_name:\n",
    "                                            datasets_detailed = f\"{train_dataset_name}, {test_dataset_name}\"\n",
    "                                    elif datasets_match:\n",
    "                                        datasets_list = datasets_match.group(1).strip()\n",
    "                                # Capture Class weights information\n",
    "                                if key.strip() == 'Class weights':\n",
    "                                    class_weights_match = re.match(r'.*Class weights: (.*)', line.strip())\n",
    "                                    if class_weights_match:\n",
    "                                        log_info['Class weights'] = class_weights_match.group(1)\n",
    "                    if log_info and not skip_file:\n",
    "                        log_info['Datasets list'] = datasets_list\n",
    "                        log_info['Datasets list detailed'] = datasets_detailed\n",
    "                        log_data.append(log_info)\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame(log_data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Accuracy'] = pd.to_numeric(df['Accuracy'], errors='coerce')\n",
    "df['Accuracy'] = df['Accuracy'].round(4)\n",
    "df['G-mean'] = pd.to_numeric(df['G-mean'], errors='coerce')\n",
    "df['G-mean'] = df['G-mean'].round(4)\n",
    "df['F-score'] = pd.to_numeric(df['F-score'], errors='coerce')\n",
    "df['F-score'] = df['F-score'].round(4)\n",
    "df['Recall'] = pd.to_numeric(df['Recall'], errors='coerce')\n",
    "df['Recall'] = df['Recall'].round(4)\n",
    "df['Precision'] = pd.to_numeric(df['Precision'], errors='coerce')\n",
    "df['Precision'] = df['Precision'].round(4)\n",
    "best_models = df.groupby('Datasets list')['Accuracy'].idxmax()\n",
    "best_models_info = df.loc[best_models, ['Datasets list', 'Accuracy', 'G-mean', 'F-score', 'Recall', 'Precision', 'Model saved to', 'Model name']]\n",
    "print(best_models_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store data for plotting\n",
    "dataset_names = []\n",
    "num_entries_list = []\n",
    "fake_counts = []\n",
    "not_fake_counts = []\n",
    "fake_percentages = []\n",
    "not_fake_percentages = []\n",
    "\n",
    "# Count number of entries and the class distribution in each dataset\n",
    "for dataset_name, file_path in dataset_mapping.items():\n",
    "    try:\n",
    "        data = pd.read_feather(file_path)\n",
    "        num_entries = len(data)\n",
    "        class_distribution = data['label'].value_counts(normalize=True) * 100\n",
    "        fake_count = data['label'].value_counts().get(0, 0)  # Count of 'Fake' entries\n",
    "        not_fake_count = data['label'].value_counts().get(1, 0)  # Count of 'Not Fake' entries\n",
    "        \n",
    "        dataset_names.append(dataset_name)\n",
    "        num_entries_list.append(num_entries)\n",
    "        fake_counts.append(fake_count)\n",
    "        not_fake_counts.append(not_fake_count)\n",
    "        fake_percentages.append(class_distribution.get(0, 0))\n",
    "        not_fake_percentages.append(class_distribution.get(1, 0))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        print(\"-\" * 30)\n",
    "    except KeyError:\n",
    "        print(f\"No 'label' column found in {dataset_name}.\")\n",
    "        print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, axs = plt.subplots(4, 4, figsize=(15, 25))\n",
    "axs = axs.flatten()\n",
    "colors = ['orange', 'blue']  # Assign colors for 'Fake' and 'Not Fake'\n",
    "\n",
    "for i, (dataset_name, num_entries, fake_count, not_fake_count, fake_percentage, not_fake_percentage) in enumerate(zip(dataset_names, num_entries_list, fake_counts, not_fake_counts, fake_percentages, not_fake_percentages)):\n",
    "    bar = axs[i].bar(['Fake', 'Not Fake'], [fake_count, not_fake_count], color=colors)\n",
    "    axs[i].set_title(f'{dataset_name}\\nEntries: {num_entries}')\n",
    "    axs[i].set_ylabel('Counts')\n",
    "\n",
    "    # Annotate bars with percentages\n",
    "    for rect, percentage in zip(bar, [fake_percentage, not_fake_percentage]):\n",
    "        height = rect.get_height()\n",
    "        axs[i].annotate(f'{percentage:.1f}%', xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3),\n",
    "                        textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(len(dataset_names), 15):\n",
    "    axs[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and combine data for each category\n",
    "combined_dataframes = {}\n",
    "for category, datasets in combined_dataset_mappings.items():\n",
    "    category_data = pd.concat([pd.read_feather(dataset_mapping[dataset]) for dataset in datasets])\n",
    "    combined_dataframes[category] = category_data\n",
    "\n",
    "# Plot class distribution for each category\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (category, data) in enumerate(combined_dataframes.items()):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    class_distribution = data['label'].value_counts(normalize=True)\n",
    "    class_distribution = class_distribution.rename({0: 'Fake', 1: 'Not Fake'})\n",
    "    class_distribution[['Fake', 'Not Fake']].plot(kind='bar', color=['orange', 'blue'])\n",
    "    plt.title(f'Combined Class Distribution - {category}\\nTotal Entries: {len(data)}')  # Include total number of entries\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Annotate bars with percentage values\n",
    "    for index, value in enumerate(class_distribution):\n",
    "        plt.text(index, value + 0.05, f'{value:.2%}', ha='center', color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average model performance for each category with accuracy on bars in a 3x2 grid\n",
    "categories = df['Datasets list'].unique()\n",
    "model_names = df['Model name'].unique()\n",
    "plt.figure(figsize=(20, 8))  # Adjust the figure size\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_data = df[df['Datasets list'] == category]\n",
    "    avg_accuracies = category_data.groupby('Model name')['Accuracy'].agg(['mean', 'count']).sort_values('mean')\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    bars = plt.barh(range(len(avg_accuracies)), avg_accuracies['mean'], align='center')\n",
    "    plt.yticks(range(len(avg_accuracies)), avg_accuracies.index)\n",
    "    plt.xlabel('Average Accuracy')\n",
    "    plt.title(f'Average Accuracy by Model - {category}')\n",
    "    min_accuracy = avg_accuracies['mean'].min()\n",
    "    plt.margins(x=0.1, y=0)  # Set margins to start just below the smallest accuracy, and keep y-axis unchanged\n",
    "    plt.xlim(0, 1.0)  # Set x-axis range from 0 to 1.0\n",
    "    \n",
    "    # Annotate bars with average accuracy percentages and number of entries\n",
    "    for bar, accuracy, count in zip(bars, avg_accuracies['mean'], avg_accuracies['count']):\n",
    "        plt.text(bar.get_width() - 0.05, bar.get_y() + bar.get_height() / 2, f'{accuracy:.1%} ({count} runs)', \n",
    "                 va='center', ha='right', color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the spacing between subplots\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average model performance for each category with accuracy on bars in a 3x2 grid\n",
    "categories = df['Datasets list'].unique()\n",
    "model_names = df['Model name'].unique()\n",
    "plt.figure(figsize=(20, 8))  # Adjust the figure size\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_data = df[df['Datasets list'] == category]\n",
    "    avg_accuracies = category_data.groupby('Model name')['G-mean'].agg(['mean', 'count']).sort_values('mean')\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    bars = plt.barh(range(len(avg_accuracies)), avg_accuracies['mean'], align='center', color='red')  # Change color to orange\n",
    "    plt.yticks(range(len(avg_accuracies)), avg_accuracies.index)\n",
    "    plt.xlabel('Average G-mean')\n",
    "    plt.title(f'Average G-mean by Model - {category}')\n",
    "    min_accuracy = avg_accuracies['mean'].min()\n",
    "    plt.margins(x=0.1, y=0)  # Set margins to start just below the smallest accuracy, and keep y-axis unchanged\n",
    "    plt.xlim(0, 1.0)  # Set x-axis range from 0 to 1.0\n",
    "    \n",
    "    # Annotate bars with average accuracy percentages and number of entries\n",
    "    for bar, accuracy, count in zip(bars, avg_accuracies['mean'], avg_accuracies['count']):\n",
    "        plt.text(bar.get_width() - 0.05, bar.get_y() + bar.get_height() / 2, f'{accuracy:.1%} ({count} runs)', \n",
    "                 va='center', ha='right', color='black', fontweight='bold')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the spacing between subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average model performance for each category with accuracy on bars in a 3x2 grid\n",
    "categories = df['Datasets list'].unique()\n",
    "model_names = df['Model name'].unique()\n",
    "plt.figure(figsize=(20, 8))  # Adjust the figure size\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_data = df[df['Datasets list'] == category]\n",
    "    avg_accuracies = category_data.groupby('Model name')['F-score'].agg(['mean', 'count']).sort_values('mean')\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    bars = plt.barh(range(len(avg_accuracies)), avg_accuracies['mean'], align='center', color='green')  # Change color to orange\n",
    "    plt.yticks(range(len(avg_accuracies)), avg_accuracies.index)\n",
    "    plt.xlabel('Average F-score')\n",
    "    plt.title(f'Average F-score by Model - {category}')\n",
    "    min_accuracy = avg_accuracies['mean'].min()\n",
    "    plt.margins(x=0.1, y=0)  # Set margins to start just below the smallest accuracy, and keep y-axis unchanged\n",
    "    plt.xlim(0, 1.0)  # Set x-axis range from 0 to 1.0\n",
    "    \n",
    "    # Annotate bars with average accuracy percentages and number of entries\n",
    "    for bar, accuracy, count in zip(bars, avg_accuracies['mean'], avg_accuracies['count']):\n",
    "        plt.text(bar.get_width() - 0.05, bar.get_y() + bar.get_height() / 2, f'{accuracy:.1%} ({count} runs)', \n",
    "                 va='center', ha='right', color='black', fontweight='bold')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_accuracies = df.groupby('Datasets list')['Accuracy'].agg(['max', 'min', 'mean'])\n",
    "category_accuracies = category_accuracies.sort_values('mean', ascending=False)\n",
    "\n",
    "# Set the 'viridis' color palette\n",
    "colors_viridis = sns.color_palette(\"viridis\", 3) \n",
    "\n",
    "# Plotting best, worst, and average accuracy by category using the 'viridis' palette\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "categories = category_accuracies.index\n",
    "bar_width = 0.3\n",
    "index = range(len(categories))\n",
    "\n",
    "plt.bar(index, category_accuracies['max'], bar_width, color=colors_viridis[0], label='Best Accuracy')\n",
    "plt.bar([i + bar_width for i in index], category_accuracies['min'], bar_width, color=colors_viridis[1], label='Worst Accuracy')\n",
    "plt.bar([i + 2 * bar_width for i in index], category_accuracies['mean'], bar_width, color=colors_viridis[2], label='Average Accuracy')\n",
    "\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Best, Worst, and Average Accuracy by Category')\n",
    "plt.xticks([i + bar_width for i in index], categories)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['G-mean'] = pd.to_numeric(df['G-mean'], errors='coerce')\n",
    "\n",
    "# Calculate best, worst, and average G-mean by category\n",
    "category_g_mean = df.groupby('Datasets list')['G-mean'].agg(['max', 'min', 'mean'])\n",
    "category_g_mean = category_g_mean.sort_values('mean', ascending=False)\n",
    "\n",
    "# Set color palette for G-mean\n",
    "colors_g_mean = sns.color_palette(\"mako\")\n",
    "\n",
    "# Plotting best, worst, and average G-mean by category with distinct color palette\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "categories = category_g_mean.index\n",
    "bar_width = 0.3\n",
    "index = range(len(categories))\n",
    "\n",
    "plt.bar(index, category_g_mean['max'], bar_width, color=colors_g_mean[2], label='Best G-mean')\n",
    "plt.bar([i + bar_width for i in index], category_g_mean['min'], bar_width, color=colors_g_mean[0], label='Worst G-mean')\n",
    "plt.bar([i + 2 * bar_width for i in index], category_g_mean['mean'], bar_width, color=colors_g_mean[4], label='Average G-mean')\n",
    "\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('G-mean')\n",
    "plt.title('Best, Worst, and Average G-mean by Category')\n",
    "plt.xticks([i + bar_width for i in index], categories)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['F-score'] = pd.to_numeric(df['F-score'], errors='coerce')\n",
    "\n",
    "# Calculate best, worst, and average F-score by category\n",
    "category_f_score = df.groupby('Datasets list')['F-score'].agg(['max', 'min', 'mean'])\n",
    "category_f_score = category_f_score.sort_values('mean', ascending=False)\n",
    "\n",
    "# Set color palette for F-score\n",
    "colors_f_score = sns.color_palette(\"rocket\")\n",
    "\n",
    "# Plotting best, worst, and average F-score by category with distinct color palette\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "categories = category_f_score.index\n",
    "bar_width = 0.3\n",
    "index = range(len(categories))\n",
    "\n",
    "plt.bar(index, category_f_score['max'], bar_width, color=colors_f_score[2], label='Best F-score')\n",
    "plt.bar([i + bar_width for i in index], category_f_score['min'], bar_width, color=colors_f_score[0], label='Worst F-score')\n",
    "plt.bar([i + 2 * bar_width for i in index], category_f_score['mean'], bar_width, color=colors_f_score[4], label='Average F-score')\n",
    "\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('F-score')\n",
    "plt.title('Best, Worst, and Average F-score by Category')\n",
    "plt.xticks([i + bar_width for i in index], categories)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Training time'] = df['Training time'].apply(lambda x: re.findall(r'\\d+\\.\\d+', str(x))[0] if re.findall(r'\\d+\\.\\d+', str(x)) else None)\n",
    "df['Inference time'] = df['Inference time'].apply(lambda x: re.findall(r'\\d+\\.\\d+', str(x))[0] if re.findall(r'\\d+\\.\\d+', str(x)) else None)\n",
    "df['Runtime'] = pd.to_numeric(df['Training time']) + pd.to_numeric(df['Inference time'])\n",
    "category_runtime = df.groupby('Datasets list')['Runtime'].mean()\n",
    "\n",
    "# Sort the categories in descending order\n",
    "category_runtime = category_runtime.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Set color palette\n",
    "colors = sns.color_palette(\"Set2\")\n",
    "\n",
    "# Plotting the average runtime per category\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "categories = category_runtime.index\n",
    "index = range(len(categories))\n",
    "\n",
    "plt.bar(index, category_runtime, color=colors)\n",
    "\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Average Runtime in Seconds')\n",
    "plt.title('Average Runtime of Models per Category')\n",
    "plt.xticks(index, categories, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Training time'] = df['Training time'].apply(lambda x: re.findall(r'\\d+\\.\\d+', str(x))[0] if re.findall(r'\\d+\\.\\d+', str(x)) else None)\n",
    "df['Inference time'] = df['Inference time'].apply(lambda x: re.findall(r'\\d+\\.\\d+', str(x))[0] if re.findall(r'\\d+\\.\\d+', str(x)) else None)\n",
    "df['Training time'] = pd.to_numeric(df['Training time'])\n",
    "df['Inference time'] = pd.to_numeric(df['Inference time'])\n",
    "category_training_time = df.groupby('Datasets list')['Training time'].mean()\n",
    "category_testing_time = df.groupby('Datasets list')['Inference time'].mean()\n",
    "\n",
    "# Sort the categories in descending order\n",
    "category_training_time = category_training_time.sort_values(ascending=False)\n",
    "category_testing_time = category_testing_time.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting training time\n",
    "plt.subplot(1, 2, 1)\n",
    "category_training_time.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Training Time per Category')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Average Training Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plotting inference time\n",
    "plt.subplot(1, 2, 2)\n",
    "category_testing_time.plot(kind='bar', color='salmon')\n",
    "plt.title('Average Inference Time per Category')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Average Inference Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
