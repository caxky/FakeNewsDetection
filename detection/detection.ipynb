{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.26.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\user\\anaconda3\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\user\\anaconda3\\lib\\site-packages (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: datetime in c:\\users\\user\\anaconda3\\lib\\site-packages (5.3)\n",
      "Requirement already satisfied: imblearn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\anaconda3\\lib\\site-packages (0.1.99)\n",
      "Requirement already satisfied: papermill in c:\\users\\user\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: protobuf in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.25.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (6.25.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (8.16.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (5.10.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\user\\anaconda3\\lib\\site-packages (from datetime) (5.4.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from imblearn) (0.11.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (8.1.7)\n",
      "Requirement already satisfied: nbformat>=5.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (5.9.2)\n",
      "Requirement already satisfied: nbclient>=0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (0.8.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (0.4)\n",
      "Requirement already satisfied: tenacity>=5.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from papermill) (8.2.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (8.3.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.8)\n",
      "Requirement already satisfied: pyzmq>=20 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\user\\anaconda3\\lib\\site-packages (from nbformat>=5.1.2->papermill) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nbformat>=5.1.2->papermill) (4.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from zope.interface->datetime) (68.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (306)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy transformers pandas torch scikit-learn pyarrow accelerate transformers[torch] transformers[sentencepiece] ipywidgets tqdm datetime imblearn sentencepiece papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import string\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, AlbertTokenizer, AlbertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, optimizer, loss_fn, train_dataloader):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # total_loss += loss.item()\n",
    "            total_loss += loss\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "        logging.info(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Default parameters, Papermill will overwrite these\n",
    "categories = 'science'\n",
    "train_data = None\n",
    "test_data = None\n",
    "select_model = 0\n",
    "freeze_layers_up_to = 0\n",
    "epochs = 1\n",
    "weight_for_class_0 = None\n",
    "weight_for_class_1 = None\n",
    "learning_rate = 0.00001\n",
    "min_acc = 0.85\n",
    "batch_size = 8\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_string(length=10):\n",
    "    letters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "run_id = generate_random_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_mapping = {\n",
    "  'crime': './results/crime',\n",
    "  'science': './results/science',\n",
    "  'health': './results/health',\n",
    "  'politics': './results/politics',\n",
    "  'social_media': './results/social_media'\n",
    "}\n",
    "\n",
    "log_dir = log_dir_mapping.get(categories, './results')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "log_filename = f'training_log_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "log_filepath = os.path.join(log_dir, log_filename)\n",
    "\n",
    "logging.basicConfig(filename=log_filepath, filemode='w', format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logging.info(f'\\nStarting detection model - {run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "0    Global warming is driving polar bears toward e...      0   \n",
      "1    The sun has gone into ‘lockdown’ which could c...      0   \n",
      "2    They tell us that we are the primary forces co...      0   \n",
      "3    The Great Barrier Reef is experiencing the mos...      0   \n",
      "4    Volcanoes Melting West Antarctic Glaciers, Not...      0   \n",
      "..                                                 ...    ...   \n",
      "902  No warming since at least 1995, no melting gla...      1   \n",
      "903  This was the case last year too, while earlier...      1   \n",
      "904  \"Disasters Cost More Than Ever — But Not Becau...      1   \n",
      "905  CO2 constitutes 80% of the non-condensing gree...      1   \n",
      "906           the Great Barrier Reef is in fine fettle      1   \n",
      "\n",
      "                                              metadata  \n",
      "0    [{'article': 'Extinction risk from global warm...  \n",
      "1    [{'article': 'Famine', 'entropy': 0.0, 'eviden...  \n",
      "2    [{'article': 'Carbon dioxide', 'entropy': 0.0,...  \n",
      "3    [{'article': 'Coral bleaching', 'entropy': 0.0...  \n",
      "4    [{'article': 'Antarctica', 'entropy': 0.693147...  \n",
      "..                                                 ...  \n",
      "902  [{'article': 'Global warming', 'entropy': 0.0,...  \n",
      "903  [{'article': 'Arctic methane emissions', 'entr...  \n",
      "904  [{'article': 'Climate change mitigation', 'ent...  \n",
      "905  [{'article': 'Greenhouse gas', 'entropy': 0.0,...  \n",
      "906  [{'article': 'Great Barrier Reef', 'entropy': ...  \n",
      "\n",
      "[907 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "root_dir = '../data'\n",
    "\n",
    "dataset_mapping = {\n",
    "  'fa-kes': '../data/crime/FA-KES-Dataset.feather',\n",
    "  'snope': '../data/crime/snope.feather',\n",
    "  'covid_claims': '../data/health/covid_claims.feather',\n",
    "  'covid_fake_news': '../data/health/covid_fake_news_dataset.feather',\n",
    "  'covid_FNIR': '../data/health/covid_FNIR.feather',\n",
    "  'fake_news': '../data/politics/fake_news_dataset.feather',\n",
    "  'isot_dataset': '../data/politics/isot_dataset.feather',\n",
    "  'liar_dataset': '../data/politics/liar_dataset.feather',\n",
    "  'pheme': '../data/politics/pheme.feather',\n",
    "  'politifact': '../data/politics/politifact_dataset.feather',\n",
    "  'climate': '../data/science/climate_dataset.feather',\n",
    "  'gossipcop': '../data/social_media/gossipcop.feather',\n",
    "  'isot_social': '../data/social_media/isot_dataset.feather',\n",
    "  'isot_multipurpose': '../data/isot_multipurpose_small.feather'\n",
    "}\n",
    "\n",
    "def load_data_from_category(category):\n",
    "    files = os.listdir(os.path.join(root_dir, category))\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        if file.endswith('.feather'):\n",
    "            df = pd.read_feather(os.path.join(root_dir, category, file))\n",
    "            dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "def load_as_train_or_test(dataset_list):\n",
    "    dataframes = []\n",
    "    for dataset in dataset_list:\n",
    "        df = pd.read_feather(dataset_mapping.get(dataset))\n",
    "        dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Use this one when running multiple categories\n",
    "#combined_dataframes = [load_data_from_category(category) for category in categories] \n",
    "# This one is for single category\n",
    "if train_data is None and test_data is None:\n",
    "    train_data_set = False\n",
    "    combined_dataframes = [load_data_from_category(categories)]\n",
    "    combined_df = pd.concat(combined_dataframes, ignore_index=True)\n",
    "    combined_df.dropna(inplace=True)\n",
    "    datasets_size = len(combined_df)\n",
    "    print(\"combined dataframe: \")\n",
    "    print(combined_df)\n",
    "\n",
    "# This one is when specific training and testing sets are used\n",
    "if train_data != None and test_data != None:\n",
    "    train_data_set = True\n",
    "    train_dataset_name, test_dataset_name = train_data, test_data\n",
    "    train_df = load_as_train_or_test(train_data)\n",
    "    test_df = load_as_train_or_test(test_data)\n",
    "    train_df.dropna(inplace=True)\n",
    "    test_df.dropna(inplace=True)\n",
    "    train_dataset_size, test_dataset_size = len(train_df), len(test_df)\n",
    "    print(\"training dataframe: \")\n",
    "    print(train_df.head())\n",
    "    print(\"testing dataframe: \")\n",
    "    print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_data_from_category(category, filenames):\\n    dataframes = []\\n    for file_name in filenames:\\n        file_path = os.path.join(root_dir, category, file_name)\\n        if os.path.exists(file_path):\\n            df = pd.read_feather(file_path)\\n            dataframes.append(df)\\n        else:\\n            print(f\"The file \\'{file_name}\\' in the \\'{category}\\' category does not exist.\")\\n    return pd.concat(dataframes, ignore_index=True)\\n\\nroot_dir = \\'../data\\'\\ncategory = \\'health\\'\\n\\n# List of filenames to include in combined_df\\n#included_filenames = [\\'isot_dataset.feather\\', \\'fake_news_dataset.feather\\', \\'pheme.feather\\', \\'liar_dataset.feather\\', \\'politifact_dataset.feather\\']\\nincluded_filenames = [\\'covid_claims.feather\\', \\'covid_fake_news_dataset.feather\\', \\'covid_FNIR.feather\\']\\ncombined_df = load_data_from_category(category, included_filenames)\\n\\n# Drop NaN values\\ncombined_df.dropna(inplace=True)\\n\\n# Print the resulting DataFrame\\nprint(combined_df)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def load_data_from_category(category, filenames):\n",
    "    dataframes = []\n",
    "    for file_name in filenames:\n",
    "        file_path = os.path.join(root_dir, category, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_feather(file_path)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"The file '{file_name}' in the '{category}' category does not exist.\")\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "root_dir = '../data'\n",
    "category = 'health'\n",
    "\n",
    "# List of filenames to include in combined_df\n",
    "#included_filenames = ['isot_dataset.feather', 'fake_news_dataset.feather', 'pheme.feather', 'liar_dataset.feather', 'politifact_dataset.feather']\n",
    "included_filenames = ['covid_claims.feather', 'covid_fake_news_dataset.feather', 'covid_FNIR.feather']\n",
    "combined_df = load_data_from_category(category, included_filenames)\n",
    "\n",
    "# Drop NaN values\n",
    "combined_df.dropna(inplace=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(combined_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Global warming is driving polar bears toward extinction'\n",
      " 'The sun has gone into ‘lockdown’ which could cause freezing weather, earthquakes and famine, say scientists'\n",
      " 'They tell us that we are the primary forces controlling earth temperatures by the burning of fossil fuels and releasing their carbon dioxide.'\n",
      " 'The Great Barrier Reef is experiencing the most widespread bleaching ever recorded'\n",
      " 'Volcanoes Melting West Antarctic Glaciers, Not Global Warming']\n",
      "[0 0 0 0 0]\n",
      "Number of unique classes: 2\n",
      "Class 0: 654 instances\n",
      "Class 1: 253 instances\n"
     ]
    }
   ],
   "source": [
    "# May need to include more columns to process metadata\n",
    "if train_data is None and test_data is None:\n",
    "  texts = combined_df['text'].values\n",
    "  labels = combined_df['label'].values\n",
    "  num_classes = combined_df['label'].nunique()\n",
    "  print(texts[:5])\n",
    "  print(labels[:5])\n",
    "\n",
    "  unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "if train_data != None and test_data != None:\n",
    "  train_texts = train_df['text'].values\n",
    "  train_labels = train_df['label'].values\n",
    "  test_texts = test_df['text'].values\n",
    "  test_labels = test_df['label'].values\n",
    "  num_classes = max(train_df['label'].nunique(), test_df['label'].nunique())\n",
    "  print(train_texts[:5])\n",
    "  print(train_labels[:5])\n",
    "  print(test_texts[:5])\n",
    "  print(test_labels[:5])\n",
    "\n",
    "  unique_classes, class_counts = np.unique(train_labels, return_counts=True)\n",
    "\n",
    "for class_label, count in zip(unique_classes, class_counts):\n",
    "  print(f\"Class {class_label}: {count} instances\")\n",
    "\n",
    "print(f\"Number of unique classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "if train_data is None and test_data is None:\n",
    "  train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=7623)\n",
    "  print(\"Split category data into training and testing sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "balancing_strategy = None\n",
    "\n",
    "if weight_for_class_0 is None and weight_for_class_1 is None:\n",
    "  balancing_strategy = 'Default weights'\n",
    "  # Calculate class frequencies\n",
    "  class_counts = np.bincount(labels) if train_data is None else np.bincount(train_labels)\n",
    "  total_samples = np.sum(class_counts)\n",
    "\n",
    "  # Calculate class weights, the original distribution of weights\n",
    "  weight_for_class_0 = class_counts[0] / sum(class_counts)\n",
    "  weight_for_class_1 = class_counts[1] / sum(class_counts)\n",
    "elif weight_for_class_0 == 'auto' and weight_for_class_1 == 'auto':\n",
    "  balancing_strategy = 'Auto weights'\n",
    "  # Calculate class frequencies\n",
    "  class_counts = np.bincount(labels) if train_data is None else np.bincount(train_labels)\n",
    "  total_samples = np.sum(class_counts)\n",
    "\n",
    "  # Calculate class weights\n",
    "  weight_for_class_0 = class_counts[1] / sum(class_counts)\n",
    "  weight_for_class_1 = class_counts[0] / sum(class_counts)\n",
    "else:\n",
    "  balancing_strategy = 'Manual weights'\n",
    "\n",
    "print(f\"Weight for class 0: {weight_for_class_0:.2f}\")\n",
    "print(f\"Weight for class 1: {weight_for_class_1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT (or BERT variation) model and tokenizer\n",
    "model_mapping = {\n",
    "    0: (BertTokenizer, BertForSequenceClassification, 'bert-base-uncased'),\n",
    "    1: (BertTokenizer, BertForSequenceClassification, 'bert-base-cased'),\n",
    "    2: (DistilBertTokenizer, DistilBertForSequenceClassification, 'distilbert-base-uncased-finetuned-sst-2-english'),\n",
    "    3: (DistilBertTokenizer, DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
    "    4: (RobertaTokenizer, RobertaForSequenceClassification, 'roberta-base'),\n",
    "    5: (AlbertTokenizer, AlbertForSequenceClassification, 'albert-base-v2')\n",
    "}\n",
    "\n",
    "tokenizer_class, model_class, model_name = model_mapping.get(select_model, (None, None, None))\n",
    "\n",
    "if tokenizer_class and model_class and model_name:\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name, num_labels=2)\n",
    "else:\n",
    "    logging.error(f\"Invalid model selection: {select_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers up to the specified layer\n",
    "if freeze_layers_up_to > 0:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if select_model == 0 or select_model == 1:\n",
    "        print(\"Layers: \"+str(len(model.bert.encoder.layer)))\n",
    "        for param in model.bert.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif select_model == 2 or select_model == 3:\n",
    "        print(\"Layers: \"+str(len(model.distilbert.transformer.layer)))\n",
    "        for param in model.distilbert.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif select_model == 4:\n",
    "        print(\"Layers: \"+str(len(model.roberta.encoder.layer)))\n",
    "        for param in model.roberta.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif select_model == 5:\n",
    "        print(\"Layers: \"+str(len(model.albert.encoder.albert_layer_groups)))\n",
    "        for param in model.albert.embeddings.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encodings to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create a DataLoader for training and testing\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average train loss: 0.0\n",
      "Epoch 2 average train loss: 0.0\n",
      "Epoch 3 average train loss: 0.0\n",
      "Epoch 4 average train loss: 0.0\n",
      "Epoch 5 average train loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Move the model and data to the GPU\n",
    "model.to(device)\n",
    "train_inputs, train_masks, train_labels = train_inputs.to(device), train_masks.to(device), train_labels.to(device)\n",
    "test_inputs, test_masks, test_labels = test_inputs.to(device), test_masks.to(device), test_labels.to(device)\n",
    "\n",
    "# Define class weights based on class imbalance\n",
    "class_weights = torch.tensor([weight_for_class_0, weight_for_class_1], dtype=torch.float)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device))\n",
    "\n",
    "# Define the optimizer with a learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Model training code\n",
    "def train_model(model, epochs, optimizer, loss_fn, train_dataloader):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "        logging.info(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "\n",
    "# Fine-tune the pre-trained BERT model\n",
    "train_start_time = time.time()\n",
    "train_model(model, epochs, optimizer, loss_fn, train_dataloader)\n",
    "train_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_start_time = time.time()\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': None}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "eval_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "precision = precision_score(test_labels.cpu(), predictions)\n",
    "recall = recall_score(test_labels.cpu(), predictions)\n",
    "f1 = f1_score(test_labels.cpu(), predictions)\n",
    "accuracy = accuracy_score(test_labels.cpu(), predictions)\n",
    "g_mean = (recall*accuracy)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "saved_model_name = f\"bert_model_{run_id}.pt\"\n",
    "model_path = os.path.join(save_dir, saved_model_name)\n",
    "\n",
    "# Only save model if accuracy meets minimum threshold\n",
    "if accuracy > min_acc:\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Training time: 31.90778636932373 seconds\n",
      "Inference time: 0.4577946662902832 seconds\n",
      "Precision: 0.5\n",
      "Recall: 0.4772727272727273\n",
      "F-score: 0.48837209302325585\n",
      "Accuracy: 0.7582417582417582\n",
      "G-mean: 0.6015713689065595\n"
     ]
    }
   ],
   "source": [
    "# Log the results\n",
    "logging.info(\"Evaluation Results\")\n",
    "logging.info(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "logging.info(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F-score: {f1}\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"G-mean: {g_mean}\")\n",
    "logging.info(\"Additional Info\")\n",
    "logging.info(f\"Model name: {model_name}\")\n",
    "if not train_data_set:\n",
    "  logging.info(f\"Datasets list: {categories}\")\n",
    "  logging.info(f\"Dataset size: {datasets_size}\")\n",
    "else:\n",
    "  logging.info(f\"Datasets list: {categories} Train: {train_dataset_name}, Test: {test_dataset_name}\")\n",
    "  logging.info(f\"Dataset size: Train: {train_dataset_size}, Test: {test_dataset_size}\")\n",
    "logging.info(f\"Layers frozen: {freeze_layers_up_to}\")\n",
    "logging.info(f\"Learning rate: {learning_rate}\")\n",
    "logging.info(f\"Class weights: {balancing_strategy} - {[weight_for_class_0, weight_for_class_1]}\")\n",
    "if accuracy > min_acc:\n",
    "  logging.info(f\"Model saved to: {model_path}\")\n",
    "else:\n",
    "  logging.info(\"Model not saved, didn't meet minimum accuracy threshold\")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "print(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"G-mean: {g_mean}\")\n",
    "print(f\"Epoch: {epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
