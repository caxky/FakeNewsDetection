{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5db550c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:11.398251Z",
     "iopub.status.busy": "2023-12-22T14:56:11.397923Z",
     "iopub.status.idle": "2023-12-22T14:56:13.064041Z",
     "shell.execute_reply": "2023-12-22T14:56:13.063471Z"
    },
    "papermill": {
     "duration": 1.670705,
     "end_time": "2023-12-22T14:56:13.065245",
     "exception": false,
     "start_time": "2023-12-22T14:56:11.394540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/kyle/.local/lib/python3.8/site-packages (1.24.2)\r\n",
      "Requirement already satisfied: transformers in /home/kyle/.local/lib/python3.8/site-packages (4.34.0)\r\n",
      "Requirement already satisfied: pandas in /home/kyle/.local/lib/python3.8/site-packages (1.5.3)\r\n",
      "Requirement already satisfied: torch in /home/kyle/.local/lib/python3.8/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/kyle/.local/lib/python3.8/site-packages (1.3.1)\r\n",
      "Requirement already satisfied: pyarrow in /home/kyle/.local/lib/python3.8/site-packages (14.0.0)\r\n",
      "Requirement already satisfied: accelerate in /home/kyle/.local/lib/python3.8/site-packages (0.24.1)\r\n",
      "Requirement already satisfied: ipywidgets in /home/kyle/.local/lib/python3.8/site-packages (8.1.1)\r\n",
      "Requirement already satisfied: tqdm in /home/kyle/.local/lib/python3.8/site-packages (4.64.1)\r\n",
      "Requirement already satisfied: datetime in /home/kyle/.local/lib/python3.8/site-packages (5.3)\r\n",
      "Requirement already satisfied: imblearn in /home/kyle/.local/lib/python3.8/site-packages (0.0)\r\n",
      "Requirement already satisfied: sentencepiece in /home/kyle/.local/lib/python3.8/site-packages (0.1.99)\r\n",
      "Requirement already satisfied: papermill in /home/kyle/.local/lib/python3.8/site-packages (2.5.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /home/kyle/.local/lib/python3.8/site-packages (from transformers) (3.12.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/kyle/.local/lib/python3.8/site-packages (from transformers) (0.17.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kyle/.local/lib/python3.8/site-packages (from transformers) (23.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kyle/.local/lib/python3.8/site-packages (from transformers) (2022.10.31)\r\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\r\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/kyle/.local/lib/python3.8/site-packages (from transformers) (0.14.1)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/kyle/.local/lib/python3.8/site-packages (from transformers) (0.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/kyle/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kyle/.local/lib/python3.8/site-packages (from pandas) (2022.7.1)\r\n",
      "Requirement already satisfied: typing-extensions in /home/kyle/.local/lib/python3.8/site-packages (from torch) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /home/kyle/.local/lib/python3.8/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /home/kyle/.local/lib/python3.8/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (2.10.1)\r\n",
      "Requirement already satisfied: fsspec in /home/kyle/.local/lib/python3.8/site-packages (from torch) (2023.9.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (12.1.3.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (2.18.1)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.1.0 in /home/kyle/.local/lib/python3.8/site-packages (from torch) (2.1.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/kyle/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.2.140)\r\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/kyle/.local/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/kyle/.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kyle/.local/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /home/kyle/.local/lib/python3.8/site-packages (from accelerate) (5.9.4)\r\n",
      "Requirement already satisfied: protobuf in /home/kyle/.local/lib/python3.8/site-packages (from transformers[sentencepiece]) (4.25.1)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/kyle/.local/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/kyle/.local/lib/python3.8/site-packages (from ipywidgets) (8.10.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/kyle/.local/lib/python3.8/site-packages (from ipywidgets) (5.9.0)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /home/kyle/.local/lib/python3.8/site-packages (from ipywidgets) (4.0.9)\r\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /home/kyle/.local/lib/python3.8/site-packages (from ipywidgets) (3.0.9)\r\n",
      "Requirement already satisfied: zope.interface in /usr/lib/python3/dist-packages (from datetime) (4.7.1)\r\n",
      "Requirement already satisfied: imbalanced-learn in /home/kyle/.local/lib/python3.8/site-packages (from imblearn) (0.11.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click in /home/kyle/.local/lib/python3.8/site-packages (from papermill) (8.1.7)\r\n",
      "Requirement already satisfied: nbformat>=5.1.2 in /home/kyle/.local/lib/python3.8/site-packages (from papermill) (5.9.2)\r\n",
      "Requirement already satisfied: nbclient>=0.2.0 in /home/kyle/.local/lib/python3.8/site-packages (from papermill) (0.9.0)\r\n",
      "Requirement already satisfied: entrypoints in /usr/lib/python3/dist-packages (from papermill) (0.3)\r\n",
      "Requirement already satisfied: tenacity>=5.0.2 in /home/kyle/.local/lib/python3.8/site-packages (from papermill) (8.2.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backcall in /home/kyle/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /home/kyle/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /home/kyle/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\r\n",
      "Requirement already satisfied: matplotlib-inline in /home/kyle/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\r\n",
      "Requirement already satisfied: pickleshare in /home/kyle/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /home/kyle/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/kyle/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\r\n",
      "Requirement already satisfied: stack-data in /home/kyle/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /home/kyle/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/kyle/.local/lib/python3.8/site-packages (from nbclient>=0.2.0->papermill) (8.0.2)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/kyle/.local/lib/python3.8/site-packages (from nbclient>=0.2.0->papermill) (5.2.0)\r\n",
      "Requirement already satisfied: fastjsonschema in /home/kyle/.local/lib/python3.8/site-packages (from nbformat>=5.1.2->papermill) (2.19.0)\r\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/lib/python3/dist-packages (from nbformat>=5.1.2->papermill) (3.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath>=0.19 in /home/kyle/.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/kyle/.local/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /home/kyle/.local/lib/python3.8/site-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (6.0.0)\r\n",
      "Requirement already satisfied: pyzmq>=23.0 in /home/kyle/.local/lib/python3.8/site-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (25.0.0)\r\n",
      "Requirement already satisfied: tornado>=6.2 in /home/kyle/.local/lib/python3.8/site-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (6.2)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/kyle/.local/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill) (3.0.0)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/kyle/.local/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /home/kyle/.local/lib/python3.8/site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: executing>=1.2.0 in /home/kyle/.local/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/kyle/.local/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\r\n",
      "Requirement already satisfied: pure-eval in /home/kyle/.local/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /home/kyle/.local/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (3.17.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy transformers pandas torch scikit-learn pyarrow accelerate transformers[torch] transformers[sentencepiece] ipywidgets tqdm datetime imblearn sentencepiece papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df2e2fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:13.072585Z",
     "iopub.status.busy": "2023-12-22T14:56:13.072220Z",
     "iopub.status.idle": "2023-12-22T14:56:15.168316Z",
     "shell.execute_reply": "2023-12-22T14:56:15.167514Z"
    },
    "papermill": {
     "duration": 2.101463,
     "end_time": "2023-12-22T14:56:15.169952",
     "exception": false,
     "start_time": "2023-12-22T14:56:13.068489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import string\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, AlbertTokenizer, AlbertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d484fa",
   "metadata": {
    "papermill": {
     "duration": 0.002939,
     "end_time": "2023-12-22T14:56:15.176142",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.173203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a3fb11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.182095Z",
     "iopub.status.busy": "2023-12-22T14:56:15.181629Z",
     "iopub.status.idle": "2023-12-22T14:56:15.185342Z",
     "shell.execute_reply": "2023-12-22T14:56:15.184776Z"
    },
    "papermill": {
     "duration": 0.008031,
     "end_time": "2023-12-22T14:56:15.186405",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.178374",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Default parameters, Papermill will overwrite these\n",
    "categories = 'science'\n",
    "train_data = None\n",
    "test_data = None\n",
    "select_model = 0\n",
    "freeze_layers_up_to = 0\n",
    "epochs = 1\n",
    "weight_for_class_0 = None\n",
    "weight_for_class_1 = None\n",
    "learning_rate = 0.00001\n",
    "min_acc = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bda09f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.191962Z",
     "iopub.status.busy": "2023-12-22T14:56:15.191621Z",
     "iopub.status.idle": "2023-12-22T14:56:15.194784Z",
     "shell.execute_reply": "2023-12-22T14:56:15.194240Z"
    },
    "papermill": {
     "duration": 0.007012,
     "end_time": "2023-12-22T14:56:15.195821",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.188809",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "freeze_layers_up_to = 0\n",
    "epochs = 1\n",
    "weight_for_class_0 = \"auto\"\n",
    "weight_for_class_1 = \"auto\"\n",
    "learning_rate = 1e-05\n",
    "min_acc = 0.5\n",
    "select_model = 5\n",
    "categories = \"health\"\n",
    "train_data = None\n",
    "test_data = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61dd0969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.201133Z",
     "iopub.status.busy": "2023-12-22T14:56:15.200846Z",
     "iopub.status.idle": "2023-12-22T14:56:15.203986Z",
     "shell.execute_reply": "2023-12-22T14:56:15.203509Z"
    },
    "papermill": {
     "duration": 0.007244,
     "end_time": "2023-12-22T14:56:15.205182",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.197938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_random_string(length=10):\n",
    "    letters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "run_id = generate_random_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29199100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.210329Z",
     "iopub.status.busy": "2023-12-22T14:56:15.210012Z",
     "iopub.status.idle": "2023-12-22T14:56:15.213957Z",
     "shell.execute_reply": "2023-12-22T14:56:15.213441Z"
    },
    "papermill": {
     "duration": 0.007525,
     "end_time": "2023-12-22T14:56:15.214880",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.207355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir_mapping = {\n",
    "  'crime': './results/crime',\n",
    "  'science': './results/science',\n",
    "  'health': './results/health',\n",
    "  'politics': './results/politics',\n",
    "  'social_media': './results/social_media'\n",
    "}\n",
    "\n",
    "log_dir = log_dir_mapping.get(categories, './results')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "log_filename = f'training_log_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "log_filepath = os.path.join(log_dir, log_filename)\n",
    "\n",
    "logging.basicConfig(filename=log_filepath, filemode='w', format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logging.info(f'\\nStarting detection model - {run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8571d9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.220069Z",
     "iopub.status.busy": "2023-12-22T14:56:15.219810Z",
     "iopub.status.idle": "2023-12-22T14:56:15.299152Z",
     "shell.execute_reply": "2023-12-22T14:56:15.298558Z"
    },
    "papermill": {
     "duration": 0.08319,
     "end_time": "2023-12-22T14:56:15.300272",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.217082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined dataframe: \n",
      "                                                    text  label  \\\n",
      "0      Facebook has shuttered a popular group for Mic...      1   \n",
      "1      We can now officially put to rest all comparis...      0   \n",
      "2      The Ministry of Health in Bermuda has confirme...      0   \n",
      "3      Purdue University President Mitch Daniels, the...      1   \n",
      "4      Locking down much of the country may have help...      1   \n",
      "...                                                  ...    ...   \n",
      "13523  in early an outbreak of new coronavirus in chi...      0   \n",
      "13524  organizers of operation gridlock the first sig...      0   \n",
      "13525  on may as federal leaders debated how to respo...      0   \n",
      "13526  in march message started to circulate on faceb...      0   \n",
      "13527  on feb the beaverton published an article stat...      0   \n",
      "\n",
      "                                                   title  \\\n",
      "0      FACEBOOK DELETES MICHIGAN ANTI-LOCKDOWN GROUP ...   \n",
      "1       Other Viewpoints: COVID-19 is worse than the flu   \n",
      "2                   Bermuda's COVID-19 cases surpass 100   \n",
      "3      Purdue University says students face 'close to...   \n",
      "4      THE HIGH COST OF LOCKING DOWN AMERICA: “WE’VE ...   \n",
      "...                                                  ...   \n",
      "13523  Did Chinese Doctors Confirm African People Are...   \n",
      "13524  Anti-Lockdown Protests Originated With Tight-K...   \n",
      "13525  Does Pelosi Want ‘Guaranteed Minimum Incomes’ ...   \n",
      "13526  Did MSNBC Reporter Say ‘I Hope Coronavirus Kil...   \n",
      "13527  Did VP Mike Pence Introduce ‘Conversion Therap...   \n",
      "\n",
      "                                                metadata  \n",
      "0              {'label': 0, 'subcategory': 'false news'}  \n",
      "1                    {'label': 1, 'subcategory': 'true'}  \n",
      "2                    {'label': 1, 'subcategory': 'true'}  \n",
      "3         {'label': 0, 'subcategory': 'partially false'}  \n",
      "4              {'label': 0, 'subcategory': 'false news'}  \n",
      "...                                                  ...  \n",
      "13523  {'URL': 'https://www.snopes.com/fact-check/cor...  \n",
      "13524  {'URL': 'https://www.snopes.com/news/2020/05/2...  \n",
      "13525  {'URL': 'https://www.snopes.com/fact-check/pel...  \n",
      "13526  {'URL': 'https://www.snopes.com/fact-check/msn...  \n",
      "13527  {'URL': 'https://www.snopes.com/fact-check/pen...  \n",
      "\n",
      "[5932 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "root_dir = '../data'\n",
    "\n",
    "dataset_mapping = {\n",
    "  'fa-kes': '../data/crime/FA-KES-Dataset.feather',\n",
    "  'snope': '../data/crime/snope.feather',\n",
    "  'covid_claims': '../data/health/covid_claims.feather',\n",
    "  'covid_fake_news': '../data/health/covid_fake_news_dataset.feather',\n",
    "  'covid_FNIR': '../data/health/covid_FNIR.feather',\n",
    "  'fake_news': '../data/politics/fake_news_dataset.feather',\n",
    "  'isot_dataset': '../data/politics/isot_dataset.feather',\n",
    "  'liar_dataset': '../data/politics/liar_dataset.feather',\n",
    "  'pheme': '../data/politics/pheme.feather',\n",
    "  'politifact': '../data/politics/politifact_dataset.feather',\n",
    "  'climate': '../data/science/climate_dataset.feather',\n",
    "  'gossipcop': '../data/social_media/gossipcop.feather',\n",
    "  'isot_social': '../data/social_media/isot_dataset.feather',\n",
    "  'isot_multipurpose': '../data/isot_multipurpose_small.feather'\n",
    "}\n",
    "\n",
    "def load_data_from_category(category):\n",
    "    files = os.listdir(os.path.join(root_dir, category))\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        if file.endswith('.feather'):\n",
    "            df = pd.read_feather(os.path.join(root_dir, category, file))\n",
    "            dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "def load_as_train_or_test(dataset_list):\n",
    "    dataframes = []\n",
    "    for dataset in dataset_list:\n",
    "        df = pd.read_feather(dataset_mapping.get(dataset))\n",
    "        dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Use this one when running multiple categories\n",
    "#combined_dataframes = [load_data_from_category(category) for category in categories] \n",
    "# This one is for single category\n",
    "if train_data is None and test_data is None:\n",
    "    train_data_set = False\n",
    "    combined_dataframes = [load_data_from_category(categories)]\n",
    "    combined_df = pd.concat(combined_dataframes, ignore_index=True)\n",
    "    combined_df.dropna(inplace=True)\n",
    "    datasets_size = len(combined_df)\n",
    "    print(\"combined dataframe: \")\n",
    "    print(combined_df)\n",
    "\n",
    "# This one is when specific training and testing sets are used\n",
    "if train_data != None and test_data != None:\n",
    "    train_data_set = True\n",
    "    train_dataset_name, test_dataset_name = train_data, test_data\n",
    "    train_df = load_as_train_or_test(train_data)\n",
    "    test_df = load_as_train_or_test(test_data)\n",
    "    train_df.dropna(inplace=True)\n",
    "    test_df.dropna(inplace=True)\n",
    "    train_dataset_size, test_dataset_size = len(train_df), len(test_df)\n",
    "    print(\"training dataframe: \")\n",
    "    print(train_df.head())\n",
    "    print(\"testing dataframe: \")\n",
    "    print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69733cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.307773Z",
     "iopub.status.busy": "2023-12-22T14:56:15.307375Z",
     "iopub.status.idle": "2023-12-22T14:56:15.312997Z",
     "shell.execute_reply": "2023-12-22T14:56:15.312448Z"
    },
    "papermill": {
     "duration": 0.010674,
     "end_time": "2023-12-22T14:56:15.314334",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.303660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_data_from_category(category, filenames):\\n    dataframes = []\\n    for file_name in filenames:\\n        file_path = os.path.join(root_dir, category, file_name)\\n        if os.path.exists(file_path):\\n            df = pd.read_feather(file_path)\\n            dataframes.append(df)\\n        else:\\n            print(f\"The file \\'{file_name}\\' in the \\'{category}\\' category does not exist.\")\\n    return pd.concat(dataframes, ignore_index=True)\\n\\nroot_dir = \\'../data\\'\\ncategory = \\'health\\'\\n\\n# List of filenames to include in combined_df\\n#included_filenames = [\\'isot_dataset.feather\\', \\'fake_news_dataset.feather\\', \\'pheme.feather\\', \\'liar_dataset.feather\\', \\'politifact_dataset.feather\\']\\nincluded_filenames = [\\'covid_claims.feather\\', \\'covid_fake_news_dataset.feather\\', \\'covid_FNIR.feather\\']\\ncombined_df = load_data_from_category(category, included_filenames)\\n\\n# Drop NaN values\\ncombined_df.dropna(inplace=True)\\n\\n# Print the resulting DataFrame\\nprint(combined_df)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def load_data_from_category(category, filenames):\n",
    "    dataframes = []\n",
    "    for file_name in filenames:\n",
    "        file_path = os.path.join(root_dir, category, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_feather(file_path)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"The file '{file_name}' in the '{category}' category does not exist.\")\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "root_dir = '../data'\n",
    "category = 'health'\n",
    "\n",
    "# List of filenames to include in combined_df\n",
    "#included_filenames = ['isot_dataset.feather', 'fake_news_dataset.feather', 'pheme.feather', 'liar_dataset.feather', 'politifact_dataset.feather']\n",
    "included_filenames = ['covid_claims.feather', 'covid_fake_news_dataset.feather', 'covid_FNIR.feather']\n",
    "combined_df = load_data_from_category(category, included_filenames)\n",
    "\n",
    "# Drop NaN values\n",
    "combined_df.dropna(inplace=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(combined_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c52c5c20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.320792Z",
     "iopub.status.busy": "2023-12-22T14:56:15.320450Z",
     "iopub.status.idle": "2023-12-22T14:56:15.326339Z",
     "shell.execute_reply": "2023-12-22T14:56:15.325792Z"
    },
    "papermill": {
     "duration": 0.01023,
     "end_time": "2023-12-22T14:56:15.327256",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.317026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook has shuttered a popular group for Michiganders who oppose their governor’s extreme lockdown measures, fueling debate about free speech during the coronavirus crisis.\\n\\n‘Michiganders Against Excessive Quarantine’ had more than 380,000 members, with media reports describing it as one of the largest anti-lockdown groups in the nation. It’s not clear if Facebook has provided an explanation for the group’s removal.\\n\\nVisiting the page gives the following message: “Sorry, this content isn’t available right now.”\\n\\nGarrett Soldano of Mattawan, Michigan created Michiganders Against Excessive Quarantine in April after becoming disenchanted with Governor Gretchen Whitmer’s stay-at-home order – a policy which he initially supported.\\n\\nThe group made headlines around the world after organizing ‘Operation Gridlock’, a protest in which thousands of cars created bumper-to-bumper traffic in the capital, Lansing.\\n\\nProtesters were encouraged to stay in their automobiles and adhere to social distancing guidelines. However, several hundred demonstrators ended up rallying outside the state house.\\n\\nMany members of the group used the space to talk about the depression and financial hardships that they were experiencing as a result of the governor’s executive orders, first enacted in March 23, which have forced all ‘non-essential’ businesses and to remain closed. Whitmer unilaterally extended her emergency powers in May, after the legislature voted to let them expire. The governor has used these executive powers as the legal basis for the state’s lockdown.\\n\\n“They will not silence us,” Soldano said in a video message after the group was deleted.\\n\\n“This just proves to all of us out there that we were right. That there is a deep agenda going on. They’re trying to keep us quiet… Now we fight back. And we have the numbers.”\\n\\nFor now, the message is unavailable on Facebook. On Wednesday, Soldano began promoting a new page, Stand Up Michigan, which gained thousands of members before also being pulled from the site.\\n\\nThe removal of Soldan’s anti-lockdown group has not gone unnoticed on social media. Former New York Times reporter Alex Berenson described the decision to pull the page as a clear example of “censorship.”\\n\\nSome challenged the notion, apparently arguing that the group’s beliefs were not worthy of being protected. Even Elon Musk, who re-opened his Tesla plant in California in defiance of local officials, weighed in. Last month, Facebook deleted anti-lockdown pages that were organizing demonstrations in California, Nebraska, and New Jersey, claiming that the groups were in violation of those states’ stay-at-home orders.\\n\\nMeanwhile, the tech giant announced that it would be working with fact-checkers to “stop the spread of misinformation and harmful content about Covid-19.” An update regarding the policy, dated May 12, states that the platform put warning labels on around 50 million “pieces of content related to Covid-19.” '\n",
      " 'We can now officially put to rest all comparisons of COVID-19 to influenza. Coronavirus is killing more Americans than the flu and almost everything else. According to data compiled by the Washington Post, COVID-19 is now the second-leading cause of death in America.\\nThe upward march of these numbers is steady and disturbing. The week of March 16, COVID-19 didn’t rank among the 15 most common causes of death. The next week, it ranked seventh. The following week, it moved up to third. The week of April 6, COVID-19 killed more people than anything besides heart disease.\\nThese numbers are a tragic milestone, and they’re also a verdict on attempts to downplay the severity of this event. Messaging from political leaders and some media voices early in this outbreak comparing it to the flu were wrong. They might have been well-intentioned efforts to keep people from panicking, but that point is moot now.\\nCoronavirus is the most formidable public health threat in a generation. The time for minimizing is over. The time for aggressive testing, social distancing and (hopefully soon) treatment is now.'\n",
      " 'The Ministry of Health in Bermuda has confirmed a newspaper report that 10 residents from the Westmeath Residential and Nursing Home had tested positive for COVID-19, taking the island’s total confirmed cases past the 100 mark.\\nA further 213 of the 1,173 tests that have been carried out, were received from across Bermuda yesterday and today, all of which were negative, the ministry said on Saturday night.\\nThese included 74 essential workers, who received negative results at the new drive-through testing facility at Southside, near the international airport, which remains closed to regular commercial flights.\\nBermuda is under lockdown until May 2, although more businesses are being allowed to reopen on a limited basis.\\nThe island has a total of 109 confirmed cases, of which 65 are active.\\nEleven people are in hospital, 54 are being actively monitored and 39 have recovered.\\nFive people have died.'\n",
      " 'Purdue University President Mitch Daniels, the former governor of Indiana, said in an April 21 letter he intends to open the West Lafayette, Indiana university for in-person classes in the fall, citing an almost “zero lethal threat” to young people from COVID-19.\\n“At least 80% of our population is made up of young people, say, 35 and under,” Daniels said of Purdue’s makeup. “All data to date tell us that the COVID-19 virus, while it transmits rapidly in this age group, poses close to zero lethal threat to them.”\\nDaniels said he estimated about 20% of the university’s population was over 35 years old and contained a “significant number of people with diabetes, asthma, hypertension, and other ailments.”\\n“We will consider new policies and practices that keep these groups separate, or minimize contact between them. Literally, our students pose a far greater danger to others than the virus poses to them,” Daniels said.\\nWhile senior adults and individuals with underlying health conditions are most at risk for the most serious symptoms and side effects of COVID-19, young people can still contract – and die – from the disease.\\nThe World Health Organization in March warned young people that they weren’t invincible to the serious effects of COVID-19, and could be hospitalized and killed by the disease caused by the novel coronavirus. Last week, doctors in New York reported instances of strokes of large-vessel strokes experienced by young people with only mild symptoms of the virus.\\nYoung people can also be asymptomatic carriers, spreading the virus without even realizing they have contracted the novel coronavirus. Anthony Fauci, director of the National Institute of Allergy and Infectious Disease, said earlier in April “ somewhere between 25% and 50%” of people infected with the virus may never show symptoms or become sick – but can still transmit the illness to others.\\nDaniels said Purdue would consider the “pre-testing” of students and faculty before August and implementing a “robust testing system during the school year.” He also said people over the age of 35 and people who have health conditions that put them at risk could either be asked or ordered to work from home.\\nClasses could be spread across days and times to reduce their size, Daniels said, adding Purdue could adopt more online instruction for students living on campus.\\nHe said the university expected “to be able to trace proximate and/or frequent contacts of those who test positive” and vulnerable people who had contact with an infected person would be ordered to self-quarantine for a two-week period.\\nThose who come in contact with an infected person but do not test positive for the virus would be monitored for symptoms as well.\\n“These concepts are preliminary, intended mainly to illustrate an overall, data-driven and research-based strategy, and to invite suggestions for their modification or exclusion in favor of better actions,” Daniels wrote.\\nThe move comes as colleges and universities, who abruptly kicked students off of campus and moved instruction online earlier this year, grapple with their plans for the future as health experts warn a vaccine for COVID-19 might not arrive before 2022.'\n",
      " 'Locking down much of the country may have helped to “flatten the curve” a bit, but it has also had some very serious consequences that public health officials did not anticipate. \\n\\nHumans were created to be social creatures, and forcing us to isolate ourselves from one another for weeks on end has turned out to be quite problematic.\\n\\nThis has especially been true for those that live alone.\\n\\nToday, the U.S. has a higher percentage of one person households than ever before, and keeping those individuals totally isolated in their own homes is not that different from putting prison inmates in solitary confinement for an extended period of time. \\n\\nIn both cases, it can be just a matter of time before people mentally break. The High Cost Of Locking Down America: “We’ve Seen A Year’s Worth Of Suicide Attempts In The Last Four Weeks”\\nIMAGE CREDITS: GETTY IMAGES / JUSTIN PAGET.\\n\\nLocking down much of the country may have helped to “flatten the curve” a bit, but it has also had some very serious consequences that public health officials did not anticipate. \\n\\nHumans were created to be social creatures, and forcing us to isolate ourselves from one another for weeks on end has turned out to be quite problematic.\\n\\nThis has especially been true for those that live alone.\\n\\nWatching endless hours of television is one of the worst possible things that you can do for your mental health.\\n\\nIf you put garbage into your mind, you are going to get garbage out.\\n\\nAnd allowing the corporate media elite to endlessly feed hour after hour of “programming” directly into your mind is incredibly self-destructive.\\n\\nIn addition to binge-watching television, many Americans have also been dealing with this crisis by turning to drugs and alcohol…Today, the U.S. has a higher percentage of one person households than ever before, and keeping those individuals totally isolated in their own homes is not that different from putting prison inmates in solitary confinement for an extended period of time. \\n\\nIn both cases, it can be just a matter of time before people mentally break.\\n\\n\\nAs thousands of Americans pour onto the now open beaches of America. Most Americans are waking up to the undeniable fact that they were lead astray. Coronavirus numbers were skewed. Authoritarian politicians abused their power. And the united front never materialized. The media’s lock down narrative has gradually become back chatter to a nation burned out on the globalist deception.\\n\\nEven before this pandemic came along, the suicide rate in the United States was already soaring, and now the number of suicides is spiking like we have never seen before.  Just consider what Dr. Mike deBoisblanc of John Muir Medical Center in Walnut Creek, California says is happening in his area. Needless to say, this is more self-destructive behavior that is just going to fuel even more anxiety, depression and despair.\\n\\nYes, this pandemic has been bad.  There have been more than 1.7 million confirmed cases of COVID-19 in the United States so far, and about 100,000 Americans have died.\\n\\nBut this isn’t the worst thing that we are going to face.  In fact, it isn’t even close.\\n\\nSo if we can’t handle what is happening right now, how in the world are we going to be able to handle what is coming?\\n\\nThe good news is that this pandemic is causing a lot of people to wake up and reflect on what really matters.\\n\\nPrior to COVID-19, a lot of Americans were living for their careers.  But this crisis has demonstrated how rapidly those jobs can disappear.  There is absolutely no loyalty in the corporate world today, and most corporations will not hesitate to lay off workers once they start bleeding money.  At this point, more than 38 million Americans have already lost their jobs, and more job losses are coming in the weeks and months ahead.\\n\\nOther Americans were living for the pleasures of this life before this pandemic came along.  But now bars, clubs and movie theaters all over the country have been shut down, professional sports leagues have been paralyzed, and this virus has even transformed dating into a hazard that many people simply don’t want to deal with.\\n\\nSo many of the things that we once valued so highly have been taken away, and maybe that is precisely what we needed.  Wrestling legend Hulk Hogan created quite an uproar when he made this point very strongly on Instagram… The High Cost Of Locking Down America: “We’ve Seen A Year’s Worth Of Suicide Attempts In The Last Four Weeks”\\nIMAGE CREDITS: GETTY IMAGES / JUSTIN PAGET.\\n\\nLocking down much of the country may have helped to “flatten the curve” a bit, but it has also had some very serious consequences that public health officials did not anticipate. \\n\\nHumans were created to be social creatures, and forcing us to isolate ourselves from one another for weeks on end has turned out to be quite problematic.\\n\\nThis has especially been true for those that live alone.\\n\\nToday, the U.S. has a higher percentage of one person households than ever before, and keeping those individuals totally isolated in their own homes is not that different from putting prison inmates in solitary confinement for an extended period of time. \\n\\nIn both cases, it can be just a matter of time before people mentally break.\\n\\n\\nAs thousands of Americans pour onto the now open beaches of America. Most Americans are waking up to the undeniable fact that they were lead astray. Coronavirus numbers were skewed. Authoritarian politicians abused their power. And the united front never materialized. The media’s lock down narrative has gradually become back chatter to a nation burned out on the globalist deception.\\n\\nEven before this pandemic came along, the suicide rate in the United States was already soaring, and now the number of suicides is spiking like we have never seen before.  Just consider what Dr. Mike deBoisblanc of John Muir Medical Center in Walnut Creek, California says is happening in his area…\\n\\n“We’ve never seen numbers like this, in such a short period of time,” he said. “I mean we’ve seen a year’s worth of suicide attempts in the last four weeks.”\\n\\nAnd suicide helplines all over America are seeing an enormous increase in the number of people calling in…\\n\\nMany crisis centers are reporting 30% to 40% increases in the number of people seeking help. The helpline at Provident is experiencing a tenfold increase compared with this time last year, when no national disaster was occurring.\\n\\nIn addition to being isolated in their own homes, Americans have also been bombarded by endless coverage of this pandemic by the mainstream media, and this has definitely helped to fuel a lot of anxiety…\\n\\nIn all, 66% are watching and consuming more news than ever before, which is particularly puzzling considering that 68% also admitted that COVID-19 coverage gives them considerable anxiety. It’s not just anxiety either; 65% said they feel overwhelmed by coronavirus news, 56% just get plain angry, 67% feel burnt out, 59% experience fearfulness, and 50% can’t help but feel hopeless after hearing the nightly news.\\n\\nAnd of course Americans are not just watching more news than ever before.  According to Comcast, the average U.S. household has actually been watching 66 hours of TV a week during this pandemic…\\n\\nNew research from telecomms firm Comcast has revealed viewing time has soared as a result, with households watching an extra eight-plus hours of TV each week – equivalent to the time spent on a full days’ work shift.\\n\\nThis means the average household now watches 66 hours of TV a week, up from 57 hours a week in early March.\\n\\nWatching endless hours of television is one of the worst possible things that you can do for your mental health.\\n\\nIf you put garbage into your mind, you are going to get garbage out.\\n\\nAnd allowing the corporate media elite to endlessly feed hour after hour of “programming” directly into your mind is incredibly self-destructive.\\n\\nIn addition to binge-watching television, many Americans have also been dealing with this crisis by turning to drugs and alcohol…\\n\\nFindings by The Recovery Village, a Florida-based network of addiction treatment facilities, reflect an “expected” increase in substance use during the pandemic, with Americans reporting a 55% rise in alcohol consumption in the last month. When it came to illicit drugs, 36% of Americans reported increased use of marijuana and prescription opioids, among others.\\n\\nNeedless to say, this is more self-destructive behavior that is just going to fuel even more anxiety, depression and despair.\\n\\nYes, this pandemic has been bad.  There have been more than 1.7 million confirmed cases of COVID-19 in the United States so far, and about 100,000 Americans have died.\\n\\nBut this isn’t the worst thing that we are going to face.  In fact, it isn’t even close.\\n\\nSo if we can’t handle what is happening right now, how in the world are we going to be able to handle what is coming?\\n\\nThe good news is that this pandemic is causing a lot of people to wake up and reflect on what really matters.\\n\\nPrior to COVID-19, a lot of Americans were living for their careers.  But this crisis has demonstrated how rapidly those jobs can disappear.  There is absolutely no loyalty in the corporate world today, and most corporations will not hesitate to lay off workers once they start bleeding money.  At this point, more than 38 million Americans have already lost their jobs, and more job losses are coming in the weeks and months ahead.\\n\\nOther Americans were living for the pleasures of this life before this pandemic came along.  But now bars, clubs and movie theaters all over the country have been shut down, professional sports leagues have been paralyzed, and this virus has even transformed dating into a hazard that many people simply don’t want to deal with.\\n\\nSo many of the things that we once valued so highly have been taken away, and maybe that is precisely what we needed.  Wrestling legend Hulk Hogan created quite an uproar when he made this point very strongly on Instagram…\\n\\n“Word up, can you handle the truth my brother only love HH,” wrote Hogan, who has 1.5 million followers on Instagram. “In three short months, just like He did with the plagues of Egypt, God has taken away everything we worship.\\n\\n“God said, ‘you want to worship athletes, I will shut down the stadiums. You want to worship musicians, I will shut down Civic Centers. You want to worship actors, I will shut down theaters. You want to worship money, I will shut down the economy and collapse the stock market. You don’t want to go to church and worship Me, I will make it where you can’t go to church.\\n\\n“‘If my people who are called by my name will humble themselves and pray and seek my face and turn from their wicked ways, then I will hear from heaven and will forgive their sin and will heal their land.”\\n\\nI couldn’t have said it better myself.  I wrote an entire book urging people to start focusing on what really matters, because true hope will not be found in the temporal things that the world is constantly chasing.\\n\\nAnd it has definitely been encouraging to hear that many young people have taken this crisis as an opportunity to start thinking about spiritual issues more…  The High Cost Of Locking Down America: “We’ve Seen A Year’s Worth Of Suicide Attempts In The Last Four Weeks”\\nIMAGE CREDITS: GETTY IMAGES / JUSTIN PAGET.\\n\\nLocking down much of the country may have helped to “flatten the curve” a bit, but it has also had some very serious consequences that public health officials did not anticipate. \\n\\nHumans were created to be social creatures, and forcing us to isolate ourselves from one another for weeks on end has turned out to be quite problematic.\\n\\nThis has especially been true for those that live alone.\\n\\nToday, the U.S. has a higher percentage of one person households than ever before, and keeping those individuals totally isolated in their own homes is not that different from putting prison inmates in solitary confinement for an extended period of time. \\n\\nIn both cases, it can be just a matter of time before people mentally break.\\n\\n\\nAs thousands of Americans pour onto the now open beaches of America. Most Americans are waking up to the undeniable fact that they were lead astray. Coronavirus numbers were skewed. Authoritarian politicians abused their power. And the united front never materialized. The media’s lock down narrative has gradually become back chatter to a nation burned out on the globalist deception.\\n\\nEven before this pandemic came along, the suicide rate in the United States was already soaring, and now the number of suicides is spiking like we have never seen before.  Just consider what Dr. Mike deBoisblanc of John Muir Medical Center in Walnut Creek, California says is happening in his area…\\n\\n“We’ve never seen numbers like this, in such a short period of time,” he said. “I mean we’ve seen a year’s worth of suicide attempts in the last four weeks.”\\n\\nAnd suicide helplines all over America are seeing an enormous increase in the number of people calling in…\\n\\nMany crisis centers are reporting 30% to 40% increases in the number of people seeking help. The helpline at Provident is experiencing a tenfold increase compared with this time last year, when no national disaster was occurring.\\n\\nIn addition to being isolated in their own homes, Americans have also been bombarded by endless coverage of this pandemic by the mainstream media, and this has definitely helped to fuel a lot of anxiety…\\n\\nIn all, 66% are watching and consuming more news than ever before, which is particularly puzzling considering that 68% also admitted that COVID-19 coverage gives them considerable anxiety. It’s not just anxiety either; 65% said they feel overwhelmed by coronavirus news, 56% just get plain angry, 67% feel burnt out, 59% experience fearfulness, and 50% can’t help but feel hopeless after hearing the nightly news.\\n\\nAnd of course Americans are not just watching more news than ever before.  According to Comcast, the average U.S. household has actually been watching 66 hours of TV a week during this pandemic…\\n\\nNew research from telecomms firm Comcast has revealed viewing time has soared as a result, with households watching an extra eight-plus hours of TV each week – equivalent to the time spent on a full days’ work shift.\\n\\nThis means the average household now watches 66 hours of TV a week, up from 57 hours a week in early March.\\n\\nWatching endless hours of television is one of the worst possible things that you can do for your mental health.\\n\\nIf you put garbage into your mind, you are going to get garbage out.\\n\\nAnd allowing the corporate media elite to endlessly feed hour after hour of “programming” directly into your mind is incredibly self-destructive.\\n\\nIn addition to binge-watching television, many Americans have also been dealing with this crisis by turning to drugs and alcohol…\\n\\nFindings by The Recovery Village, a Florida-based network of addiction treatment facilities, reflect an “expected” increase in substance use during the pandemic, with Americans reporting a 55% rise in alcohol consumption in the last month. When it came to illicit drugs, 36% of Americans reported increased use of marijuana and prescription opioids, among others.\\n\\nNeedless to say, this is more self-destructive behavior that is just going to fuel even more anxiety, depression and despair.\\n\\nYes, this pandemic has been bad.  There have been more than 1.7 million confirmed cases of COVID-19 in the United States so far, and about 100,000 Americans have died.\\n\\nBut this isn’t the worst thing that we are going to face.  In fact, it isn’t even close.\\n\\nSo if we can’t handle what is happening right now, how in the world are we going to be able to handle what is coming?\\n\\nThe good news is that this pandemic is causing a lot of people to wake up and reflect on what really matters.\\n\\nPrior to COVID-19, a lot of Americans were living for their careers.  But this crisis has demonstrated how rapidly those jobs can disappear.  There is absolutely no loyalty in the corporate world today, and most corporations will not hesitate to lay off workers once they start bleeding money.  At this point, more than 38 million Americans have already lost their jobs, and more job losses are coming in the weeks and months ahead.\\n\\nOther Americans were living for the pleasures of this life before this pandemic came along.  But now bars, clubs and movie theaters all over the country have been shut down, professional sports leagues have been paralyzed, and this virus has even transformed dating into a hazard that many people simply don’t want to deal with.\\n\\nSo many of the things that we once valued so highly have been taken away, and maybe that is precisely what we needed.  Wrestling legend Hulk Hogan created quite an uproar when he made this point very strongly on Instagram…\\n\\n“Word up, can you handle the truth my brother only love HH,” wrote Hogan, who has 1.5 million followers on Instagram. “In three short months, just like He did with the plagues of Egypt, God has taken away everything we worship.\\n\\n“God said, ‘you want to worship athletes, I will shut down the stadiums. You want to worship musicians, I will shut down Civic Centers. You want to worship actors, I will shut down theaters. You want to worship money, I will shut down the economy and collapse the stock market. You don’t want to go to church and worship Me, I will make it where you can’t go to church.\\n\\n“‘If my people who are called by my name will humble themselves and pray and seek my face and turn from their wicked ways, then I will hear from heaven and will forgive their sin and will heal their land.”\\n\\nI couldn’t have said it better myself.  I wrote an entire book urging people to start focusing on what really matters, because true hope will not be found in the temporal things that the world is constantly chasing.\\n\\nAnd it has definitely been encouraging to hear that many young people have taken this crisis as an opportunity to start thinking about spiritual issues more…\\n\\nA majority of teens and young adults say they are praying more or thinking about spiritual issues more than they were prior to the coronavirus pandemic, according to a new survey.\\n\\nThe poll of 800 high school students and 800 post-secondary students found that 67 percent said they’re either praying more often or thinking about spiritual issues more compared to the time before the virus.\\n\\nThere is hope, but it won’t be found on a television screen, at the bottom of a bottle or at the end of a needle.\\n\\nSo please don’t give up.  Despite all of the craziness in our world today, the best chapters of your life can still be ahead of you.\\n\\nBut if you keep feeding your mind endless hours of media “programming” and you keep chasing the things that they tell you to chase, all that is ahead of you is more pain, more anxiety, more depression and more despair.']\n",
      "[1 0 0 1 1]\n",
      "Class 0: 3284 instances\n",
      "Class 1: 2648 instances\n",
      "Number of unique classes: 2\n"
     ]
    }
   ],
   "source": [
    "# May need to include more columns to process metadata\n",
    "if train_data is None and test_data is None:\n",
    "  texts = combined_df['text'].values\n",
    "  labels = combined_df['label'].values\n",
    "  num_classes = combined_df['label'].nunique()\n",
    "  print(texts[:5])\n",
    "  print(labels[:5])\n",
    "\n",
    "  unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "if train_data != None and test_data != None:\n",
    "  train_texts = train_df['text'].values\n",
    "  train_labels = train_df['label'].values\n",
    "  test_texts = test_df['text'].values\n",
    "  test_labels = test_df['label'].values\n",
    "  num_classes = max(train_df['label'].nunique(), test_df['label'].nunique())\n",
    "  print(train_texts[:5])\n",
    "  print(train_labels[:5])\n",
    "  print(test_texts[:5])\n",
    "  print(test_labels[:5])\n",
    "\n",
    "  unique_classes, class_counts = np.unique(train_labels, return_counts=True)\n",
    "\n",
    "for class_label, count in zip(unique_classes, class_counts):\n",
    "  print(f\"Class {class_label}: {count} instances\")\n",
    "\n",
    "print(f\"Number of unique classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a527e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.333862Z",
     "iopub.status.busy": "2023-12-22T14:56:15.333475Z",
     "iopub.status.idle": "2023-12-22T14:56:15.337644Z",
     "shell.execute_reply": "2023-12-22T14:56:15.337039Z"
    },
    "papermill": {
     "duration": 0.008434,
     "end_time": "2023-12-22T14:56:15.338548",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.330114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split category data into training and testing sets\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "if train_data is None and test_data is None:\n",
    "  train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=7623)\n",
    "  print(\"Split category data into training and testing sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f315a620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.345078Z",
     "iopub.status.busy": "2023-12-22T14:56:15.344865Z",
     "iopub.status.idle": "2023-12-22T14:56:15.349385Z",
     "shell.execute_reply": "2023-12-22T14:56:15.348776Z"
    },
    "papermill": {
     "duration": 0.008995,
     "end_time": "2023-12-22T14:56:15.350534",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.341539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.45\n",
      "Weight for class 1: 0.55\n"
     ]
    }
   ],
   "source": [
    "balancing_strategy = None\n",
    "\n",
    "if weight_for_class_0 is None and weight_for_class_1 is None:\n",
    "  balancing_strategy = 'Default weights'\n",
    "  # Calculate class frequencies\n",
    "  class_counts = np.bincount(labels) if train_data is None else np.bincount(train_labels)\n",
    "  total_samples = np.sum(class_counts)\n",
    "\n",
    "  # Calculate class weights, the original distribution of weights\n",
    "  weight_for_class_0 = class_counts[0] / sum(class_counts)\n",
    "  weight_for_class_1 = class_counts[1] / sum(class_counts)\n",
    "elif weight_for_class_0 == 'auto' and weight_for_class_1 == 'auto':\n",
    "  balancing_strategy = 'Auto weights'\n",
    "  # Calculate class frequencies\n",
    "  class_counts = np.bincount(labels) if train_data is None else np.bincount(train_labels)\n",
    "  total_samples = np.sum(class_counts)\n",
    "\n",
    "  # Calculate class weights\n",
    "  weight_for_class_0 = class_counts[1] / sum(class_counts)\n",
    "  weight_for_class_1 = class_counts[0] / sum(class_counts)\n",
    "else:\n",
    "  balancing_strategy = 'Manual weights'\n",
    "\n",
    "print(f\"Weight for class 0: {weight_for_class_0:.2f}\")\n",
    "print(f\"Weight for class 1: {weight_for_class_1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e2f127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.358011Z",
     "iopub.status.busy": "2023-12-22T14:56:15.357525Z",
     "iopub.status.idle": "2023-12-22T14:56:15.790492Z",
     "shell.execute_reply": "2023-12-22T14:56:15.789748Z"
    },
    "papermill": {
     "duration": 0.438145,
     "end_time": "2023-12-22T14:56:15.791785",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.353640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT (or BERT variation) model and tokenizer\n",
    "model_mapping = {\n",
    "    0: (BertTokenizer, BertForSequenceClassification, 'bert-base-uncased'),\n",
    "    1: (BertTokenizer, BertForSequenceClassification, 'bert-base-cased'),\n",
    "    2: (DistilBertTokenizer, DistilBertForSequenceClassification, 'distilbert-base-uncased-finetuned-sst-2-english'),\n",
    "    3: (DistilBertTokenizer, DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
    "    4: (RobertaTokenizer, RobertaForSequenceClassification, 'roberta-base'),\n",
    "    5: (AlbertTokenizer, AlbertForSequenceClassification, 'albert-base-v2')\n",
    "}\n",
    "\n",
    "tokenizer_class, model_class, model_name = model_mapping.get(select_model, (None, None, None))\n",
    "\n",
    "if tokenizer_class and model_class and model_name:\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name, num_labels=2)\n",
    "else:\n",
    "    logging.error(f\"Invalid model selection: {select_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05062999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.800837Z",
     "iopub.status.busy": "2023-12-22T14:56:15.800224Z",
     "iopub.status.idle": "2023-12-22T14:56:15.804801Z",
     "shell.execute_reply": "2023-12-22T14:56:15.804164Z"
    },
    "papermill": {
     "duration": 0.010363,
     "end_time": "2023-12-22T14:56:15.806181",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.795818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Freeze the layers up to the specified layer\n",
    "if freeze_layers_up_to > 0:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if select_model == 0 or select_model == 1:\n",
    "        print(\"Layers: \"+str(len(model.bert.encoder.layer)))\n",
    "        for param in model.bert.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif select_model == 2 or select_model == 3:\n",
    "        print(\"Layers: \"+str(len(model.distilbert.transformer.layer)))\n",
    "        for param in model.distilbert.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif select_model == 4:\n",
    "        print(\"Layers: \"+str(len(model.roberta.encoder.layer)))\n",
    "        for param in model.roberta.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif select_model == 5:\n",
    "        print(\"Layers: \"+str(len(model.albert.encoder.albert_layer_groups)))\n",
    "        for param in model.albert.embeddings.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8295d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:15.813869Z",
     "iopub.status.busy": "2023-12-22T14:56:15.813504Z",
     "iopub.status.idle": "2023-12-22T14:56:28.638966Z",
     "shell.execute_reply": "2023-12-22T14:56:28.638348Z"
    },
    "papermill": {
     "duration": 12.831066,
     "end_time": "2023-12-22T14:56:28.640495",
     "exception": false,
     "start_time": "2023-12-22T14:56:15.809429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8f7226a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:28.648544Z",
     "iopub.status.busy": "2023-12-22T14:56:28.648240Z",
     "iopub.status.idle": "2023-12-22T14:56:29.038546Z",
     "shell.execute_reply": "2023-12-22T14:56:29.037843Z"
    },
    "papermill": {
     "duration": 0.395487,
     "end_time": "2023-12-22T14:56:29.039715",
     "exception": false,
     "start_time": "2023-12-22T14:56:28.644228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the encodings to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create a DataLoader for training and testing\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=8)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37a9504f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:29.047756Z",
     "iopub.status.busy": "2023-12-22T14:56:29.047445Z",
     "iopub.status.idle": "2023-12-22T14:56:29.192212Z",
     "shell.execute_reply": "2023-12-22T14:56:29.191647Z"
    },
    "papermill": {
     "duration": 0.149753,
     "end_time": "2023-12-22T14:56:29.193207",
     "exception": false,
     "start_time": "2023-12-22T14:56:29.043454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fd57946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T14:56:29.200309Z",
     "iopub.status.busy": "2023-12-22T14:56:29.199880Z",
     "iopub.status.idle": "2023-12-22T15:00:20.169267Z",
     "shell.execute_reply": "2023-12-22T15:00:20.168658Z"
    },
    "papermill": {
     "duration": 230.976881,
     "end_time": "2023-12-22T15:00:20.173242",
     "exception": false,
     "start_time": "2023-12-22T14:56:29.196361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22229/2533579605.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average train loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Move the model and data to the GPU\n",
    "model.to(device)\n",
    "train_inputs, train_masks, train_labels = train_inputs.to(device), train_masks.to(device), train_labels.to(device)\n",
    "test_inputs, test_masks, test_labels = test_inputs.to(device), test_masks.to(device), test_labels.to(device)\n",
    "\n",
    "# Define class weights based on class imbalance\n",
    "class_weights = torch.tensor([weight_for_class_0, weight_for_class_1], dtype=torch.float)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device))\n",
    "\n",
    "# Define the optimizer with a learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Model training code\n",
    "def train_model(model, epochs, optimizer, loss_fn, train_dataloader):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "        logging.info(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "\n",
    "# Fine-tune the pre-trained BERT model\n",
    "train_start_time = time.time()\n",
    "train_model(model, epochs, optimizer, loss_fn, train_dataloader)\n",
    "train_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dde8a498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T15:00:20.181998Z",
     "iopub.status.busy": "2023-12-22T15:00:20.181383Z",
     "iopub.status.idle": "2023-12-22T15:00:41.108430Z",
     "shell.execute_reply": "2023-12-22T15:00:41.107832Z"
    },
    "papermill": {
     "duration": 20.932847,
     "end_time": "2023-12-22T15:00:41.109940",
     "exception": false,
     "start_time": "2023-12-22T15:00:20.177093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_start_time = time.time()\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': None}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "eval_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6246f134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T15:00:41.118486Z",
     "iopub.status.busy": "2023-12-22T15:00:41.118089Z",
     "iopub.status.idle": "2023-12-22T15:00:41.127452Z",
     "shell.execute_reply": "2023-12-22T15:00:41.126873Z"
    },
    "papermill": {
     "duration": 0.014749,
     "end_time": "2023-12-22T15:00:41.128671",
     "exception": false,
     "start_time": "2023-12-22T15:00:41.113922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "precision = precision_score(test_labels.cpu(), predictions)\n",
    "recall = recall_score(test_labels.cpu(), predictions)\n",
    "f1 = f1_score(test_labels.cpu(), predictions)\n",
    "accuracy = accuracy_score(test_labels.cpu(), predictions)\n",
    "g_mean = (recall*accuracy)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7d971ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T15:00:41.136114Z",
     "iopub.status.busy": "2023-12-22T15:00:41.135813Z",
     "iopub.status.idle": "2023-12-22T15:00:41.176804Z",
     "shell.execute_reply": "2023-12-22T15:00:41.176161Z"
    },
    "papermill": {
     "duration": 0.046233,
     "end_time": "2023-12-22T15:00:41.178320",
     "exception": false,
     "start_time": "2023-12-22T15:00:41.132087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dir = './models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "saved_model_name = f\"bert_model_{run_id}.pt\"\n",
    "model_path = os.path.join(save_dir, saved_model_name)\n",
    "\n",
    "# Only save model if accuracy meets minimum threshold\n",
    "if accuracy > min_acc:\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3be3557e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T15:00:41.185821Z",
     "iopub.status.busy": "2023-12-22T15:00:41.185526Z",
     "iopub.status.idle": "2023-12-22T15:00:41.191747Z",
     "shell.execute_reply": "2023-12-22T15:00:41.191145Z"
    },
    "papermill": {
     "duration": 0.010942,
     "end_time": "2023-12-22T15:00:41.192700",
     "exception": false,
     "start_time": "2023-12-22T15:00:41.181758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Training time: 230.77180671691895 seconds\n",
      "Inference time: 20.922882080078125 seconds\n",
      "Precision: 0.8943820224719101\n",
      "Recall: 0.7467166979362101\n",
      "F-score: 0.8139059304703475\n",
      "Accuracy: 0.8466722830665543\n",
      "G-mean: 0.7951253558059694\n"
     ]
    }
   ],
   "source": [
    "# Log the results\n",
    "logging.info(\"Evaluation Results\")\n",
    "logging.info(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "logging.info(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F-score: {f1}\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"G-mean: {g_mean}\")\n",
    "logging.info(\"Additional Info\")\n",
    "logging.info(f\"Model name: {model_name}\")\n",
    "if not train_data_set:\n",
    "  logging.info(f\"Datasets list: {categories}\")\n",
    "  logging.info(f\"Dataset size: {datasets_size}\")\n",
    "else:\n",
    "  logging.info(f\"Datasets list: {categories} Train: {train_dataset_name}, Test: {test_dataset_name}\")\n",
    "  logging.info(f\"Dataset size: Train: {train_dataset_size}, Test: {test_dataset_size}\")\n",
    "logging.info(f\"Layers frozen: {freeze_layers_up_to}\")\n",
    "logging.info(f\"Learning rate: {learning_rate}\")\n",
    "logging.info(f\"Class weights: {balancing_strategy} - {[weight_for_class_0, weight_for_class_1]}\")\n",
    "if accuracy > min_acc:\n",
    "  logging.info(f\"Model saved to: {model_path}\")\n",
    "else:\n",
    "  logging.info(\"Model not saved, didn't meet minimum accuracy threshold\")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "print(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"G-mean: {g_mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 271.201968,
   "end_time": "2023-12-22T15:00:42.013851",
   "environment_variables": {},
   "exception": null,
   "input_path": "detection.ipynb",
   "output_path": "output_detection.ipynb",
   "parameters": {
    "categories": "health",
    "epochs": 1,
    "freeze_layers_up_to": 0,
    "learning_rate": 1e-05,
    "min_acc": 0.5,
    "select_model": 5,
    "test_data": null,
    "train_data": null,
    "weight_for_class_0": "auto",
    "weight_for_class_1": "auto"
   },
   "start_time": "2023-12-22T14:56:10.811883",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}