{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torch in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: torchvision in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (0.16.1+cu121)\n",
      "Requirement already satisfied: torchaudio in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: numpy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: requests in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (4.36.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pyarrow in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (14.0.1)\n",
      "Requirement already satisfied: accelerate in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: ipywidgets in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (8.1.1)\n",
      "Requirement already satisfied: tqdm in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: filelock in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: psutil in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from accelerate) (2.1.1+cu121)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (5.14.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: colorama in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: decorator in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install transformers pandas scikit-learn pyarrow accelerate transformers[torch] ipywidgets tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, DistilBertTokenizer, DistilBertForSequenceClassification, AlbertTokenizer, AlbertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = f'logs/training_log_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log'\n",
    "logging.basicConfig(filename=log_filename, filemode='w', format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_category_data(file_path: str, category: str):\n",
    "    df = pd.read_feather(file_path)\n",
    "    # add a column for the category\n",
    "    df['category'] = category\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "  'bert-base-uncased': {\n",
    "    'model': BertForSequenceClassification,\n",
    "    'tokenizer': BertTokenizer,\n",
    "  },\n",
    "  'distilbert-base-uncased': {\n",
    "    'model': DistilBertForSequenceClassification,\n",
    "    'tokenizer': DistilBertTokenizer,\n",
    "  },\n",
    "  'distilbert-base-uncased-finetuned-sst-2-english': {\n",
    "    'model': DistilBertForSequenceClassification,\n",
    "    'tokenizer': DistilBertTokenizer,\n",
    "  },\n",
    "  'albert-base-v2': {\n",
    "    'model': AlbertForSequenceClassification,\n",
    "    'tokenizer': AlbertTokenizer,\n",
    "  },\n",
    "  'roberta-base': {\n",
    "    'model': RobertaForSequenceClassification,\n",
    "    'tokenizer': RobertaTokenizer,\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "  'crime': ['FA-KES-Dataset', 'snope'],\n",
    "  'health': ['covid_claims', 'covid_fake_news_dataset', 'covid_FNIR'],\n",
    "  'politics': ['politifact_dataset', 'liar_dataset', 'fake_news_dataset', 'pheme'],\n",
    "  'science': ['climate_dataset'],\n",
    "  'social_media': ['isot_dataset', 'gossipcop']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               text  label  metadata  title  author\n",
      "category                                           \n",
      "crime          1119   1119      1119      0       0\n",
      "health        13521  13528     13528   5939       0\n",
      "politics      61173  61212     61212      0    6424\n",
      "science         907    907       907      0       0\n",
      "social_media  67038  67038     67038      0       0\n"
     ]
    }
   ],
   "source": [
    "# show info on the categories and datasets\n",
    "data = pd.concat([load_category_data(os.path.join(os.path.realpath('.'), f'..\\data\\{category}\\{dataset}.feather'), category) for category, datasets in categories.items() for dataset in datasets])\n",
    "print(data.groupby('category').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               text\n",
      "category           \n",
      "crime          1119\n",
      "health        13521\n",
      "politics      61173\n",
      "science         907\n",
      "social_media  67038\n"
     ]
    }
   ],
   "source": [
    "# keep only the text and category columns\n",
    "data = data[['text', 'category']]\n",
    "data.dropna(inplace=True)\n",
    "print(data.groupby('category').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wed 05 Apr 2017 Syria attack symptoms consiste...</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri 07 Apr 2017 at 0914 Homs governor says U.S...</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sun 16 Apr 2017 Death toll from Aleppo bomb at...</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed 19 Apr 2017 Aleppo bomb blast kills six Sy...</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sun 10 Jul 2016 29 Syria Rebels Dead in Fighti...</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text category  \\\n",
       "0  Wed 05 Apr 2017 Syria attack symptoms consiste...    crime   \n",
       "1  Fri 07 Apr 2017 at 0914 Homs governor says U.S...    crime   \n",
       "2  Sun 16 Apr 2017 Death toll from Aleppo bomb at...    crime   \n",
       "3  Wed 19 Apr 2017 Aleppo bomb blast kills six Sy...    crime   \n",
       "4  Sun 10 Jul 2016 29 Syria Rebels Dead in Fighti...    crime   \n",
       "\n",
       "   encoded_category  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['encoded_category'] = data['category'].astype('category').cat.codes\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social_media</th>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text  encoded_category\n",
       "category                            \n",
       "crime          850               850\n",
       "health         850               850\n",
       "politics       850               850\n",
       "science        850               850\n",
       "social_media   850               850"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match the number of samples for each category\n",
    "df = data.groupby('category').apply(lambda x: x.sample(n=850, random_state=42))\n",
    "df = df.reset_index(drop=True)\n",
    "df.groupby('category').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = df['text'].to_list()\n",
    "df_labels = df['encoded_category'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(df_texts, df_labels, test_size=0.2, random_state=7623)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "model_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging.info('====================================================')\n",
    "# load the tokenizer and model\n",
    "tokenizer = model_map[model_name]['tokenizer'].from_pretrained(model_name)\n",
    "model = model_map[model_name]['model'].from_pretrained(model_name, num_labels=len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the encodings\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "validation_encodings = tokenizer(list(validation_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encodings to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_encodings['input_ids'])\n",
    "validation_masks = torch.tensor(validation_encodings['attention_mask'])\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "# Create a DataLoader for training and testing\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=8)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model and data to the GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "train_inputs, train_masks, train_labels = train_inputs.to(device), train_masks.to(device), train_labels.to(device)\n",
    "validation_inputs, validation_masks, validation_labels = validation_inputs.to(device), validation_masks.to(device), validation_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/425 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 119/425 [08:47<22:35,  4.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\School\\CSI4900\\Fake-News-Detection\\classification\\bert.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs, attention_mask\u001b[39m=\u001b[39mmasks, labels\u001b[39m=\u001b[39mlabels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 7\n",
    "\n",
    "# Fine-tune the pre-trained BERT model\n",
    "train_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, masks, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "    logging.info(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "train_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/{model_name}-{model_time}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_start_time = time.time()\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in validation_dataloader:\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': None}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "eval_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "precision = precision_score(validation_labels.cpu(), predictions, average='macro')\n",
    "recall = recall_score(validation_labels.cpu(), predictions, average='macro')\n",
    "f1 = f1_score(validation_labels.cpu(), predictions, average='macro')\n",
    "accuracy = accuracy_score(validation_labels.cpu(), predictions)\n",
    "g_mean = (recall*accuracy)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased 2023-12-14_10-10-31 with 7 epochs: Evaluation Results:\n",
      "Training time: 1758.0164849758148 seconds\n",
      "Inference time: 31.92399549484253 seconds\n",
      "Precision: 0.9687700728774725\n",
      "Recall: 0.9679539862164452\n",
      "F-score: 0.9682843239433515\n",
      "Accuracy: 0.9682352941176471\n",
      "G-mean: 0.9680946299492776\n"
     ]
    }
   ],
   "source": [
    "# Log the results\n",
    "logging.info(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results:\")\n",
    "logging.info(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "logging.info(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F-score: {f1}\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"G-mean: {g_mean}\")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results:\")\n",
    "print(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "print(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"G-mean: {g_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased 2023-12-14_10-10-31 with 7 epochs: Evaluation Results (completely new data):\n",
      "Training time: 1758.0164849758148 seconds\n",
      "Inference time: 38.20261216163635 seconds\n",
      "Precision: 0.9674714467233013\n",
      "Recall: 0.9678126143521029\n",
      "F-score: 0.967560059860937\n",
      "Accuracy: 0.968\n",
      "G-mean: 0.9679063026413433\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluate the model further by testing it with ones that are not in df but are in data\n",
    "# get the data that is not in df but is in data\n",
    "test_data = data[~data['text'].isin(df['text'])]\n",
    "\n",
    "test_data = test_data.groupby('category').apply(lambda x: x.sample(n=160, random_state=42, replace=True))\n",
    "\n",
    "# Load the data\n",
    "test_texts = test_data['text'].to_list()\n",
    "test_labels = test_data['encoded_category'].to_list()\n",
    "\n",
    "# Tokenize the data\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
    "\n",
    "# Convert the encodings to PyTorch tensors\n",
    "\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "\n",
    "# Create a DataLoader for testing\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_start_time = time.time()\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': None}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "eval_end_time = time.time()\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(test_labels, predictions, average='macro')\n",
    "recall = recall_score(test_labels, predictions, average='macro')\n",
    "f1 = f1_score(test_labels, predictions, average='macro')\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "g_mean = (recall*accuracy)**0.5\n",
    "\n",
    "# Log the results\n",
    "logging.info(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results (completely new data):\")\n",
    "logging.info(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "logging.info(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F-score: {f1}\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"G-mean: {g_mean}\")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results (completely new data):\")\n",
    "print(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "print(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"G-mean: {g_mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
