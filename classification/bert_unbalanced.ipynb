{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: torchvision in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (0.16.1+cu121)\n",
      "Requirement already satisfied: torchaudio in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: numpy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: requests in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (4.36.0)\n",
      "Requirement already satisfied: pandas in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pyarrow in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (14.0.1)\n",
      "Requirement already satisfied: accelerate in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: ipywidgets in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (8.1.1)\n",
      "Requirement already satisfied: tqdm in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: filelock in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: psutil in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from accelerate) (2.1.1+cu121)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (5.14.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: colorama in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: decorator in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install transformers pandas scikit-learn pyarrow accelerate transformers[torch] ipywidgets tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, DistilBertTokenizerFast, DistilBertForSequenceClassification, AlbertTokenizer, AlbertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = f'logs/training_log_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log'\n",
    "logging.basicConfig(filename=log_filename, filemode='w', format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_category_data(file_path: str, category: str):\n",
    "    df = pd.read_feather(file_path)\n",
    "    # add a column for the category\n",
    "    df['category'] = category\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "  'bert-base-uncased': {\n",
    "    'model': BertForSequenceClassification,\n",
    "    'tokenizer': BertTokenizer,\n",
    "  },\n",
    "  'distilbert-base-uncased': {\n",
    "    'model': DistilBertForSequenceClassification,\n",
    "    'tokenizer': DistilBertTokenizerFast,\n",
    "  },\n",
    "  'distilbert-base-uncased-finetuned-sst-2-english': {\n",
    "    'model': DistilBertForSequenceClassification,\n",
    "    'tokenizer': DistilBertTokenizerFast,\n",
    "  },\n",
    "  'albert-base-v2': {\n",
    "    'model': AlbertForSequenceClassification,\n",
    "    'tokenizer': AlbertTokenizer,\n",
    "  },\n",
    "  'roberta-base': {\n",
    "    'model': RobertaForSequenceClassification,\n",
    "    'tokenizer': RobertaTokenizer,\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "  'crime': ['FA-KES-Dataset', 'snope'],\n",
    "  'health': ['covid_claims', 'covid_fake_news_dataset', 'covid_FNIR'],\n",
    "  'politics': ['politifact_dataset', 'liar_dataset', 'fake_news_dataset', 'pheme'],\n",
    "  'science': ['climate_dataset'],\n",
    "  'social_media': ['isot_dataset', 'gossipcop']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datasets = {\n",
    "  'crime': ['FA-KES-Dataset'],\n",
    "  'health': ['covid_FNIR', 'covid_fake_news_dataset'],\n",
    "  'politics': ['politifact_dataset', 'fake_news_dataset'],\n",
    "  'science': ['climate_dataset'],\n",
    "  'social_media': ['isot_dataset']\n",
    "}\n",
    "\n",
    "testing_datasets = {\n",
    "  'crime': ['snope'],\n",
    "  'health': ['covid_claims'],\n",
    "  'politics': ['pheme', 'liar_dataset', ],\n",
    "  'science': ['climate_dataset'],\n",
    "  'social_media': ['gossipcop']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               text  label  metadata  title\n",
      "category                                   \n",
      "crime           804    804       804      0\n",
      "health        10707  10707     10707   3118\n",
      "politics      41913  41952     41952      0\n",
      "science         907    907       907      0\n",
      "social_media  44898  44898     44898      0\n",
      "               text  label  metadata  title  author\n",
      "category                                           \n",
      "crime           315    315       315      0       0\n",
      "health         2814   2821      2821   2821       0\n",
      "politics      19260  19260     19260      0    6424\n",
      "science         907    907       907      0       0\n",
      "social_media  22140  22140     22140      0       0\n"
     ]
    }
   ],
   "source": [
    "# show info on the categories and datasets\n",
    "training_data = pd.concat([load_category_data(os.path.join(os.path.realpath('.'), f'..\\data\\{category}\\{dataset}.feather'), category) for category, datasets in training_datasets.items() for dataset in datasets])\n",
    "testing_data = pd.concat([load_category_data(os.path.join(os.path.realpath('.'), f'..\\data\\{category}\\{dataset}.feather'), category) for category, datasets in testing_datasets.items() for dataset in datasets])\n",
    "\n",
    "print(training_data.groupby('category').count())\n",
    "print(testing_data.groupby('category').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the text and category columns\n",
    "training_data = training_data[['text', 'category']]\n",
    "training_data.dropna(inplace=True)\n",
    "testing_data = testing_data[['text', 'category']]\n",
    "testing_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (99229, 2)\n",
      "               text\n",
      "category           \n",
      "crime           804\n",
      "health        10707\n",
      "politics      41913\n",
      "science         907\n",
      "social_media  44898\n",
      "Testing data: (45436, 2)\n",
      "               text\n",
      "category           \n",
      "crime           315\n",
      "health         2814\n",
      "politics      19260\n",
      "science         907\n",
      "social_media  22140\n"
     ]
    }
   ],
   "source": [
    "# log the dataset information\n",
    "logging.info(f'Training data: {training_data.shape}')\n",
    "print(f'Training data: {training_data.shape}')\n",
    "logging.info(training_data.groupby('category').count())\n",
    "print(training_data.groupby('category').count())\n",
    "\n",
    "logging.info(f'Testing data: {testing_data.shape}')\n",
    "print(f'Testing data: {testing_data.shape}')\n",
    "logging.info(testing_data.groupby('category').count())\n",
    "print(testing_data.groupby('category').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Was an Italian Economist Removed from a Plane ...</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8524</th>\n",
       "      <td>Shelby Township Meijer Human Trafficking Warning</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8523</th>\n",
       "      <td>Shelby Township Meijer Human Trafficking Warning</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8522</th>\n",
       "      <td>Shelby Township Meijer Human Trafficking Warning</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8521</th>\n",
       "      <td>Shelby Township Meijer Human Trafficking Warning</td>\n",
       "      <td>crime</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text category  \\\n",
       "174   Was an Italian Economist Removed from a Plane ...    crime   \n",
       "8524   Shelby Township Meijer Human Trafficking Warning    crime   \n",
       "8523   Shelby Township Meijer Human Trafficking Warning    crime   \n",
       "8522   Shelby Township Meijer Human Trafficking Warning    crime   \n",
       "8521   Shelby Township Meijer Human Trafficking Warning    crime   \n",
       "\n",
       "      encoded_category  \n",
       "174                  0  \n",
       "8524                 0  \n",
       "8523                 0  \n",
       "8522                 0  \n",
       "8521                 0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['encoded_category'] = training_data['category'].astype('category').cat.codes\n",
    "training_data.head()\n",
    "\n",
    "testing_data['encoded_category'] = testing_data['category'].astype('category').cat.codes\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = training_data['text'].to_list()\n",
    "df_labels = training_data['encoded_category'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df_texts, df_labels, test_size=0.2, random_state=7623)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "model_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging.info('====================================================')\n",
    "# load the tokenizer and model\n",
    "tokenizer = model_map[model_name]['tokenizer'].from_pretrained(model_name)\n",
    "model = model_map[model_name]['model'].from_pretrained(model_name, num_labels=len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the encodings\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encodings to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create a DataLoader for training and testing\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=8)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "data_test_dataloader = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model and data to the GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "train_inputs, train_masks, train_labels = train_inputs.to(device), train_masks.to(device), train_labels.to(device)\n",
    "test_inputs, test_masks, test_labels = test_inputs.to(device), test_masks.to(device), test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  2%|▏         | 174/9923 [19:34<18:16:55,  6.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\School\\CSI4900\\Fake-News-Detection\\classification\\bert_unbalanced.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(t\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     inputs, masks, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;32md:\\Projects\\School\\CSI4900\\Fake-News-Detection\\classification\\bert_unbalanced.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(t\u001b[39m.\u001b[39;49mto(device) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     inputs, masks, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/School/CSI4900/Fake-News-Detection/classification/bert_unbalanced.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 1\n",
    "\n",
    "# Fine-tune the pre-trained BERT model\n",
    "train_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, masks, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "    logging.info(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "train_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/{model_name}-{model_time}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_start_time = time.time()\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in data_test_dataloader:\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': None}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "eval_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "precision = precision_score(test_labels.cpu(), predictions, average='macro')\n",
    "recall = recall_score(test_labels.cpu(), predictions, average='macro')\n",
    "f1 = f1_score(test_labels.cpu(), predictions, average='macro')\n",
    "accuracy = accuracy_score(test_labels.cpu(), predictions)\n",
    "g_mean = (recall*accuracy)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased 2023-12-14_10-10-31 with 7 epochs: Evaluation Results:\n",
      "Training time: 1758.0164849758148 seconds\n",
      "Inference time: 31.92399549484253 seconds\n",
      "Precision: 0.9687700728774725\n",
      "Recall: 0.9679539862164452\n",
      "F-score: 0.9682843239433515\n",
      "Accuracy: 0.9682352941176471\n",
      "G-mean: 0.9680946299492776\n"
     ]
    }
   ],
   "source": [
    "# Log the results\n",
    "logging.info(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results:\")\n",
    "logging.info(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "logging.info(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F-score: {f1}\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"G-mean: {g_mean}\")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results:\")\n",
    "print(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "print(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"G-mean: {g_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model further by testing it the testing datasets\n",
    "# Create the test data\n",
    "data_test_texts = testing_data['text'].to_list()\n",
    "data_test_labels = testing_data['encoded_category'].to_list()\n",
    "\n",
    "# Tokenize the data\n",
    "data_test_encodings = tokenizer(list(data_test_texts), truncation=True, padding=True)\n",
    "\n",
    "# Convert the encodings to PyTorch tensors\n",
    "data_test_inputs = torch.tensor(data_test_encodings['input_ids'])\n",
    "data_test_masks = torch.tensor(data_test_encodings['attention_mask'])\n",
    "\n",
    "# Create a DataLoader for testing\n",
    "data_test_data = TensorDataset(data_test_inputs, data_test_masks)\n",
    "data_test_dataloader = DataLoader(data_test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased 2023-12-14_10-10-31 with 7 epochs: Evaluation Results (completely new data):\n",
      "Training time: 1758.0164849758148 seconds\n",
      "Inference time: 38.20261216163635 seconds\n",
      "Precision: 0.9674714467233013\n",
      "Recall: 0.9678126143521029\n",
      "F-score: 0.967560059860937\n",
      "Accuracy: 0.968\n",
      "G-mean: 0.9679063026413433\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "eval_start_time = time.time()\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in data_test_dataloader:\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': None}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "eval_end_time = time.time()\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(data_test_labels, predictions, average='macro')\n",
    "recall = recall_score(data_test_labels, predictions, average='macro')\n",
    "f1 = f1_score(data_test_labels, predictions, average='macro')\n",
    "accuracy = accuracy_score(data_test_labels, predictions)\n",
    "g_mean = (recall*accuracy)**0.5\n",
    "\n",
    "# Log the results\n",
    "logging.info(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results (completely new data):\")\n",
    "logging.info(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "logging.info(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F-score: {f1}\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"G-mean: {g_mean}\")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results (completely new data):\")\n",
    "print(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "print(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"G-mean: {g_mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
