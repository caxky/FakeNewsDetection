2023-12-15 13:40:11,617 - ====================================================
2023-12-15 13:40:11,826 - Model: distilbert-base-uncased 2023-12-15_13-40-11 with 7 epochs
2023-12-15 13:43:58,458 - Epoch 1 average train loss: 0.5789494058314492
2023-12-15 13:47:46,416 - Epoch 2 average train loss: 0.11354863854892114
2023-12-15 13:51:34,480 - Epoch 3 average train loss: 0.059937605225645445
2023-12-15 13:55:22,516 - Epoch 4 average train loss: 0.02920909368104356
2023-12-15 13:59:10,603 - Epoch 5 average train loss: 0.014320658542873229
2023-12-15 14:02:58,950 - Epoch 6 average train loss: 0.017956281941630604
2023-12-15 14:06:46,983 - Epoch 7 average train loss: 0.012640176955336596
2023-12-15 14:07:12,480 - 2023-12-15_13-40-11 with 7 epochs: Evaluation Results:
2023-12-15 14:07:12,480 - Training time: 1595.0943279266357 seconds
2023-12-15 14:07:12,480 - Inference time: 25.48500156402588 seconds
2023-12-15 14:07:12,480 - Precision: 0.963512005254152
2023-12-15 14:07:12,481 - Recall: 0.961788254796385
2023-12-15 14:07:12,481 - F-score: 0.9624818397007209
2023-12-15 14:07:12,481 - Accuracy: 0.9623529411764706
2023-12-15 14:07:12,481 - G-mean: 0.9620705565561632
2023-12-15 14:07:41,487 - distilbert-base-uncased 2023-12-15_13-40-11 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 14:07:41,488 - Training time: 1595.0943279266357 seconds
2023-12-15 14:07:41,488 - Inference time: 25.479997158050537 seconds
2023-12-15 14:07:41,488 - Precision: 0.9676506624470523
2023-12-15 14:07:41,488 - Recall: 0.9668439261400161
2023-12-15 14:07:41,488 - F-score: 0.9670200118138593
2023-12-15 14:07:41,488 - Accuracy: 0.9670588235294117
2023-12-15 14:07:41,488 - G-mean: 0.9669513688648057
2023-12-15 14:07:43,350 - ====================================================
2023-12-15 14:07:43,582 - Model: distilbert-base-uncased 2023-12-15_14-07-43 with 7 epochs
2023-12-15 14:11:31,421 - Epoch 1 average train loss: 0.5937578926805188
2023-12-15 14:15:19,419 - Epoch 2 average train loss: 0.10969461009563769
2023-12-15 14:19:07,367 - Epoch 3 average train loss: 0.057061831405495894
2023-12-15 14:22:55,386 - Epoch 4 average train loss: 0.029423591591527357
2023-12-15 14:26:43,349 - Epoch 5 average train loss: 0.015819255350815024
2023-12-15 14:30:31,254 - Epoch 6 average train loss: 0.008465967567993657
2023-12-15 14:34:19,179 - Epoch 7 average train loss: 0.011037352300995468
2023-12-15 14:34:44,820 - 2023-12-15_14-07-43 with 7 epochs: Evaluation Results:
2023-12-15 14:34:44,820 - Training time: 1595.5413086414337 seconds
2023-12-15 14:34:44,820 - Inference time: 25.633986473083496 seconds
2023-12-15 14:34:44,820 - Precision: 0.9663197218978151
2023-12-15 14:34:44,820 - Recall: 0.9658791519506463
2023-12-15 14:34:44,821 - F-score: 0.9660815131942462
2023-12-15 14:34:44,821 - Accuracy: 0.9658823529411765
2023-12-15 14:34:44,821 - G-mean: 0.9658807524445854
2023-12-15 14:35:13,752 - distilbert-base-uncased 2023-12-15_14-07-43 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 14:35:13,752 - Training time: 1595.5413086414337 seconds
2023-12-15 14:35:13,752 - Inference time: 25.557998418807983 seconds
2023-12-15 14:35:13,752 - Precision: 0.9730394545285153
2023-12-15 14:35:13,752 - Recall: 0.9734102840528251
2023-12-15 14:35:13,752 - F-score: 0.9731245302178296
2023-12-15 14:35:13,752 - Accuracy: 0.9729411764705882
2023-12-15 14:35:13,753 - G-mean: 0.9731757019957522
2023-12-15 14:35:15,598 - ====================================================
2023-12-15 14:35:15,903 - Model: distilbert-base-uncased 2023-12-15_14-35-15 with 7 epochs
2023-12-15 14:39:03,371 - Epoch 1 average train loss: 0.5775601739918484
2023-12-15 14:42:51,005 - Epoch 2 average train loss: 0.11181511507533928
2023-12-15 14:46:38,536 - Epoch 3 average train loss: 0.05084418397835072
2023-12-15 14:50:26,202 - Epoch 4 average train loss: 0.030644468949724207
2023-12-15 14:54:13,889 - Epoch 5 average train loss: 0.025798555303995004
2023-12-15 14:58:01,521 - Epoch 6 average train loss: 0.013619109056479133
2023-12-15 15:01:49,036 - Epoch 7 average train loss: 0.00853401021025253
2023-12-15 15:02:14,566 - 2023-12-15_14-35-15 with 7 epochs: Evaluation Results:
2023-12-15 15:02:14,566 - Training time: 1593.0782516002655 seconds
2023-12-15 15:02:14,566 - Inference time: 25.521997213363647 seconds
2023-12-15 15:02:14,566 - Precision: 0.9625327361382366
2023-12-15 15:02:14,566 - Recall: 0.9632406098637615
2023-12-15 15:02:14,566 - F-score: 0.9627672012991445
2023-12-15 15:02:14,566 - Accuracy: 0.9623529411764706
2023-12-15 15:02:14,566 - G-mean: 0.9627966732197448
2023-12-15 15:02:43,394 - distilbert-base-uncased 2023-12-15_14-35-15 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 15:02:43,395 - Training time: 1593.0782516002655 seconds
2023-12-15 15:02:43,395 - Inference time: 25.395999670028687 seconds
2023-12-15 15:02:43,395 - Precision: 0.9657608887260254
2023-12-15 15:02:43,395 - Recall: 0.9669350341258787
2023-12-15 15:02:43,395 - F-score: 0.9661807860262112
2023-12-15 15:02:43,395 - Accuracy: 0.9658823529411765
2023-12-15 15:02:43,395 - G-mean: 0.9664085502016011
2023-12-15 15:02:43,431 - Model distilbert-base-uncased 2023-12-15_14-35-15 not saved
2023-12-15 15:02:43,432 - ====================================================
2023-12-15 15:02:43,671 - Model: distilbert-base-uncased 2023-12-15_15-02-43 with 7 epochs
2023-12-15 15:06:31,046 - Epoch 1 average train loss: 0.5864121431024636
2023-12-15 15:10:18,629 - Epoch 2 average train loss: 0.10502869882566088
2023-12-15 15:14:06,036 - Epoch 3 average train loss: 0.052980355217176325
2023-12-15 15:17:53,500 - Epoch 4 average train loss: 0.027559269421240862
2023-12-15 15:21:40,939 - Epoch 5 average train loss: 0.01821303503450883
2023-12-15 15:25:28,371 - Epoch 6 average train loss: 0.009630934431162827
2023-12-15 15:29:15,842 - Epoch 7 average train loss: 0.003902514010825304
2023-12-15 15:29:41,288 - 2023-12-15_15-02-43 with 7 epochs: Evaluation Results:
2023-12-15 15:29:41,288 - Training time: 1592.1098968982697 seconds
2023-12-15 15:29:41,288 - Inference time: 25.43699884414673 seconds
2023-12-15 15:29:41,288 - Precision: 0.9676116437263887
2023-12-15 15:29:41,288 - Recall: 0.9671070202674187
2023-12-15 15:29:41,288 - F-score: 0.9673327262631009
2023-12-15 15:29:41,288 - Accuracy: 0.9670588235294117
2023-12-15 15:29:41,288 - G-mean: 0.9670829215981662
2023-12-15 15:30:10,038 - distilbert-base-uncased 2023-12-15_15-02-43 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 15:30:10,038 - Training time: 1592.1098968982697 seconds
2023-12-15 15:30:10,038 - Inference time: 25.41900134086609 seconds
2023-12-15 15:30:10,038 - Precision: 0.9728229901175294
2023-12-15 15:30:10,038 - Recall: 0.9732879432579876
2023-12-15 15:30:10,038 - F-score: 0.9730283376335545
2023-12-15 15:30:10,039 - Accuracy: 0.9729411764705882
2023-12-15 15:30:10,039 - G-mean: 0.9731145444181099
2023-12-15 15:30:10,077 - Model distilbert-base-uncased 2023-12-15_15-02-43 not saved
2023-12-15 15:30:10,077 - ====================================================
2023-12-15 15:30:10,330 - Model: distilbert-base-uncased 2023-12-15_15-30-10 with 7 epochs
2023-12-15 15:33:57,699 - Epoch 1 average train loss: 0.5617780186060597
2023-12-15 15:37:45,182 - Epoch 2 average train loss: 0.10826818089493934
2023-12-15 15:41:32,650 - Epoch 3 average train loss: 0.051884098709287015
2023-12-15 15:45:20,120 - Epoch 4 average train loss: 0.0351684036653708
2023-12-15 15:49:07,761 - Epoch 5 average train loss: 0.01775489245157908
2023-12-15 15:52:55,253 - Epoch 6 average train loss: 0.012165071025271626
2023-12-15 15:56:42,759 - Epoch 7 average train loss: 0.006694026143146296
2023-12-15 15:57:08,206 - 2023-12-15_15-30-10 with 7 epochs: Evaluation Results:
2023-12-15 15:57:08,206 - Training time: 1592.3671236038208 seconds
2023-12-15 15:57:08,206 - Inference time: 25.438997507095337 seconds
2023-12-15 15:57:08,206 - Precision: 0.969857890566224
2023-12-15 15:57:08,206 - Recall: 0.9695644501821297
2023-12-15 15:57:08,206 - F-score: 0.9695866691471877
2023-12-15 15:57:08,206 - Accuracy: 0.9694117647058823
2023-12-15 15:57:08,206 - G-mean: 0.9694881044381859
2023-12-15 15:57:36,917 - distilbert-base-uncased 2023-12-15_15-30-10 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 15:57:36,917 - Training time: 1592.3671236038208 seconds
2023-12-15 15:57:36,917 - Inference time: 25.301964282989502 seconds
2023-12-15 15:57:36,917 - Precision: 0.97256835044876
2023-12-15 15:57:36,917 - Recall: 0.9735720758376697
2023-12-15 15:57:36,917 - F-score: 0.9729436077732061
2023-12-15 15:57:36,917 - Accuracy: 0.9729411764705882
2023-12-15 15:57:36,917 - G-mean: 0.9732565750327172
2023-12-15 15:57:38,772 - ====================================================
2023-12-15 15:57:39,031 - Model: distilbert-base-uncased 2023-12-15_15-57-39 with 8 epochs
2023-12-15 16:01:26,348 - Epoch 1 average train loss: 0.5966189862086493
2023-12-15 16:05:13,731 - Epoch 2 average train loss: 0.10830284991027678
2023-12-15 16:09:01,147 - Epoch 3 average train loss: 0.05331897896550158
2023-12-15 16:12:48,583 - Epoch 4 average train loss: 0.03274930899195811
2023-12-15 16:16:36,060 - Epoch 5 average train loss: 0.016513515314881634
2023-12-15 16:20:23,511 - Epoch 6 average train loss: 0.006028205530909712
2023-12-15 16:24:10,995 - Epoch 7 average train loss: 0.008277308408629752
2023-12-15 16:27:58,417 - Epoch 8 average train loss: 0.0010842179833236835
2023-12-15 16:28:23,828 - 2023-12-15_15-57-39 with 8 epochs: Evaluation Results:
2023-12-15 16:28:23,828 - Training time: 1819.3316252231598 seconds
2023-12-15 16:28:23,828 - Inference time: 25.40198588371277 seconds
2023-12-15 16:28:23,828 - Precision: 0.9611211971813974
2023-12-15 16:28:23,828 - Recall: 0.9615548518785009
2023-12-15 16:28:23,828 - F-score: 0.9611833866567553
2023-12-15 16:28:23,828 - Accuracy: 0.9611764705882353
2023-12-15 16:28:23,828 - G-mean: 0.9613656426176104
2023-12-15 16:28:52,641 - distilbert-base-uncased 2023-12-15_15-57-39 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 16:28:52,641 - Training time: 1819.3316252231598 seconds
2023-12-15 16:28:52,641 - Inference time: 25.332000017166138 seconds
2023-12-15 16:28:52,641 - Precision: 0.9644340443946646
2023-12-15 16:28:52,641 - Recall: 0.9657184224371145
2023-12-15 16:28:52,642 - F-score: 0.9645557596362606
2023-12-15 16:28:52,642 - Accuracy: 0.9647058823529412
2023-12-15 16:28:52,642 - G-mean: 0.9652120196214338
2023-12-15 16:28:54,493 - ====================================================
2023-12-15 16:28:54,750 - Model: distilbert-base-uncased 2023-12-15_16-28-54 with 8 epochs
2023-12-15 16:32:42,873 - Epoch 1 average train loss: 0.5979608694770757
2023-12-15 16:36:31,073 - Epoch 2 average train loss: 0.11563731328091201
2023-12-15 16:40:19,193 - Epoch 3 average train loss: 0.05430171197499423
2023-12-15 16:44:07,396 - Epoch 4 average train loss: 0.033868483749820905
2023-12-15 16:47:55,990 - Epoch 5 average train loss: 0.02452756924636881
2023-12-15 16:51:44,407 - Epoch 6 average train loss: 0.01328946042668951
2023-12-15 16:55:32,633 - Epoch 7 average train loss: 0.005485319650718285
2023-12-15 16:59:20,896 - Epoch 8 average train loss: 0.007187388804321767
2023-12-15 16:59:46,340 - 2023-12-15_16-28-54 with 8 epochs: Evaluation Results:
2023-12-15 16:59:46,340 - Training time: 1826.0920028686523 seconds
2023-12-15 16:59:46,340 - Inference time: 25.436043977737427 seconds
2023-12-15 16:59:46,340 - Precision: 0.9646308691351362
2023-12-15 16:59:46,340 - Recall: 0.9633370833276214
2023-12-15 16:59:46,340 - F-score: 0.9637155349620106
2023-12-15 16:59:46,341 - Accuracy: 0.9635294117647059
2023-12-15 16:59:46,341 - G-mean: 0.9634332427468913
2023-12-15 17:00:15,093 - distilbert-base-uncased 2023-12-15_16-28-54 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 17:00:15,093 - Training time: 1826.0920028686523 seconds
2023-12-15 17:00:15,093 - Inference time: 25.33799910545349 seconds
2023-12-15 17:00:15,093 - Precision: 0.9697844822154582
2023-12-15 17:00:15,093 - Recall: 0.9699613629498091
2023-12-15 17:00:15,093 - F-score: 0.9696735775302734
2023-12-15 17:00:15,093 - Accuracy: 0.9694117647058823
2023-12-15 17:00:15,093 - G-mean: 0.9696865248902334
2023-12-15 17:00:16,965 - ====================================================
2023-12-15 17:00:17,227 - Model: distilbert-base-uncased 2023-12-15_17-00-17 with 8 epochs
2023-12-15 17:04:05,057 - Epoch 1 average train loss: 0.5839509420359836
2023-12-15 17:07:53,114 - Epoch 2 average train loss: 0.11500309150665998
2023-12-15 17:11:41,052 - Epoch 3 average train loss: 0.05606793854933451
2023-12-15 17:15:29,035 - Epoch 4 average train loss: 0.03089479684610577
2023-12-15 17:19:17,052 - Epoch 5 average train loss: 0.018522814159171983
2023-12-15 17:23:05,105 - Epoch 6 average train loss: 0.009406444233266965
2023-12-15 17:26:53,079 - Epoch 7 average train loss: 0.01454352378293954
2023-12-15 17:30:41,045 - Epoch 8 average train loss: 0.005654740368875483
2023-12-15 17:31:06,536 - 2023-12-15_17-00-17 with 8 epochs: Evaluation Results:
2023-12-15 17:31:06,536 - Training time: 1823.7642765045166 seconds
2023-12-15 17:31:06,536 - Inference time: 25.483234882354736 seconds
2023-12-15 17:31:06,536 - Precision: 0.9688177272253224
2023-12-15 17:31:06,536 - Recall: 0.9682732531497429
2023-12-15 17:31:06,536 - F-score: 0.9684247082391698
2023-12-15 17:31:06,537 - Accuracy: 0.9682352941176471
2023-12-15 17:31:06,537 - G-mean: 0.9682542734476788
2023-12-15 17:31:35,260 - distilbert-base-uncased 2023-12-15_17-00-17 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 17:31:35,260 - Training time: 1823.7642765045166 seconds
2023-12-15 17:31:35,260 - Inference time: 25.318000316619873 seconds
2023-12-15 17:31:35,260 - Precision: 0.9660890623646446
2023-12-15 17:31:35,260 - Recall: 0.965896911254063
2023-12-15 17:31:35,260 - F-score: 0.9658989423676296
2023-12-15 17:31:35,260 - Accuracy: 0.9658823529411765
2023-12-15 17:31:35,260 - G-mean: 0.9658896320701911
2023-12-15 17:31:35,299 - Model distilbert-base-uncased 2023-12-15_17-00-17 not saved
2023-12-15 17:31:35,299 - ====================================================
2023-12-15 17:31:35,628 - Model: distilbert-base-uncased 2023-12-15_17-31-35 with 8 epochs
2023-12-15 17:35:23,158 - Epoch 1 average train loss: 0.5759680652092485
2023-12-15 17:39:10,768 - Epoch 2 average train loss: 0.11475224654245025
2023-12-15 17:42:58,390 - Epoch 3 average train loss: 0.0551780225698124
2023-12-15 17:46:46,065 - Epoch 4 average train loss: 0.03020476537756622
2023-12-15 17:50:33,700 - Epoch 5 average train loss: 0.016515232457587604
2023-12-15 17:54:21,254 - Epoch 6 average train loss: 0.007569212179754258
2023-12-15 17:58:08,858 - Epoch 7 average train loss: 0.007891011607340154
2023-12-15 18:01:56,526 - Epoch 8 average train loss: 0.005653029988718438
2023-12-15 18:02:21,914 - 2023-12-15_17-31-35 with 8 epochs: Evaluation Results:
2023-12-15 18:02:21,915 - Training time: 1820.8380043506622 seconds
2023-12-15 18:02:21,915 - Inference time: 25.37999963760376 seconds
2023-12-15 18:02:21,915 - Precision: 0.9616953827601364
2023-12-15 18:02:21,915 - Recall: 0.9616405280743434
2023-12-15 18:02:21,915 - F-score: 0.9613817320630524
2023-12-15 18:02:21,915 - Accuracy: 0.9611764705882353
2023-12-15 18:02:21,915 - G-mean: 0.961408471332089
2023-12-15 18:02:50,577 - distilbert-base-uncased 2023-12-15_17-31-35 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 18:02:50,577 - Training time: 1820.8380043506622 seconds
2023-12-15 18:02:50,578 - Inference time: 25.32496166229248 seconds
2023-12-15 18:02:50,578 - Precision: 0.9615765240659637
2023-12-15 18:02:50,578 - Recall: 0.9629785532238875
2023-12-15 18:02:50,578 - F-score: 0.9619767794658152
2023-12-15 18:02:50,578 - Accuracy: 0.9623529411764706
2023-12-15 18:02:50,578 - G-mean: 0.9626656963790029
2023-12-15 18:02:50,616 - Model distilbert-base-uncased 2023-12-15_17-31-35 not saved
2023-12-15 18:02:50,616 - ====================================================
2023-12-15 18:02:50,845 - Model: distilbert-base-uncased 2023-12-15_18-02-50 with 8 epochs
2023-12-15 18:06:38,297 - Epoch 1 average train loss: 0.5811482067318524
2023-12-15 18:10:25,892 - Epoch 2 average train loss: 0.11888830656952717
2023-12-15 18:14:13,562 - Epoch 3 average train loss: 0.05670655759927981
2023-12-15 18:18:01,274 - Epoch 4 average train loss: 0.029222703001681057
2023-12-15 18:21:48,947 - Epoch 5 average train loss: 0.024884688461966373
2023-12-15 18:25:36,649 - Epoch 6 average train loss: 0.011980129854808397
2023-12-15 18:29:24,314 - Epoch 7 average train loss: 0.0069276350031962945
2023-12-15 18:33:12,069 - Epoch 8 average train loss: 0.015573114713393699
2023-12-15 18:33:37,451 - 2023-12-15_18-02-50 with 8 epochs: Evaluation Results:
2023-12-15 18:33:37,451 - Training time: 1821.1651198863983 seconds
2023-12-15 18:33:37,451 - Inference time: 25.37399959564209 seconds
2023-12-15 18:33:37,451 - Precision: 0.959404891319625
2023-12-15 18:33:37,451 - Recall: 0.9582690649861376
2023-12-15 18:33:37,451 - F-score: 0.9586973534052998
2023-12-15 18:33:37,451 - Accuracy: 0.9588235294117647
2023-12-15 18:33:37,451 - G-mean: 0.95854625710819
2023-12-15 18:34:06,108 - distilbert-base-uncased 2023-12-15_18-02-50 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 18:34:06,108 - Training time: 1821.1651198863983 seconds
2023-12-15 18:34:06,108 - Inference time: 25.33896517753601 seconds
2023-12-15 18:34:06,108 - Precision: 0.9638908447982256
2023-12-15 18:34:06,108 - Recall: 0.9632204263787678
2023-12-15 18:34:06,108 - F-score: 0.963456829178603
2023-12-15 18:34:06,108 - Accuracy: 0.9635294117647059
2023-12-15 18:34:06,108 - G-mean: 0.9633749066840402
2023-12-15 18:34:06,148 - Model distilbert-base-uncased 2023-12-15_18-02-50 not saved
2023-12-15 18:34:06,149 - ====================================================
2023-12-15 18:34:06,409 - Model: distilbert-base-uncased 2023-12-15_18-34-06 with 9 epochs
2023-12-15 18:37:53,586 - Epoch 1 average train loss: 0.5662812026809244
2023-12-15 18:41:40,960 - Epoch 2 average train loss: 0.11288366275675156
2023-12-15 18:45:28,436 - Epoch 3 average train loss: 0.054489392284303904
2023-12-15 18:49:15,899 - Epoch 4 average train loss: 0.029114148999180862
2023-12-15 18:53:03,312 - Epoch 5 average train loss: 0.015814252823372097
2023-12-15 18:56:50,751 - Epoch 6 average train loss: 0.009762072884135277
2023-12-15 19:00:38,307 - Epoch 7 average train loss: 0.006596474172821378
2023-12-15 19:04:25,512 - Epoch 8 average train loss: 0.00032185308955376966
2023-12-15 19:08:12,861 - Epoch 9 average train loss: 0.006598009177205848
2023-12-15 19:08:38,408 - 2023-12-15_18-34-06 with 9 epochs: Evaluation Results:
2023-12-15 19:08:38,408 - Training time: 2046.3974449634552 seconds
2023-12-15 19:08:38,408 - Inference time: 25.53800082206726 seconds
2023-12-15 19:08:38,408 - Precision: 0.9654946309328656
2023-12-15 19:08:38,408 - Recall: 0.9651080829325178
2023-12-15 19:08:38,408 - F-score: 0.9652699686633026
2023-12-15 19:08:38,408 - Accuracy: 0.9647058823529412
2023-12-15 19:08:38,408 - G-mean: 0.9649069616866541
2023-12-15 19:09:07,193 - distilbert-base-uncased 2023-12-15_18-34-06 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 19:09:07,193 - Training time: 2046.3974449634552 seconds
2023-12-15 19:09:07,193 - Inference time: 25.44300103187561 seconds
2023-12-15 19:09:07,193 - Precision: 0.9628619727670362
2023-12-15 19:09:07,194 - Recall: 0.9626308732704866
2023-12-15 19:09:07,194 - F-score: 0.9625664563899579
2023-12-15 19:09:07,194 - Accuracy: 0.9623529411764706
2023-12-15 19:09:07,194 - G-mean: 0.9624918971914138
2023-12-15 19:09:09,063 - ====================================================
2023-12-15 19:09:09,332 - Model: distilbert-base-uncased 2023-12-15_19-09-09 with 9 epochs
2023-12-15 19:12:56,785 - Epoch 1 average train loss: 0.599387826183263
2023-12-15 19:16:44,345 - Epoch 2 average train loss: 0.11519852143657558
2023-12-15 19:20:31,880 - Epoch 3 average train loss: 0.05901532311211614
2023-12-15 19:24:19,409 - Epoch 4 average train loss: 0.032104605379076126
2023-12-15 19:28:06,934 - Epoch 5 average train loss: 0.020434616124805283
2023-12-15 19:31:54,369 - Epoch 6 average train loss: 0.015822533525197822
2023-12-15 19:35:41,946 - Epoch 7 average train loss: 0.01125932740225621
2023-12-15 19:39:29,417 - Epoch 8 average train loss: 0.00518855628780062
2023-12-15 19:43:17,022 - Epoch 9 average train loss: 0.0031358173632778583
2023-12-15 19:43:42,528 - 2023-12-15_19-09-09 with 9 epochs: Evaluation Results:
2023-12-15 19:43:42,528 - Training time: 2047.6346371173859 seconds
2023-12-15 19:43:42,528 - Inference time: 25.497997999191284 seconds
2023-12-15 19:43:42,528 - Precision: 0.9630782040406105
2023-12-15 19:43:42,528 - Recall: 0.9626415269871732
2023-12-15 19:43:42,528 - F-score: 0.9628360475308337
2023-12-15 19:43:42,528 - Accuracy: 0.9623529411764706
2023-12-15 19:43:42,528 - G-mean: 0.9624972232659764
2023-12-15 19:44:11,310 - distilbert-base-uncased 2023-12-15_19-09-09 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 19:44:11,310 - Training time: 2047.6346371173859 seconds
2023-12-15 19:44:11,310 - Inference time: 25.427971839904785 seconds
2023-12-15 19:44:11,310 - Precision: 0.9772955353797768
2023-12-15 19:44:11,310 - Recall: 0.9781941698146268
2023-12-15 19:44:11,310 - F-score: 0.9776792703977009
2023-12-15 19:44:11,310 - Accuracy: 0.9776470588235294
2023-12-15 19:44:11,310 - G-mean: 0.9779205760579915
2023-12-15 19:44:13,165 - ====================================================
2023-12-15 19:44:13,408 - Model: distilbert-base-uncased 2023-12-15_19-44-13 with 9 epochs
2023-12-15 19:48:00,976 - Epoch 1 average train loss: 0.5720079434180961
2023-12-15 19:51:48,547 - Epoch 2 average train loss: 0.11133230477352353
2023-12-15 19:55:36,272 - Epoch 3 average train loss: 0.055512386831071445
2023-12-15 19:59:24,012 - Epoch 4 average train loss: 0.032027203788268656
2023-12-15 20:03:11,737 - Epoch 5 average train loss: 0.016834048879716325
2023-12-15 20:06:59,500 - Epoch 6 average train loss: 0.013629367145202945
2023-12-15 20:10:47,130 - Epoch 7 average train loss: 0.007833696618909015
2023-12-15 20:14:34,847 - Epoch 8 average train loss: 0.005264432700693279
2023-12-15 20:18:22,605 - Epoch 9 average train loss: 0.0010778197634927517
2023-12-15 20:18:48,177 - 2023-12-15_19-44-13 with 9 epochs: Evaluation Results:
2023-12-15 20:18:48,177 - Training time: 2049.1426887512207 seconds
2023-12-15 20:18:48,177 - Inference time: 25.565033435821533 seconds
2023-12-15 20:18:48,177 - Precision: 0.9649257802989446
2023-12-15 20:18:48,177 - Recall: 0.9646239637236167
2023-12-15 20:18:48,177 - F-score: 0.964732271922801
2023-12-15 20:18:48,177 - Accuracy: 0.9647058823529412
2023-12-15 20:18:48,177 - G-mean: 0.9646649221687202
2023-12-15 20:19:17,060 - distilbert-base-uncased 2023-12-15_19-44-13 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 20:19:17,061 - Training time: 2049.1426887512207 seconds
2023-12-15 20:19:17,061 - Inference time: 25.51099944114685 seconds
2023-12-15 20:19:17,061 - Precision: 0.9736812743501032
2023-12-15 20:19:17,061 - Recall: 0.9749916792047693
2023-12-15 20:19:17,061 - F-score: 0.9740948298502161
2023-12-15 20:19:17,061 - Accuracy: 0.9741176470588235
2023-12-15 20:19:17,061 - G-mean: 0.9745545651470117
2023-12-15 20:19:17,099 - Model distilbert-base-uncased 2023-12-15_19-44-13 not saved
2023-12-15 20:19:17,099 - ====================================================
2023-12-15 20:19:17,349 - Model: distilbert-base-uncased 2023-12-15_20-19-17 with 9 epochs
2023-12-15 20:23:05,219 - Epoch 1 average train loss: 0.5866361496991971
2023-12-15 20:26:53,149 - Epoch 2 average train loss: 0.10934170057011001
2023-12-15 20:30:41,176 - Epoch 3 average train loss: 0.05490378229276222
2023-12-15 20:34:29,140 - Epoch 4 average train loss: 0.029175256477559315
2023-12-15 20:38:17,198 - Epoch 5 average train loss: 0.019872846509028666
2023-12-15 20:42:05,208 - Epoch 6 average train loss: 0.011033699236371939
2023-12-15 20:45:53,168 - Epoch 7 average train loss: 0.007743031524587423
2023-12-15 20:49:41,160 - Epoch 8 average train loss: 0.005797374126628068
2023-12-15 20:53:29,092 - Epoch 9 average train loss: 0.0055302289759599855
2023-12-15 20:53:54,591 - 2023-12-15_20-19-17 with 9 epochs: Evaluation Results:
2023-12-15 20:53:54,591 - Training time: 2051.6812455654144 seconds
2023-12-15 20:53:54,591 - Inference time: 25.491000413894653 seconds
2023-12-15 20:53:54,591 - Precision: 0.9648573836030738
2023-12-15 20:53:54,591 - Recall: 0.9646582236254908
2023-12-15 20:53:54,591 - F-score: 0.9646075879595657
2023-12-15 20:53:54,592 - Accuracy: 0.9647058823529412
2023-12-15 20:53:54,592 - G-mean: 0.9646820526949021
2023-12-15 20:54:23,381 - distilbert-base-uncased 2023-12-15_20-19-17 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 20:54:23,381 - Training time: 2051.6812455654144 seconds
2023-12-15 20:54:23,381 - Inference time: 25.47000026702881 seconds
2023-12-15 20:54:23,382 - Precision: 0.9677900500897337
2023-12-15 20:54:23,382 - Recall: 0.9689851162143283
2023-12-15 20:54:23,382 - F-score: 0.9681973230588931
2023-12-15 20:54:23,382 - Accuracy: 0.9682352941176471
2023-12-15 20:54:23,382 - G-mean: 0.9686101326092984
2023-12-15 20:54:23,420 - Model distilbert-base-uncased 2023-12-15_20-19-17 not saved
2023-12-15 20:54:23,420 - ====================================================
2023-12-15 20:54:23,677 - Model: distilbert-base-uncased 2023-12-15_20-54-23 with 9 epochs
2023-12-15 20:58:11,286 - Epoch 1 average train loss: 0.5886495894719572
2023-12-15 21:01:58,993 - Epoch 2 average train loss: 0.1151937513645081
2023-12-15 21:05:46,503 - Epoch 3 average train loss: 0.05771271373090499
2023-12-15 21:09:32,537 - Epoch 4 average train loss: 0.031733626229688526
2023-12-15 21:13:18,461 - Epoch 5 average train loss: 0.01670540455263108
2023-12-15 21:17:04,489 - Epoch 6 average train loss: 0.010121909610187526
2023-12-15 21:20:50,467 - Epoch 7 average train loss: 0.010486749240183545
2023-12-15 21:24:36,509 - Epoch 8 average train loss: 0.007728766582406345
2023-12-15 21:28:22,625 - Epoch 9 average train loss: 0.004569332801517756
2023-12-15 21:28:47,824 - 2023-12-15_20-54-23 with 9 epochs: Evaluation Results:
2023-12-15 21:28:47,824 - Training time: 2038.8855981826782 seconds
2023-12-15 21:28:47,824 - Inference time: 25.191999912261963 seconds
2023-12-15 21:28:47,824 - Precision: 0.9719997418059105
2023-12-15 21:28:47,824 - Recall: 0.9720765755396723
2023-12-15 21:28:47,824 - F-score: 0.9720186743089314
2023-12-15 21:28:47,824 - Accuracy: 0.971764705882353
2023-12-15 21:28:47,824 - G-mean: 0.9719206282019301
2023-12-15 21:29:16,331 - distilbert-base-uncased 2023-12-15_20-54-23 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 21:29:16,331 - Training time: 2038.8855981826782 seconds
2023-12-15 21:29:16,331 - Inference time: 25.142000675201416 seconds
2023-12-15 21:29:16,331 - Precision: 0.9642684677492769
2023-12-15 21:29:16,331 - Recall: 0.9654906785814301
2023-12-15 21:29:16,331 - F-score: 0.9647096935233066
2023-12-15 21:29:16,331 - Accuracy: 0.9647058823529412
2023-12-15 21:29:16,331 - G-mean: 0.9650982006948507
2023-12-15 21:29:16,394 - Model distilbert-base-uncased 2023-12-15_20-54-23 not saved
2023-12-15 21:29:16,394 - ====================================================
2023-12-15 21:29:16,682 - Model: distilbert-base-uncased 2023-12-15_21-29-16 with 10 epochs
2023-12-15 21:33:06,507 - Epoch 1 average train loss: 0.6196429972525905
2023-12-15 21:36:54,116 - Epoch 2 average train loss: 0.11927221397924073
2023-12-15 21:40:41,722 - Epoch 3 average train loss: 0.06411992382039042
2023-12-15 21:44:29,473 - Epoch 4 average train loss: 0.034158769381944745
2023-12-15 21:48:17,173 - Epoch 5 average train loss: 0.02246126229792614
2023-12-15 21:52:05,112 - Epoch 6 average train loss: 0.014192373995628099
2023-12-15 21:55:53,087 - Epoch 7 average train loss: 0.009982761700384264
2023-12-15 21:59:41,293 - Epoch 8 average train loss: 0.00453110776232172
2023-12-15 22:03:29,388 - Epoch 9 average train loss: 0.006340022100133972
2023-12-15 22:07:17,894 - Epoch 10 average train loss: 0.0011003625366420773
2023-12-15 22:07:43,597 - 2023-12-15_21-29-16 with 10 epochs: Evaluation Results:
2023-12-15 22:07:43,597 - Training time: 2281.1437199115753 seconds
2023-12-15 22:07:43,597 - Inference time: 25.69500231742859 seconds
2023-12-15 22:07:43,598 - Precision: 0.9664600209368309
2023-12-15 22:07:43,598 - Recall: 0.9655650761054819
2023-12-15 22:07:43,598 - F-score: 0.9659249572048957
2023-12-15 22:07:43,598 - Accuracy: 0.9658823529411765
2023-12-15 22:07:43,598 - G-mean: 0.9657237014936462
2023-12-15 22:08:12,639 - distilbert-base-uncased 2023-12-15_21-29-16 with 10 epochs: Evaluation Results (completely new data):
2023-12-15 22:08:12,639 - Training time: 2281.1437199115753 seconds
2023-12-15 22:08:12,639 - Inference time: 25.625969886779785 seconds
2023-12-15 22:08:12,639 - Precision: 0.9712796921684582
2023-12-15 22:08:12,639 - Recall: 0.9722826277087664
2023-12-15 22:08:12,639 - F-score: 0.9716579808356387
2023-12-15 22:08:12,639 - Accuracy: 0.971764705882353
2023-12-15 22:08:12,639 - G-mean: 0.972023632300126
2023-12-15 22:08:14,529 - ====================================================
2023-12-15 22:08:14,797 - Model: distilbert-base-uncased 2023-12-15_22-08-14 with 10 epochs
2023-12-15 22:12:02,570 - Epoch 1 average train loss: 0.5856343828141689
2023-12-15 22:15:50,630 - Epoch 2 average train loss: 0.11411360169815667
2023-12-15 22:19:38,799 - Epoch 3 average train loss: 0.05447961441076854
2023-12-15 22:23:27,180 - Epoch 4 average train loss: 0.02937062169863459
2023-12-15 22:27:15,602 - Epoch 5 average train loss: 0.02199350568164578
2023-12-15 22:31:04,702 - Epoch 6 average train loss: 0.01697900059975355
2023-12-15 22:35:06,593 - Epoch 7 average train loss: 0.008274932388859966
2023-12-15 22:39:09,582 - Epoch 8 average train loss: 0.013111150964824072
2023-12-15 22:43:10,213 - Epoch 9 average train loss: 0.00289369199950563
2023-12-15 22:47:07,222 - Epoch 10 average train loss: 0.0012667431736832335
2023-12-15 22:47:33,349 - 2023-12-15_22-08-14 with 10 epochs: Evaluation Results:
2023-12-15 22:47:33,349 - Training time: 2332.365564107895 seconds
2023-12-15 22:47:33,349 - Inference time: 26.117998361587524 seconds
2023-12-15 22:47:33,349 - Precision: 0.9675340304821922
2023-12-15 22:47:33,349 - Recall: 0.9671746656194248
2023-12-15 22:47:33,349 - F-score: 0.9671801395274515
2023-12-15 22:47:33,349 - Accuracy: 0.9670588235294117
2023-12-15 22:47:33,349 - G-mean: 0.9671167428399599
2023-12-15 22:48:02,481 - distilbert-base-uncased 2023-12-15_22-08-14 with 10 epochs: Evaluation Results (completely new data):
2023-12-15 22:48:02,481 - Training time: 2332.365564107895 seconds
2023-12-15 22:48:02,481 - Inference time: 25.623000144958496 seconds
2023-12-15 22:48:02,481 - Precision: 0.967542249982967
2023-12-15 22:48:02,481 - Recall: 0.9686059627500658
2023-12-15 22:48:02,481 - F-score: 0.9679127987603581
2023-12-15 22:48:02,481 - Accuracy: 0.9682352941176471
2023-12-15 22:48:02,481 - G-mean: 0.9684206106994092
2023-12-15 22:48:02,520 - Model distilbert-base-uncased 2023-12-15_22-08-14 not saved
2023-12-15 22:48:02,520 - ====================================================
2023-12-15 22:48:02,766 - Model: distilbert-base-uncased 2023-12-15_22-48-02 with 10 epochs
2023-12-15 22:52:04,409 - Epoch 1 average train loss: 0.586530374560286
2023-12-15 22:56:18,976 - Epoch 2 average train loss: 0.11289132840274012
2023-12-15 23:00:10,886 - Epoch 3 average train loss: 0.05389704091803116
2023-12-15 23:03:59,838 - Epoch 4 average train loss: 0.03470159953788799
2023-12-15 23:07:45,962 - Epoch 5 average train loss: 0.01672529655015644
2023-12-15 23:11:33,283 - Epoch 6 average train loss: 0.007048357240581776
2023-12-15 23:15:19,765 - Epoch 7 average train loss: 0.014103490402442678
2023-12-15 23:19:07,043 - Epoch 8 average train loss: 0.005330395510159981
2023-12-15 23:22:53,996 - Epoch 9 average train loss: 0.0005765828778470784
2023-12-15 23:26:42,206 - Epoch 10 average train loss: 0.006871781243385425
2023-12-15 23:27:07,023 - 2023-12-15_22-48-02 with 10 epochs: Evaluation Results:
2023-12-15 23:27:07,023 - Training time: 2319.376227617264 seconds
2023-12-15 23:27:07,023 - Inference time: 24.80900001525879 seconds
2023-12-15 23:27:07,023 - Precision: 0.968528328279261
2023-12-15 23:27:07,023 - Recall: 0.9680190638355433
2023-12-15 23:27:07,023 - F-score: 0.9682222557220375
2023-12-15 23:27:07,023 - Accuracy: 0.9682352941176471
2023-12-15 23:27:07,023 - G-mean: 0.9681271729397417
2023-12-15 23:27:35,363 - distilbert-base-uncased 2023-12-15_22-48-02 with 10 epochs: Evaluation Results (completely new data):
2023-12-15 23:27:35,364 - Training time: 2319.376227617264 seconds
2023-12-15 23:27:35,364 - Inference time: 24.87900400161743 seconds
2023-12-15 23:27:35,364 - Precision: 0.9656951319029489
2023-12-15 23:27:35,364 - Recall: 0.9665268118478755
2023-12-15 23:27:35,364 - F-score: 0.96600468205738
2023-12-15 23:27:35,364 - Accuracy: 0.9658823529411765
2023-12-15 23:27:35,364 - G-mean: 0.9662045286627257
2023-12-15 23:27:35,458 - Model distilbert-base-uncased 2023-12-15_22-48-02 not saved
2023-12-15 23:27:35,458 - ====================================================
2023-12-15 23:27:35,722 - Model: distilbert-base-uncased 2023-12-15_23-27-35 with 10 epochs
2023-12-15 23:31:52,532 - Epoch 1 average train loss: 0.5911657013086712
2023-12-15 23:35:53,530 - Epoch 2 average train loss: 0.11086565331939388
2023-12-15 23:39:53,226 - Epoch 3 average train loss: 0.051107093913151935
2023-12-15 23:44:19,424 - Epoch 4 average train loss: 0.027206277033752377
2023-12-15 23:48:23,386 - Epoch 5 average train loss: 0.01975253282197039
2023-12-15 23:52:30,477 - Epoch 6 average train loss: 0.01051972812260775
2023-12-15 23:56:53,472 - Epoch 7 average train loss: 0.006079036794869941
2023-12-16 00:01:12,097 - Epoch 8 average train loss: 0.0014728616454693325
2023-12-16 00:05:26,340 - Epoch 9 average train loss: 0.01976038063471017
2023-12-16 00:09:41,056 - Epoch 10 average train loss: 0.02643539174263852
2023-12-16 00:10:09,666 - 2023-12-15_23-27-35 with 10 epochs: Evaluation Results:
2023-12-16 00:10:09,667 - Training time: 2525.2625439167023 seconds
2023-12-16 00:10:09,667 - Inference time: 28.60300302505493 seconds
2023-12-16 00:10:09,667 - Precision: 0.9700350873182847
2023-12-16 00:10:09,667 - Recall: 0.9693436463180621
2023-12-16 00:10:09,667 - F-score: 0.9695402032298475
2023-12-16 00:10:09,667 - Accuracy: 0.9694117647058823
2023-12-16 00:10:09,667 - G-mean: 0.9693777049136354
2023-12-16 00:10:41,744 - distilbert-base-uncased 2023-12-15_23-27-35 with 10 epochs: Evaluation Results (completely new data):
2023-12-16 00:10:41,744 - Training time: 2525.2625439167023 seconds
2023-12-16 00:10:41,744 - Inference time: 28.470036029815674 seconds
2023-12-16 00:10:41,744 - Precision: 0.9693974135237656
2023-12-16 00:10:41,744 - Recall: 0.9698329566150965
2023-12-16 00:10:41,744 - F-score: 0.9695476964386485
2023-12-16 00:10:41,744 - Accuracy: 0.9694117647058823
2023-12-16 00:10:41,744 - G-mean: 0.9696223377904224
2023-12-16 00:10:41,788 - Model distilbert-base-uncased 2023-12-15_23-27-35 not saved
2023-12-16 00:10:41,789 - ====================================================
2023-12-16 00:10:42,029 - Model: distilbert-base-uncased 2023-12-16_00-10-42 with 10 epochs
2023-12-16 00:14:32,208 - Epoch 1 average train loss: 0.5874017753846505
2023-12-16 00:18:23,201 - Epoch 2 average train loss: 0.10901882997330498
2023-12-16 00:22:13,608 - Epoch 3 average train loss: 0.04843928583952434
2023-12-16 00:26:08,413 - Epoch 4 average train loss: 0.02701061100972926
2023-12-16 00:30:01,077 - Epoch 5 average train loss: 0.022518071917652645
2023-12-16 00:33:51,519 - Epoch 6 average train loss: 0.011524677832939607
2023-12-16 00:37:41,784 - Epoch 7 average train loss: 0.009980695181464612
2023-12-16 00:41:31,124 - Epoch 8 average train loss: 0.0034581764414392904
2023-12-16 00:45:20,423 - Epoch 9 average train loss: 0.008184487620909588
2023-12-16 00:49:09,890 - Epoch 10 average train loss: 0.0023730239424099527
2023-12-16 00:49:35,831 - 2023-12-16_00-10-42 with 10 epochs: Evaluation Results:
2023-12-16 00:49:35,831 - Training time: 2307.798164367676 seconds
2023-12-16 00:49:35,831 - Inference time: 25.932000160217285 seconds
2023-12-16 00:49:35,831 - Precision: 0.9629251591847611
2023-12-16 00:49:35,832 - Recall: 0.9626757868890469
2023-12-16 00:49:35,832 - F-score: 0.9627231144257966
2023-12-16 00:49:35,832 - Accuracy: 0.9623529411764706
2023-12-16 00:49:35,832 - G-mean: 0.9625143504966809
2023-12-16 00:50:05,212 - distilbert-base-uncased 2023-12-16_00-10-42 with 10 epochs: Evaluation Results (completely new data):
2023-12-16 00:50:05,212 - Training time: 2307.798164367676 seconds
2023-12-16 00:50:05,212 - Inference time: 25.708998918533325 seconds
2023-12-16 00:50:05,213 - Precision: 0.9739204582415016
2023-12-16 00:50:05,213 - Recall: 0.9747348665353442
2023-12-16 00:50:05,213 - F-score: 0.9742533344883123
2023-12-16 00:50:05,213 - Accuracy: 0.9741176470588235
2023-12-16 00:50:05,213 - G-mean: 0.9744262079273145
2023-12-16 00:50:07,299 - ====================================================
2023-12-16 00:50:07,639 - Model: distilbert-base-uncased 2023-12-16_00-50-07 with 11 epochs
2023-12-16 00:53:57,286 - Epoch 1 average train loss: 0.5925788691464593
2023-12-16 00:57:46,982 - Epoch 2 average train loss: 0.1077366958865348
2023-12-16 01:01:36,651 - Epoch 3 average train loss: 0.05567942564211348
2023-12-16 01:05:31,796 - Epoch 4 average train loss: 0.03035937396782067
2023-12-16 01:09:22,036 - Epoch 5 average train loss: 0.015369814595700625
2023-12-16 01:13:12,025 - Epoch 6 average train loss: 0.007296098020366009
2023-12-16 01:17:01,746 - Epoch 7 average train loss: 0.010218540033914477
2023-12-16 01:20:51,622 - Epoch 8 average train loss: 0.010322859177326523
2023-12-16 01:24:41,093 - Epoch 9 average train loss: 0.0069113651852582255
2023-12-16 01:28:30,751 - Epoch 10 average train loss: 0.00562171982266163
2023-12-16 01:32:20,184 - Epoch 11 average train loss: 0.004151218690201537
2023-12-16 01:32:45,980 - 2023-12-16_00-50-07 with 11 epochs: Evaluation Results:
2023-12-16 01:32:45,980 - Training time: 2532.488135099411 seconds
2023-12-16 01:32:45,981 - Inference time: 25.788999319076538 seconds
2023-12-16 01:32:45,981 - Precision: 0.9700361262448892
2023-12-16 01:32:45,981 - Recall: 0.9695353813683892
2023-12-16 01:32:45,981 - F-score: 0.9694604206236125
2023-12-16 01:32:45,981 - Accuracy: 0.9694117647058823
2023-12-16 01:32:45,981 - G-mean: 0.9694735710668553
2023-12-16 01:33:15,046 - distilbert-base-uncased 2023-12-16_00-50-07 with 11 epochs: Evaluation Results (completely new data):
2023-12-16 01:33:15,046 - Training time: 2532.488135099411 seconds
2023-12-16 01:33:15,046 - Inference time: 25.691030263900757 seconds
2023-12-16 01:33:15,046 - Precision: 0.9641888935946781
2023-12-16 01:33:15,046 - Recall: 0.9652416803553635
2023-12-16 01:33:15,046 - F-score: 0.9645744110608856
2023-12-16 01:33:15,046 - Accuracy: 0.9647058823529412
2023-12-16 01:33:15,047 - G-mean: 0.9649737441666776
2023-12-16 01:33:17,047 - ====================================================
2023-12-16 01:33:17,281 - Model: distilbert-base-uncased 2023-12-16_01-33-17 with 11 epochs
2023-12-16 01:37:06,705 - Epoch 1 average train loss: 0.6106879932915463
2023-12-16 01:40:55,922 - Epoch 2 average train loss: 0.11057464419261498
2023-12-16 01:44:45,003 - Epoch 3 average train loss: 0.05137268220129258
2023-12-16 01:48:34,330 - Epoch 4 average train loss: 0.02548902264714022
2023-12-16 01:52:23,670 - Epoch 5 average train loss: 0.014044094527660705
2023-12-16 01:56:12,805 - Epoch 6 average train loss: 0.016419381119091723
2023-12-16 02:00:02,059 - Epoch 7 average train loss: 0.006751987155088607
2023-12-16 02:03:51,162 - Epoch 8 average train loss: 0.0027897209247149638
2023-12-16 02:07:40,521 - Epoch 9 average train loss: 0.003681732419594054
2023-12-16 02:11:29,732 - Epoch 10 average train loss: 0.0031917473549230423
2023-12-16 02:15:18,931 - Epoch 11 average train loss: 0.012206506855828218
2023-12-16 02:15:44,589 - 2023-12-16_01-33-17 with 11 epochs: Evaluation Results:
2023-12-16 02:15:44,590 - Training time: 2521.5933084487915 seconds
2023-12-16 02:15:44,590 - Inference time: 25.65000081062317 seconds
2023-12-16 02:15:44,590 - Precision: 0.9720868302390041
2023-12-16 02:15:44,590 - Recall: 0.9719833035585754
2023-12-16 02:15:44,590 - F-score: 0.9719794119979668
2023-12-16 02:15:44,590 - Accuracy: 0.971764705882353
2023-12-16 02:15:44,590 - G-mean: 0.9718739985744843
2023-12-16 02:16:13,562 - distilbert-base-uncased 2023-12-16_01-33-17 with 11 epochs: Evaluation Results (completely new data):
2023-12-16 02:16:13,562 - Training time: 2521.5933084487915 seconds
2023-12-16 02:16:13,562 - Inference time: 25.581000089645386 seconds
2023-12-16 02:16:13,562 - Precision: 0.9665646068948444
2023-12-16 02:16:13,562 - Recall: 0.9677939124965509
2023-12-16 02:16:13,562 - F-score: 0.9670278051486445
2023-12-16 02:16:13,562 - Accuracy: 0.9670588235294117
2023-12-16 02:16:13,563 - G-mean: 0.9674262981942556
2023-12-16 02:16:15,430 - ====================================================
2023-12-16 02:16:15,680 - Model: distilbert-base-uncased 2023-12-16_02-16-15 with 11 epochs
2023-12-16 02:20:05,147 - Epoch 1 average train loss: 0.5902632620317094
2023-12-16 02:23:54,680 - Epoch 2 average train loss: 0.10944440974032178
2023-12-16 02:27:44,260 - Epoch 3 average train loss: 0.052017452786950504
2023-12-16 02:31:33,731 - Epoch 4 average train loss: 0.03236471092821482
2023-12-16 02:35:23,279 - Epoch 5 average train loss: 0.017502141772385904
2023-12-16 02:39:12,844 - Epoch 6 average train loss: 0.011501946926445645
2023-12-16 02:43:02,388 - Epoch 7 average train loss: 0.006336743031587342
2023-12-16 02:46:52,141 - Epoch 8 average train loss: 0.005217870519143831
2023-12-16 02:50:41,690 - Epoch 9 average train loss: 0.005043712977541658
2023-12-16 02:54:31,208 - Epoch 10 average train loss: 0.009587690223308848
2023-12-16 02:58:20,811 - Epoch 11 average train loss: 0.007064282575986854
2023-12-16 02:58:46,403 - 2023-12-16_02-16-15 with 11 epochs: Evaluation Results:
2023-12-16 02:58:46,403 - Training time: 2525.075624227524 seconds
2023-12-16 02:58:46,403 - Inference time: 25.58500027656555 seconds
2023-12-16 02:58:46,403 - Precision: 0.9613002089888966
2023-12-16 02:58:46,403 - Recall: 0.961331643317312
2023-12-16 02:58:46,404 - F-score: 0.960757394249109
2023-12-16 02:58:46,404 - Accuracy: 0.9611764705882353
2023-12-16 02:58:46,404 - G-mean: 0.9612540538216326
2023-12-16 02:59:15,406 - distilbert-base-uncased 2023-12-16_02-16-15 with 11 epochs: Evaluation Results (completely new data):
2023-12-16 02:59:15,406 - Training time: 2525.075624227524 seconds
2023-12-16 02:59:15,406 - Inference time: 25.60900044441223 seconds
2023-12-16 02:59:15,406 - Precision: 0.9632098965191116
2023-12-16 02:59:15,406 - Recall: 0.96430488460989
2023-12-16 02:59:15,406 - F-score: 0.9632182346605859
2023-12-16 02:59:15,406 - Accuracy: 0.9635294117647059
2023-12-16 02:59:15,406 - G-mean: 0.963917070203656
2023-12-16 02:59:15,445 - Model distilbert-base-uncased 2023-12-16_02-16-15 not saved
2023-12-16 02:59:15,445 - ====================================================
2023-12-16 02:59:15,696 - Model: distilbert-base-uncased 2023-12-16_02-59-15 with 11 epochs
2023-12-16 03:03:05,092 - Epoch 1 average train loss: 0.6022822165401542
2023-12-16 03:06:54,642 - Epoch 2 average train loss: 0.11279927448753048
2023-12-16 03:10:44,141 - Epoch 3 average train loss: 0.05822701383601217
2023-12-16 03:14:33,683 - Epoch 4 average train loss: 0.03264098780941876
2023-12-16 03:18:23,196 - Epoch 5 average train loss: 0.015204177238321041
2023-12-16 03:22:12,600 - Epoch 6 average train loss: 0.012808722291443059
2023-12-16 03:26:02,013 - Epoch 7 average train loss: 0.010944973771248067
2023-12-16 03:29:51,570 - Epoch 8 average train loss: 0.011935237045673763
2023-12-16 03:33:40,993 - Epoch 9 average train loss: 0.0030153853099310415
2023-12-16 03:37:30,506 - Epoch 10 average train loss: 0.0054807985488664335
2023-12-16 03:41:19,995 - Epoch 11 average train loss: 0.005641170289334909
2023-12-16 03:41:45,835 - 2023-12-16_02-59-15 with 11 epochs: Evaluation Results:
2023-12-16 03:41:45,835 - Training time: 2524.235808134079 seconds
2023-12-16 03:41:45,835 - Inference time: 25.83199954032898 seconds
2023-12-16 03:41:45,835 - Precision: 0.9590245393791446
2023-12-16 03:41:45,835 - Recall: 0.9587167382541478
2023-12-16 03:41:45,835 - F-score: 0.9585746794639176
2023-12-16 03:41:45,835 - Accuracy: 0.9588235294117647
2023-12-16 03:41:45,836 - G-mean: 0.9587701323461099
2023-12-16 03:42:14,991 - distilbert-base-uncased 2023-12-16_02-59-15 with 11 epochs: Evaluation Results (completely new data):
2023-12-16 03:42:14,991 - Training time: 2524.235808134079 seconds
2023-12-16 03:42:14,991 - Inference time: 25.78099536895752 seconds
2023-12-16 03:42:14,991 - Precision: 0.9690204069084251
2023-12-16 03:42:14,991 - Recall: 0.9701830412656184
2023-12-16 03:42:14,991 - F-score: 0.9692030119276775
2023-12-16 03:42:14,991 - Accuracy: 0.9694117647058823
2023-12-16 03:42:14,991 - G-mean: 0.9697973263115459
2023-12-16 03:42:16,877 - ====================================================
2023-12-16 03:42:17,133 - Model: distilbert-base-uncased 2023-12-16_03-42-17 with 11 epochs
2023-12-16 03:46:06,691 - Epoch 1 average train loss: 0.5851134758223505
2023-12-16 03:49:56,216 - Epoch 2 average train loss: 0.10917103582883582
2023-12-16 03:53:45,657 - Epoch 3 average train loss: 0.05094659220317707
2023-12-16 03:57:35,170 - Epoch 4 average train loss: 0.029386526931570296
2023-12-16 04:01:24,803 - Epoch 5 average train loss: 0.017962824187433238
2023-12-16 04:05:14,469 - Epoch 6 average train loss: 0.009228477429338347
2023-12-16 04:09:04,254 - Epoch 7 average train loss: 0.006556705261429991
2023-12-16 04:12:53,931 - Epoch 8 average train loss: 0.004291198186761261
2023-12-16 04:16:43,640 - Epoch 9 average train loss: 0.001812050497486352
2023-12-16 04:20:33,289 - Epoch 10 average train loss: 0.004230438380053623
2023-12-16 04:24:23,011 - Epoch 11 average train loss: 0.010488827239731857
2023-12-16 04:24:48,885 - 2023-12-16_03-42-17 with 11 epochs: Evaluation Results:
2023-12-16 04:24:48,885 - Training time: 2525.823135614395 seconds
2023-12-16 04:24:48,885 - Inference time: 25.865999698638916 seconds
2023-12-16 04:24:48,885 - Precision: 0.9645248701921556
2023-12-16 04:24:48,885 - Recall: 0.965350578483176
2023-12-16 04:24:48,885 - F-score: 0.9648614055150514
2023-12-16 04:24:48,885 - Accuracy: 0.9647058823529412
2023-12-16 04:24:48,885 - G-mean: 0.9650281765811476
2023-12-16 04:25:18,085 - distilbert-base-uncased 2023-12-16_03-42-17 with 11 epochs: Evaluation Results (completely new data):
2023-12-16 04:25:18,085 - Training time: 2525.823135614395 seconds
2023-12-16 04:25:18,085 - Inference time: 25.796000242233276 seconds
2023-12-16 04:25:18,085 - Precision: 0.9688987756362322
2023-12-16 04:25:18,085 - Recall: 0.9699279774996767
2023-12-16 04:25:18,085 - F-score: 0.9691896220047564
2023-12-16 04:25:18,085 - Accuracy: 0.9694117647058823
2023-12-16 04:25:18,085 - G-mean: 0.9696698367514425
2023-12-16 04:25:18,122 - Model distilbert-base-uncased 2023-12-16_03-42-17 not saved
2023-12-16 04:25:18,123 - ====================================================
2023-12-16 04:25:18,376 - Model: distilbert-base-uncased 2023-12-16_04-25-18 with 12 epochs
2023-12-16 04:29:07,393 - Epoch 1 average train loss: 0.5736894657506663
2023-12-16 04:32:56,537 - Epoch 2 average train loss: 0.10992968384614762
2023-12-16 04:36:45,754 - Epoch 3 average train loss: 0.05198907743043759
2023-12-16 04:40:34,809 - Epoch 4 average train loss: 0.03052139524863485
2023-12-16 04:44:23,950 - Epoch 5 average train loss: 0.01878251064240056
2023-12-16 04:48:13,550 - Epoch 6 average train loss: 0.014388621707638617
2023-12-16 04:52:03,596 - Epoch 7 average train loss: 0.010560251899912734
2023-12-16 04:55:53,731 - Epoch 8 average train loss: 0.003688105048803503
2023-12-16 04:59:44,407 - Epoch 9 average train loss: 0.0029644777839027536
2023-12-16 05:03:33,734 - Epoch 10 average train loss: 0.007724217725476859
2023-12-16 05:07:23,090 - Epoch 11 average train loss: 0.004390743036379076
2023-12-16 05:11:12,251 - Epoch 12 average train loss: 0.00531450983873048
2023-12-16 05:11:38,018 - 2023-12-16_04-25-18 with 12 epochs: Evaluation Results:
2023-12-16 05:11:38,018 - Training time: 2753.8221368789673 seconds
2023-12-16 05:11:38,018 - Inference time: 25.7580349445343 seconds
2023-12-16 05:11:38,018 - Precision: 0.9580226027876009
2023-12-16 05:11:38,018 - Recall: 0.9565373753792439
2023-12-16 05:11:38,018 - F-score: 0.9571465676997566
2023-12-16 05:11:38,018 - Accuracy: 0.9564705882352941
2023-12-16 05:11:38,018 - G-mean: 0.9565039812243489
2023-12-16 05:12:07,038 - distilbert-base-uncased 2023-12-16_04-25-18 with 12 epochs: Evaluation Results (completely new data):
2023-12-16 05:12:07,038 - Training time: 2753.8221368789673 seconds
2023-12-16 05:12:07,039 - Inference time: 25.624963998794556 seconds
2023-12-16 05:12:07,039 - Precision: 0.9686464603154841
2023-12-16 05:12:07,039 - Recall: 0.9688215755260001
2023-12-16 05:12:07,039 - F-score: 0.9686603432739348
2023-12-16 05:12:07,039 - Accuracy: 0.9682352941176471
2023-12-16 05:12:07,039 - G-mean: 0.9685283904599488
2023-12-16 05:12:08,920 - ====================================================
2023-12-16 05:12:09,188 - Model: distilbert-base-uncased 2023-12-16_05-12-09 with 12 epochs
2023-12-16 05:15:58,610 - Epoch 1 average train loss: 0.5955552841898273
2023-12-16 05:19:48,183 - Epoch 2 average train loss: 0.11816440102370346
2023-12-16 05:23:37,557 - Epoch 3 average train loss: 0.06003831820343347
2023-12-16 05:27:26,945 - Epoch 4 average train loss: 0.03216132439672947
2023-12-16 05:31:16,411 - Epoch 5 average train loss: 0.01672809120377197
2023-12-16 05:35:05,879 - Epoch 6 average train loss: 0.013158748431322987
2023-12-16 05:38:55,332 - Epoch 7 average train loss: 0.006669163142728126
2023-12-16 05:42:44,758 - Epoch 8 average train loss: 0.004151954066405511
2023-12-16 05:46:34,317 - Epoch 9 average train loss: 0.005914539422562538
2023-12-16 05:50:23,828 - Epoch 10 average train loss: 0.0039029357165075934
2023-12-16 05:54:13,305 - Epoch 11 average train loss: 0.005624273022266982
2023-12-16 05:58:02,854 - Epoch 12 average train loss: 0.005671582721804631
2023-12-16 05:58:28,631 - 2023-12-16_05-12-09 with 12 epochs: Evaluation Results:
2023-12-16 05:58:28,631 - Training time: 2753.610151529312 seconds
2023-12-16 05:58:28,632 - Inference time: 25.770000457763672 seconds
2023-12-16 05:58:28,632 - Precision: 0.9722869535706167
2023-12-16 05:58:28,632 - Recall: 0.9718900315774786
2023-12-16 05:58:28,632 - F-score: 0.971992416003405
2023-12-16 05:58:28,632 - Accuracy: 0.971764705882353
2023-12-16 05:58:28,632 - G-mean: 0.9718273667096843
2023-12-16 05:58:57,696 - distilbert-base-uncased 2023-12-16_05-12-09 with 12 epochs: Evaluation Results (completely new data):
2023-12-16 05:58:57,696 - Training time: 2753.610151529312 seconds
2023-12-16 05:58:57,696 - Inference time: 25.740997314453125 seconds
2023-12-16 05:58:57,696 - Precision: 0.9715482165319959
2023-12-16 05:58:57,696 - Recall: 0.9721542213740539
2023-12-16 05:58:57,696 - F-score: 0.9718088049932717
2023-12-16 05:58:57,696 - Accuracy: 0.971764705882353
2023-12-16 05:58:57,696 - G-mean: 0.9719594441157745
2023-12-16 05:58:59,575 - ====================================================
2023-12-16 05:58:59,908 - Model: distilbert-base-uncased 2023-12-16_05-58-59 with 12 epochs
2023-12-16 06:02:49,411 - Epoch 1 average train loss: 0.5849954052795382
2023-12-16 06:06:39,186 - Epoch 2 average train loss: 0.10871227695004028
2023-12-16 06:10:28,808 - Epoch 3 average train loss: 0.054948962992605044
2023-12-16 06:14:18,407 - Epoch 4 average train loss: 0.026935134299975984
2023-12-16 06:18:08,115 - Epoch 5 average train loss: 0.018263265315087183
2023-12-16 06:21:57,835 - Epoch 6 average train loss: 0.016020919789683404
2023-12-16 06:25:47,458 - Epoch 7 average train loss: 0.005637681652065915
2023-12-16 06:29:37,224 - Epoch 8 average train loss: 0.0007452064422849456
2023-12-16 06:33:26,953 - Epoch 9 average train loss: 0.0015048311615486027
2023-12-16 06:37:16,650 - Epoch 10 average train loss: 0.0006198034424297101
2023-12-16 06:41:06,484 - Epoch 11 average train loss: 0.0004029734064961838
2023-12-16 06:44:56,311 - Epoch 12 average train loss: 0.000023086690709319288
2023-12-16 06:45:22,136 - 2023-12-16_05-58-59 with 12 epochs: Evaluation Results:
2023-12-16 06:45:22,136 - Training time: 2756.3492317199707 seconds
2023-12-16 06:45:22,136 - Inference time: 25.816999197006226 seconds
2023-12-16 06:45:22,136 - Precision: 0.9690792330275599
2023-12-16 06:45:22,136 - Recall: 0.9681713478958631
2023-12-16 06:45:22,136 - F-score: 0.9685176463202592
2023-12-16 06:45:22,136 - Accuracy: 0.9682352941176471
2023-12-16 06:45:22,136 - G-mean: 0.968203320478829
2023-12-16 06:45:51,502 - distilbert-base-uncased 2023-12-16_05-58-59 with 12 epochs: Evaluation Results (completely new data):
2023-12-16 06:45:51,502 - Training time: 2756.3492317199707 seconds
2023-12-16 06:45:51,502 - Inference time: 25.704996347427368 seconds
2023-12-16 06:45:51,502 - Precision: 0.9703212495445189
2023-12-16 06:45:51,502 - Recall: 0.9698876520579279
2023-12-16 06:45:51,503 - F-score: 0.9699347358478903
2023-12-16 06:45:51,503 - Accuracy: 0.9694117647058823
2023-12-16 06:45:51,503 - G-mean: 0.9696496791872417
2023-12-16 06:45:51,537 - Model distilbert-base-uncased 2023-12-16_05-58-59 not saved
2023-12-16 06:45:51,538 - ====================================================
2023-12-16 06:45:51,804 - Model: distilbert-base-uncased 2023-12-16_06-45-51 with 12 epochs
2023-12-16 06:49:41,194 - Epoch 1 average train loss: 0.5881601994791451
2023-12-16 06:53:30,582 - Epoch 2 average train loss: 0.1091167745761135
2023-12-16 06:57:20,057 - Epoch 3 average train loss: 0.053076161641408416
2023-12-16 07:01:09,470 - Epoch 4 average train loss: 0.02577409823945559
2023-12-16 07:04:58,870 - Epoch 5 average train loss: 0.01832160234122592
2023-12-16 07:08:48,321 - Epoch 6 average train loss: 0.010708784428219695
2023-12-16 07:12:37,410 - Epoch 7 average train loss: 0.007611190683468628
2023-12-16 07:16:26,543 - Epoch 8 average train loss: 0.010544871641143212
2023-12-16 07:20:16,015 - Epoch 9 average train loss: 0.0008774544639976846
2023-12-16 07:24:05,085 - Epoch 10 average train loss: 0.0036053906363924717
2023-12-16 07:27:54,269 - Epoch 11 average train loss: 0.0005680610800225302
2023-12-16 07:31:43,362 - Epoch 12 average train loss: 0.002728356390772901
2023-12-16 07:32:09,165 - 2023-12-16_06-45-51 with 12 epochs: Evaluation Results:
2023-12-16 07:32:09,165 - Training time: 2751.5011796951294 seconds
2023-12-16 07:32:09,166 - Inference time: 25.79599952697754 seconds
2023-12-16 07:32:09,166 - Precision: 0.970878275800256
2023-12-16 07:32:09,166 - Recall: 0.9704379172119889
2023-12-16 07:32:09,166 - F-score: 0.9706262188661541
2023-12-16 07:32:09,166 - Accuracy: 0.9705882352941176
2023-12-16 07:32:09,166 - G-mean: 0.970513073342798
2023-12-16 07:32:38,283 - distilbert-base-uncased 2023-12-16_06-45-51 with 12 epochs: Evaluation Results (completely new data):
2023-12-16 07:32:38,284 - Training time: 2751.5011796951294 seconds
2023-12-16 07:32:38,284 - Inference time: 25.74199938774109 seconds
2023-12-16 07:32:38,284 - Precision: 0.9696533002842594
2023-12-16 07:32:38,284 - Recall: 0.9702368622567082
2023-12-16 07:32:38,284 - F-score: 0.9698423293377874
2023-12-16 07:32:38,284 - Accuracy: 0.9694117647058823
2023-12-16 07:32:38,284 - G-mean: 0.9698242257352482
2023-12-16 07:32:38,319 - Model distilbert-base-uncased 2023-12-16_06-45-51 not saved
2023-12-16 07:32:38,320 - ====================================================
2023-12-16 07:32:38,919 - Model: distilbert-base-uncased 2023-12-16_07-32-38 with 12 epochs
2023-12-16 07:36:28,222 - Epoch 1 average train loss: 0.5699738681929953
2023-12-16 07:40:17,719 - Epoch 2 average train loss: 0.11200082281056573
2023-12-16 07:44:06,954 - Epoch 3 average train loss: 0.05832530242664849
2023-12-16 07:47:56,298 - Epoch 4 average train loss: 0.035109061956515204
2023-12-16 07:51:45,317 - Epoch 5 average train loss: 0.017199289166532895
2023-12-16 07:55:34,464 - Epoch 6 average train loss: 0.01615805836284862
2023-12-16 07:59:23,641 - Epoch 7 average train loss: 0.009243547161150834
2023-12-16 08:03:12,747 - Epoch 8 average train loss: 0.005208941911302699
2023-12-16 08:07:01,904 - Epoch 9 average train loss: 0.0101297119583914
2023-12-16 08:10:51,097 - Epoch 10 average train loss: 0.0008015497010423774
2023-12-16 08:14:40,252 - Epoch 11 average train loss: 0.0027135248339618554
2023-12-16 08:18:29,361 - Epoch 12 average train loss: 0.001657826298445905
2023-12-16 08:18:55,137 - 2023-12-16_07-32-38 with 12 epochs: Evaluation Results:
2023-12-16 08:18:55,137 - Training time: 2750.377801656723 seconds
2023-12-16 08:18:55,137 - Inference time: 25.767999410629272 seconds
2023-12-16 08:18:55,137 - Precision: 0.9670493354913287
2023-12-16 08:18:55,137 - Recall: 0.9673321407678778
2023-12-16 08:18:55,137 - F-score: 0.9670342877162262
2023-12-16 08:18:55,138 - Accuracy: 0.9670588235294117
2023-12-16 08:18:55,138 - G-mean: 0.9671954724941444
2023-12-16 08:19:24,189 - distilbert-base-uncased 2023-12-16_07-32-38 with 12 epochs: Evaluation Results (completely new data):
2023-12-16 08:19:24,190 - Training time: 2750.377801656723 seconds
2023-12-16 08:19:24,190 - Inference time: 25.690998077392578 seconds
2023-12-16 08:19:24,190 - Precision: 0.9628434082650952
2023-12-16 08:19:24,190 - Recall: 0.9643920910511119
2023-12-16 08:19:24,190 - F-score: 0.9633092533521328
2023-12-16 08:19:24,190 - Accuracy: 0.9635294117647059
2023-12-16 08:19:24,190 - G-mean: 0.9639606549029958
2023-12-16 08:19:24,227 - Model distilbert-base-uncased 2023-12-16_07-32-38 not saved
2023-12-16 08:19:24,227 - Total program time: 67167.05037212372 seconds
