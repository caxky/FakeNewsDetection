2023-12-13 12:45:16,027 - ====================================================
2023-12-13 12:45:16,226 - Model: distilbert-base-uncased 2023-12-13_12-45-16 with 1 epochs
2023-12-13 12:49:05,616 - Epoch 1 average train loss: 0.5973617835956462
2023-12-13 12:49:31,833 - 2023-12-13_12-45-16 with 1 epochs: Evaluation Results:
2023-12-13 12:49:31,833 - Training time: 229.3189992904663 seconds
2023-12-13 12:49:31,833 - Inference time: 26.208999633789062 seconds
2023-12-13 12:49:31,833 - Precision: 0.9585404949275365
2023-12-13 12:49:31,833 - Recall: 0.9581328689918506
2023-12-13 12:49:31,833 - F-score: 0.958250805532877
2023-12-13 12:49:31,833 - Accuracy: 0.9576470588235294
2023-12-13 12:49:31,838 - G-mean: 0.9578899331093295
2023-12-13 12:50:01,485 - distilbert-base-uncased 2023-12-13_12-45-16 with 1 epochs: Evaluation Results (completely new data):
2023-12-13 12:50:01,485 - Training time: 229.3189992904663 seconds
2023-12-13 12:50:01,485 - Inference time: 26.120001554489136 seconds
2023-12-13 12:50:01,485 - Precision: 0.9589250962293578
2023-12-13 12:50:01,485 - Recall: 0.9594797989545978
2023-12-13 12:50:01,485 - F-score: 0.95903913061051
2023-12-13 12:50:01,485 - Accuracy: 0.9588235294117647
2023-12-13 12:50:01,485 - G-mean: 0.9591516080541896
2023-12-13 12:50:03,321 - ====================================================
2023-12-13 12:50:03,568 - Model: distilbert-base-uncased 2023-12-13_12-50-03 with 2 epochs
2023-12-13 12:54:00,444 - Epoch 1 average train loss: 0.5769999483578345
2023-12-13 12:57:57,999 - Epoch 2 average train loss: 0.10989378306576435
2023-12-13 12:58:23,713 - 2023-12-13_12-50-03 with 2 epochs: Evaluation Results:
2023-12-13 12:58:23,714 - Training time: 474.3742003440857 seconds
2023-12-13 12:58:23,714 - Inference time: 25.705999612808228 seconds
2023-12-13 12:58:23,714 - Precision: 0.9625732432493311
2023-12-13 12:58:23,714 - Recall: 0.96091369465668
2023-12-13 12:58:23,714 - F-score: 0.9614461259682793
2023-12-13 12:58:23,714 - Accuracy: 0.9611764705882353
2023-12-13 12:58:23,714 - G-mean: 0.9610450736411944
2023-12-13 12:58:52,715 - distilbert-base-uncased 2023-12-13_12-50-03 with 2 epochs: Evaluation Results (completely new data):
2023-12-13 12:58:52,715 - Training time: 474.3742003440857 seconds
2023-12-13 12:58:52,715 - Inference time: 25.460001945495605 seconds
2023-12-13 12:58:52,715 - Precision: 0.9674017516675253
2023-12-13 12:58:52,715 - Recall: 0.9673395178950281
2023-12-13 12:58:52,716 - F-score: 0.9672115469401679
2023-12-13 12:58:52,716 - Accuracy: 0.9670588235294117
2023-12-13 12:58:52,716 - G-mean: 0.9671991605295541
2023-12-13 12:58:54,619 - ====================================================
2023-12-13 12:58:54,867 - Model: distilbert-base-uncased 2023-12-13_12-58-54 with 3 epochs
2023-12-13 13:02:45,466 - Epoch 1 average train loss: 0.6136223534976735
2023-12-13 13:06:36,265 - Epoch 2 average train loss: 0.11813618888530661
2023-12-13 13:10:26,180 - Epoch 3 average train loss: 0.05554681884015308
2023-12-13 13:10:51,794 - 2023-12-13_12-58-54 with 3 epochs: Evaluation Results:
2023-12-13 13:10:51,794 - Training time: 691.2561404705048 seconds
2023-12-13 13:10:51,794 - Inference time: 25.605997562408447 seconds
2023-12-13 13:10:51,794 - Precision: 0.9661225200480015
2023-12-13 13:10:51,794 - Recall: 0.9661708803155827
2023-12-13 13:10:51,794 - F-score: 0.96613695315865
2023-12-13 13:10:51,794 - Accuracy: 0.9658823529411765
2023-12-13 13:10:51,794 - G-mean: 0.9660266058564138
2023-12-13 13:11:20,742 - distilbert-base-uncased 2023-12-13_12-58-54 with 3 epochs: Evaluation Results (completely new data):
2023-12-13 13:11:20,742 - Training time: 691.2561404705048 seconds
2023-12-13 13:11:20,742 - Inference time: 25.51800036430359 seconds
2023-12-13 13:11:20,742 - Precision: 0.9713623276392834
2023-12-13 13:11:20,742 - Recall: 0.9723116965225069
2023-12-13 13:11:20,742 - F-score: 0.971705010682091
2023-12-13 13:11:20,742 - Accuracy: 0.971764705882353
2023-12-13 13:11:20,742 - G-mean: 0.9720381627267345
2023-12-13 13:11:22,642 - ====================================================
2023-12-13 13:11:22,914 - Model: distilbert-base-uncased 2023-12-13_13-11-22 with 4 epochs
2023-12-13 13:15:12,265 - Epoch 1 average train loss: 0.5944221166477484
2023-12-13 13:19:02,257 - Epoch 2 average train loss: 0.11166091751526384
2023-12-13 13:22:51,965 - Epoch 3 average train loss: 0.057755497776191024
2023-12-13 13:26:41,760 - Epoch 4 average train loss: 0.031026590298861265
2023-12-13 13:27:07,322 - 2023-12-13_13-11-22 with 4 epochs: Evaluation Results:
2023-12-13 13:27:07,322 - Training time: 918.7897639274597 seconds
2023-12-13 13:27:07,322 - Inference time: 25.554999113082886 seconds
2023-12-13 13:27:07,323 - Precision: 0.9709680993675385
2023-12-13 13:27:07,323 - Recall: 0.9704721771138628
2023-12-13 13:27:07,323 - F-score: 0.9706915170600603
2023-12-13 13:27:07,323 - Accuracy: 0.9705882352941176
2023-12-13 13:27:07,323 - G-mean: 0.970530204469178
2023-12-13 13:27:36,420 - distilbert-base-uncased 2023-12-13_13-11-22 with 4 epochs: Evaluation Results (completely new data):
2023-12-13 13:27:36,420 - Training time: 918.7897639274597 seconds
2023-12-13 13:27:36,420 - Inference time: 25.63622212409973 seconds
2023-12-13 13:27:36,420 - Precision: 0.9701654537782449
2023-12-13 13:27:36,420 - Recall: 0.9711489058248326
2023-12-13 13:27:36,420 - F-score: 0.9705556797825075
2023-12-13 13:27:36,420 - Accuracy: 0.9705882352941176
2023-12-13 13:27:36,421 - G-mean: 0.9708685300865084
2023-12-13 13:27:36,464 - Model distilbert-base-uncased 2023-12-13_13-11-22 not saved
2023-12-13 13:47:43,924 - ====================================================
2023-12-13 13:47:44,192 - Model: distilbert-base-uncased 2023-12-13_13-47-44 with 6 epochs
2023-12-13 13:51:33,409 - Epoch 1 average train loss: 0.5877651650327094
2023-12-13 13:55:23,449 - Epoch 2 average train loss: 0.1119526221690809
2023-12-13 13:59:14,318 - Epoch 3 average train loss: 0.05847498544546611
2023-12-13 14:03:04,776 - Epoch 4 average train loss: 0.029884635561200627
2023-12-13 14:06:55,110 - Epoch 5 average train loss: 0.018973554813659147
2023-12-13 14:10:45,551 - Epoch 6 average train loss: 0.010642450723440989
2023-12-13 14:11:11,098 - 2023-12-13_13-47-44 with 6 epochs: Evaluation Results:
2023-12-13 14:11:11,098 - Training time: 1381.3005282878876 seconds
2023-12-13 14:11:11,098 - Inference time: 25.53899836540222 seconds
2023-12-13 14:11:11,099 - Precision: 0.9650632802198501
2023-12-13 14:11:11,099 - Recall: 0.9645306917425198
2023-12-13 14:11:11,099 - F-score: 0.964711635331768
2023-12-13 14:11:11,099 - Accuracy: 0.9647058823529412
2023-12-13 14:11:11,099 - G-mean: 0.964618283070542
2023-12-13 14:11:40,485 - distilbert-base-uncased 2023-12-13_13-47-44 with 6 epochs: Evaluation Results (completely new data):
2023-12-13 14:11:40,485 - Training time: 1381.3005282878876 seconds
2023-12-13 14:11:40,485 - Inference time: 25.324000120162964 seconds
2023-12-13 14:11:40,485 - Precision: 0.9718168699688772
2023-12-13 14:11:40,485 - Recall: 0.9720318805792163
2023-12-13 14:11:40,485 - F-score: 0.9718827954948208
2023-12-13 14:11:40,485 - Accuracy: 0.971764705882353
2023-12-13 14:11:40,485 - G-mean: 0.971898284049999
2023-12-13 14:11:42,419 - ====================================================
2023-12-13 14:11:42,722 - Model: distilbert-base-uncased 2023-12-13_14-11-42 with 7 epochs
2023-12-13 14:15:32,320 - Epoch 1 average train loss: 0.5808945907564724
2023-12-13 14:19:22,817 - Epoch 2 average train loss: 0.10982146213598111
2023-12-13 14:23:13,283 - Epoch 3 average train loss: 0.055936870658222365
2023-12-13 14:27:03,726 - Epoch 4 average train loss: 0.02605405989787815
2023-12-13 14:30:54,291 - Epoch 5 average train loss: 0.015664054628132897
2023-12-13 14:34:45,166 - Epoch 6 average train loss: 0.008911350475254414
2023-12-13 14:38:37,542 - Epoch 7 average train loss: 0.010980615874731859
2023-12-13 14:39:03,613 - 2023-12-13_14-11-42 with 7 epochs: Evaluation Results:
2023-12-13 14:39:03,613 - Training time: 1614.7641351222992 seconds
2023-12-13 14:39:03,613 - Inference time: 26.063977479934692 seconds
2023-12-13 14:39:03,613 - Precision: 0.9676560046133649
2023-12-13 14:39:03,613 - Recall: 0.966953861755357
2023-12-13 14:39:03,613 - F-score: 0.9671458252306826
2023-12-13 14:39:03,614 - Accuracy: 0.9670588235294117
2023-12-13 14:39:03,614 - G-mean: 0.9670063412182761
2023-12-13 14:39:32,762 - distilbert-base-uncased 2023-12-13_14-11-42 with 7 epochs: Evaluation Results (completely new data):
2023-12-13 14:39:32,762 - Training time: 1614.7641351222992 seconds
2023-12-13 14:39:32,762 - Inference time: 25.53971791267395 seconds
2023-12-13 14:39:32,762 - Precision: 0.9739221396537587
2023-12-13 14:39:32,762 - Recall: 0.9743574619745651
2023-12-13 14:39:32,762 - F-score: 0.974114757510066
2023-12-13 14:39:32,763 - Accuracy: 0.9741176470588235
2023-12-13 14:39:32,763 - G-mean: 0.974237547137694
2023-12-13 14:39:34,740 - ====================================================
2023-12-13 14:39:35,024 - Model: distilbert-base-uncased 2023-12-13_14-39-35 with 8 epochs
2023-12-13 14:43:32,105 - Epoch 1 average train loss: 0.5963952012535404
2023-12-13 14:47:26,268 - Epoch 2 average train loss: 0.11380052597645451
2023-12-13 14:51:32,551 - Epoch 3 average train loss: 0.055089507340727484
2023-12-13 14:55:37,298 - Epoch 4 average train loss: 0.03372458054191049
2023-12-13 14:59:32,818 - Epoch 5 average train loss: 0.014551561415907653
2023-12-13 15:03:24,827 - Epoch 6 average train loss: 0.007426449753248188
2023-12-13 15:07:22,665 - Epoch 7 average train loss: 0.0023473944253740173
2023-12-13 15:11:27,107 - Epoch 8 average train loss: 0.003323070900557522
2023-12-13 15:11:52,757 - 2023-12-13_14-39-35 with 8 epochs: Evaluation Results:
2023-12-13 15:11:52,757 - Training time: 1912.022117614746 seconds
2023-12-13 15:11:52,758 - Inference time: 25.64199686050415 seconds
2023-12-13 15:11:52,758 - Precision: 0.9711531527807982
2023-12-13 15:11:52,758 - Recall: 0.9705697657313511
2023-12-13 15:11:52,758 - F-score: 0.9707288442581115
2023-12-13 15:11:52,758 - Accuracy: 0.9705882352941176
2023-12-13 15:11:52,758 - G-mean: 0.9705790004688012
2023-12-13 15:12:21,575 - distilbert-base-uncased 2023-12-13_14-39-35 with 8 epochs: Evaluation Results (completely new data):
2023-12-13 15:12:21,575 - Training time: 1912.022117614746 seconds
2023-12-13 15:12:21,575 - Inference time: 25.31395721435547 seconds
2023-12-13 15:12:21,575 - Precision: 0.9665697978314001
2023-12-13 15:12:21,575 - Recall: 0.9676297160145853
2023-12-13 15:12:21,575 - F-score: 0.9669456836353023
2023-12-13 15:12:21,575 - Accuracy: 0.9670588235294117
2023-12-13 15:12:21,575 - G-mean: 0.9673442276569203
2023-12-13 15:12:21,615 - Model distilbert-base-uncased 2023-12-13_14-39-35 not saved
2023-12-13 15:12:21,615 - Total program time: 8841.3764462471 seconds
