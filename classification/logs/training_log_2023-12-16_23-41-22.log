2023-12-16 23:41:39,122 - ====================================================
2023-12-16 23:41:39,679 - Model: distilbert-base-uncased 2023-12-16_23-41-39 with 7 epochs
2023-12-16 23:45:32,605 - Epoch 1 average train loss: 0.571156
2023-12-16 23:49:30,443 - Epoch 2 average train loss: 0.108039
2023-12-16 23:53:29,286 - Epoch 3 average train loss: 0.056148
2023-12-16 23:57:34,426 - Epoch 4 average train loss: 0.030039
2023-12-17 00:01:21,289 - Epoch 5 average train loss: 0.019162
2023-12-17 00:05:05,091 - Epoch 6 average train loss: 0.009042
2023-12-17 00:08:47,910 - Epoch 7 average train loss: 0.001542
2023-12-17 00:09:12,730 - 2023-12-16_23-41-39 with 7 epochs: Evaluation Results:
2023-12-17 00:09:12,734 - Training time: 1628.1568524837494 seconds
2023-12-17 00:09:12,734 - Inference time: 24.811999559402466 seconds
2023-12-17 00:09:12,734 - Precision: 0.9662938047605009
2023-12-17 00:09:12,734 - Recall: 0.9657858799695495
2023-12-17 00:09:12,734 - F-score: 0.9659766924020232
2023-12-17 00:09:12,735 - Accuracy: 0.9658823529411765
2023-12-17 00:09:12,735 - G-mean: 0.9658341152508297
2023-12-17 00:09:40,937 - distilbert-base-uncased 2023-12-16_23-41-39 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 00:09:40,937 - Training time: 1628.1568524837494 seconds
2023-12-17 00:09:40,937 - Inference time: 24.552998542785645 seconds
2023-12-17 00:09:40,937 - Precision: 0.9724507842272315
2023-12-17 00:09:40,937 - Recall: 0.9737937541534791
2023-12-17 00:09:40,937 - F-score: 0.9729295660055112
2023-12-17 00:09:40,937 - Accuracy: 0.9729411764705882
2023-12-17 00:09:40,937 - G-mean: 0.9733673719648696
2023-12-17 00:09:42,939 - ====================================================
2023-12-17 00:09:43,251 - Model: distilbert-base-uncased 2023-12-17_00-09-43 with 7 epochs
2023-12-17 00:13:25,660 - Epoch 1 average train loss: 0.589917
2023-12-17 00:17:08,005 - Epoch 2 average train loss: 0.113434
2023-12-17 00:20:50,134 - Epoch 3 average train loss: 0.055837
2023-12-17 00:24:32,495 - Epoch 4 average train loss: 0.026748
2023-12-17 00:28:14,872 - Epoch 5 average train loss: 0.016811
2023-12-17 00:31:57,325 - Epoch 6 average train loss: 0.009492
2023-12-17 00:35:39,211 - Epoch 7 average train loss: 0.005929
2023-12-17 00:36:03,854 - 2023-12-17_00-09-43 with 7 epochs: Evaluation Results:
2023-12-17 00:36:03,854 - Training time: 1555.9020760059357 seconds
2023-12-17 00:36:03,854 - Inference time: 24.635998249053955 seconds
2023-12-17 00:36:03,854 - Precision: 0.9665444554502001
2023-12-17 00:36:03,854 - Recall: 0.9657559367040671
2023-12-17 00:36:03,854 - F-score: 0.9660372632014884
2023-12-17 00:36:03,854 - Accuracy: 0.9658823529411765
2023-12-17 00:36:03,854 - G-mean: 0.9658191427542914
2023-12-17 00:36:31,793 - distilbert-base-uncased 2023-12-17_00-09-43 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 00:36:31,793 - Training time: 1555.9020760059357 seconds
2023-12-17 00:36:31,794 - Inference time: 24.590010166168213 seconds
2023-12-17 00:36:31,794 - Precision: 0.969627516206501
2023-12-17 00:36:31,794 - Recall: 0.9694785553281833
2023-12-17 00:36:31,794 - F-score: 0.9694002147265884
2023-12-17 00:36:31,794 - Accuracy: 0.9694117647058823
2023-12-17 00:36:31,794 - G-mean: 0.9694451594418343
2023-12-17 00:36:31,837 - Model distilbert-base-uncased 2023-12-17_00-09-43 not saved
2023-12-17 00:36:31,838 - ====================================================
2023-12-17 00:36:32,091 - Model: distilbert-base-uncased 2023-12-17_00-36-32 with 7 epochs
2023-12-17 00:40:11,795 - Epoch 1 average train loss: 0.589127
2023-12-17 00:43:52,233 - Epoch 2 average train loss: 0.111628
2023-12-17 00:47:32,836 - Epoch 3 average train loss: 0.055227
2023-12-17 00:51:13,711 - Epoch 4 average train loss: 0.035039
2023-12-17 00:54:54,029 - Epoch 5 average train loss: 0.021664
2023-12-17 00:58:34,267 - Epoch 6 average train loss: 0.011758
2023-12-17 01:02:15,052 - Epoch 7 average train loss: 0.011719
2023-12-17 01:02:39,453 - 2023-12-17_00-36-32 with 7 epochs: Evaluation Results:
2023-12-17 01:02:39,453 - Training time: 1542.8954546451569 seconds
2023-12-17 01:02:39,453 - Inference time: 24.39299511909485 seconds
2023-12-17 01:02:39,453 - Precision: 0.9643589097736853
2023-12-17 01:02:39,453 - Recall: 0.9633952209551027
2023-12-17 01:02:39,454 - F-score: 0.9637828059355472
2023-12-17 01:02:39,454 - Accuracy: 0.9635294117647059
2023-12-17 01:02:39,454 - G-mean: 0.9634623140236461
2023-12-17 01:03:07,173 - distilbert-base-uncased 2023-12-17_00-36-32 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 01:03:07,173 - Training time: 1542.8954546451569 seconds
2023-12-17 01:03:07,173 - Inference time: 24.286967515945435 seconds
2023-12-17 01:03:07,173 - Precision: 0.9698621118550397
2023-12-17 01:03:07,173 - Recall: 0.9698559598889622
2023-12-17 01:03:07,173 - F-score: 0.9695833876007482
2023-12-17 01:03:07,173 - Accuracy: 0.9694117647058823
2023-12-17 01:03:07,173 - G-mean: 0.9696338368613567
2023-12-17 01:03:07,212 - Model distilbert-base-uncased 2023-12-17_00-36-32 not saved
2023-12-17 01:03:07,212 - ====================================================
2023-12-17 01:03:07,539 - Model: distilbert-base-uncased 2023-12-17_01-03-07 with 7 epochs
2023-12-17 01:06:47,673 - Epoch 1 average train loss: 0.582742
2023-12-17 01:10:28,040 - Epoch 2 average train loss: 0.114703
2023-12-17 01:14:08,314 - Epoch 3 average train loss: 0.059298
2023-12-17 01:17:48,323 - Epoch 4 average train loss: 0.031064
2023-12-17 01:21:28,460 - Epoch 5 average train loss: 0.015927
2023-12-17 01:25:08,396 - Epoch 6 average train loss: 0.010145
2023-12-17 01:28:48,501 - Epoch 7 average train loss: 0.011116
2023-12-17 01:29:12,941 - 2023-12-17_01-03-07 with 7 epochs: Evaluation Results:
2023-12-17 01:29:12,941 - Training time: 1540.8975825309753 seconds
2023-12-17 01:29:12,941 - Inference time: 24.43199920654297 seconds
2023-12-17 01:29:12,941 - Precision: 0.9720794818992644
2023-12-17 01:29:12,941 - Recall: 0.9719533602930932
2023-12-17 01:29:12,941 - F-score: 0.9719174006468692
2023-12-17 01:29:12,941 - Accuracy: 0.971764705882353
2023-12-17 01:29:12,941 - G-mean: 0.9718590285100933
2023-12-17 01:29:40,561 - distilbert-base-uncased 2023-12-17_01-03-07 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 01:29:40,562 - Training time: 1540.8975825309753 seconds
2023-12-17 01:29:40,562 - Inference time: 24.29099988937378 seconds
2023-12-17 01:29:40,562 - Precision: 0.9664119966188682
2023-12-17 01:29:40,562 - Recall: 0.9673789688850352
2023-12-17 01:29:40,562 - F-score: 0.9667268092810295
2023-12-17 01:29:40,562 - Accuracy: 0.9670588235294117
2023-12-17 01:29:40,562 - G-mean: 0.9672188829613788
2023-12-17 01:29:40,598 - Model distilbert-base-uncased 2023-12-17_01-03-07 not saved
2023-12-17 01:29:40,598 - ====================================================
2023-12-17 01:29:40,853 - Model: distilbert-base-uncased 2023-12-17_01-29-40 with 7 epochs
2023-12-17 01:33:19,963 - Epoch 1 average train loss: 0.607183
2023-12-17 01:36:59,283 - Epoch 2 average train loss: 0.111677
2023-12-17 01:40:38,321 - Epoch 3 average train loss: 0.052148
2023-12-17 01:44:17,526 - Epoch 4 average train loss: 0.029242
2023-12-17 01:47:56,847 - Epoch 5 average train loss: 0.012890
2023-12-17 01:51:36,151 - Epoch 6 average train loss: 0.008446
2023-12-17 01:55:15,330 - Epoch 7 average train loss: 0.010289
2023-12-17 01:55:39,819 - 2023-12-17_01-29-40 with 7 epochs: Evaluation Results:
2023-12-17 01:55:39,819 - Training time: 1534.4123516082764 seconds
2023-12-17 01:55:39,819 - Inference time: 24.481998443603516 seconds
2023-12-17 01:55:39,820 - Precision: 0.9623598304011495
2023-12-17 01:55:39,820 - Recall: 0.9606879183625834
2023-12-17 01:55:39,820 - F-score: 0.961372749766098
2023-12-17 01:55:39,820 - Accuracy: 0.9611764705882353
2023-12-17 01:55:39,820 - G-mean: 0.9609321634270063
2023-12-17 01:56:07,635 - distilbert-base-uncased 2023-12-17_01-29-40 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 01:56:07,635 - Training time: 1534.4123516082764 seconds
2023-12-17 01:56:07,635 - Inference time: 24.45600199699402 seconds
2023-12-17 01:56:07,635 - Precision: 0.9759307131848116
2023-12-17 01:56:07,635 - Recall: 0.9756716622808177
2023-12-17 01:56:07,635 - F-score: 0.9756547317609687
2023-12-17 01:56:07,635 - Accuracy: 0.9752941176470589
2023-12-17 01:56:07,635 - G-mean: 0.9754828716986318
2023-12-17 01:56:09,557 - ====================================================
2023-12-17 01:56:09,831 - Model: distilbert-base-uncased 2023-12-17_01-56-09 with 7 epochs
2023-12-17 01:59:49,734 - Epoch 1 average train loss: 0.593856
2023-12-17 02:03:29,879 - Epoch 2 average train loss: 0.112558
2023-12-17 02:07:09,892 - Epoch 3 average train loss: 0.054523
2023-12-17 02:10:49,838 - Epoch 4 average train loss: 0.029893
2023-12-17 02:14:29,740 - Epoch 5 average train loss: 0.019301
2023-12-17 02:18:09,707 - Epoch 6 average train loss: 0.015182
2023-12-17 02:21:49,665 - Epoch 7 average train loss: 0.006711
2023-12-17 02:22:14,381 - 2023-12-17_01-56-09 with 7 epochs: Evaluation Results:
2023-12-17 02:22:14,381 - Training time: 1539.7792756557465 seconds
2023-12-17 02:22:14,381 - Inference time: 24.708000421524048 seconds
2023-12-17 02:22:14,381 - Precision: 0.9662621667382407
2023-12-17 02:22:14,381 - Recall: 0.9657568111558088
2023-12-17 02:22:14,381 - F-score: 0.9659122329854638
2023-12-17 02:22:14,381 - Accuracy: 0.9658823529411765
2023-12-17 02:22:14,381 - G-mean: 0.9658195800086785
2023-12-17 02:22:42,447 - distilbert-base-uncased 2023-12-17_01-56-09 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 02:22:42,447 - Training time: 1539.7792756557465 seconds
2023-12-17 02:22:42,448 - Inference time: 24.611027002334595 seconds
2023-12-17 02:22:42,448 - Precision: 0.9702410972281996
2023-12-17 02:22:42,448 - Recall: 0.9713106976096771
2023-12-17 02:22:42,448 - F-score: 0.9706315647646967
2023-12-17 02:22:42,448 - Accuracy: 0.9705882352941176
2023-12-17 02:22:42,448 - G-mean: 0.9709493992558391
2023-12-17 02:22:42,486 - Model distilbert-base-uncased 2023-12-17_01-56-09 not saved
2023-12-17 02:22:42,486 - ====================================================
2023-12-17 02:22:42,710 - Model: distilbert-base-uncased 2023-12-17_02-22-42 with 7 epochs
2023-12-17 02:26:22,078 - Epoch 1 average train loss: 0.590924
2023-12-17 02:30:01,570 - Epoch 2 average train loss: 0.113448
2023-12-17 02:33:41,075 - Epoch 3 average train loss: 0.051597
2023-12-17 02:37:20,612 - Epoch 4 average train loss: 0.026888
2023-12-17 02:41:00,182 - Epoch 5 average train loss: 0.013730
2023-12-17 02:44:39,764 - Epoch 6 average train loss: 0.010117
2023-12-17 02:48:19,319 - Epoch 7 average train loss: 0.008207
2023-12-17 02:48:43,768 - 2023-12-17_02-22-42 with 7 epochs: Evaluation Results:
2023-12-17 02:48:43,768 - Training time: 1536.5487415790558 seconds
2023-12-17 02:48:43,768 - Inference time: 24.43999981880188 seconds
2023-12-17 02:48:43,768 - Precision: 0.9684330363688515
2023-12-17 02:48:43,768 - Recall: 0.9684949314655522
2023-12-17 02:48:43,768 - F-score: 0.9683609085266796
2023-12-17 02:48:43,768 - Accuracy: 0.9682352941176471
2023-12-17 02:48:43,769 - G-mean: 0.9683651040898775
2023-12-17 02:49:11,527 - distilbert-base-uncased 2023-12-17_02-22-42 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 02:49:11,527 - Training time: 1536.5487415790558 seconds
2023-12-17 02:49:11,527 - Inference time: 24.370965242385864 seconds
2023-12-17 02:49:11,527 - Precision: 0.9699891008889997
2023-12-17 02:49:11,527 - Recall: 0.9712174256285803
2023-12-17 02:49:11,527 - F-score: 0.9703934389933208
2023-12-17 02:49:11,527 - Accuracy: 0.9705882352941176
2023-12-17 02:49:11,527 - G-mean: 0.970902779493261
2023-12-17 02:49:11,591 - Model distilbert-base-uncased 2023-12-17_02-22-42 not saved
2023-12-17 02:49:11,591 - ====================================================
2023-12-17 02:49:11,850 - Model: distilbert-base-uncased 2023-12-17_02-49-11 with 7 epochs
2023-12-17 02:52:51,041 - Epoch 1 average train loss: 0.629871
2023-12-17 02:56:30,428 - Epoch 2 average train loss: 0.118180
2023-12-17 03:00:09,764 - Epoch 3 average train loss: 0.057183
2023-12-17 03:03:49,072 - Epoch 4 average train loss: 0.030586
2023-12-17 03:07:28,432 - Epoch 5 average train loss: 0.018961
2023-12-17 03:11:08,140 - Epoch 6 average train loss: 0.009317
2023-12-17 03:14:48,049 - Epoch 7 average train loss: 0.006514
2023-12-17 03:15:12,502 - 2023-12-17_02-49-11 with 7 epochs: Evaluation Results:
2023-12-17 03:15:12,502 - Training time: 1536.1401708126068 seconds
2023-12-17 03:15:12,502 - Inference time: 24.444998741149902 seconds
2023-12-17 03:15:12,502 - Precision: 0.9709671184932965
2023-12-17 03:15:12,502 - Recall: 0.9707914440471604
2023-12-17 03:15:12,502 - F-score: 0.9707586179197163
2023-12-17 03:15:12,502 - Accuracy: 0.9705882352941176
2023-12-17 03:15:12,502 - G-mean: 0.9706898343530551
2023-12-17 03:15:40,188 - distilbert-base-uncased 2023-12-17_02-49-11 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 03:15:40,188 - Training time: 1536.1401708126068 seconds
2023-12-17 03:15:40,188 - Inference time: 24.316965103149414 seconds
2023-12-17 03:15:40,188 - Precision: 0.9679110650733591
2023-12-17 03:15:40,188 - Recall: 0.9686410971036814
2023-12-17 03:15:40,188 - F-score: 0.9679573539800851
2023-12-17 03:15:40,188 - Accuracy: 0.9682352941176471
2023-12-17 03:15:40,189 - G-mean: 0.9684381743552984
2023-12-17 03:15:40,227 - Model distilbert-base-uncased 2023-12-17_02-49-11 not saved
2023-12-17 03:15:40,228 - ====================================================
2023-12-17 03:15:40,484 - Model: distilbert-base-uncased 2023-12-17_03-15-40 with 7 epochs
2023-12-17 03:19:19,742 - Epoch 1 average train loss: 0.565975
2023-12-17 03:22:59,225 - Epoch 2 average train loss: 0.115175
2023-12-17 03:26:38,771 - Epoch 3 average train loss: 0.056032
2023-12-17 03:30:18,252 - Epoch 4 average train loss: 0.033567
2023-12-17 03:33:57,743 - Epoch 5 average train loss: 0.020026
2023-12-17 03:37:37,316 - Epoch 6 average train loss: 0.013230
2023-12-17 03:41:16,882 - Epoch 7 average train loss: 0.004907
2023-12-17 03:41:41,349 - 2023-12-17_03-15-40 with 7 epochs: Evaluation Results:
2023-12-17 03:41:41,349 - Training time: 1536.338500738144 seconds
2023-12-17 03:41:41,349 - Inference time: 24.45900011062622 seconds
2023-12-17 03:41:41,349 - Precision: 0.9611303588884234
2023-12-17 03:41:41,349 - Recall: 0.9600291896569167
2023-12-17 03:41:41,349 - F-score: 0.96042255791595
2023-12-17 03:41:41,349 - Accuracy: 0.96
2023-12-17 03:41:41,349 - G-mean: 0.9600145947175178
2023-12-17 03:42:09,082 - distilbert-base-uncased 2023-12-17_03-15-40 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 03:42:09,082 - Training time: 1536.338500738144 seconds
2023-12-17 03:42:09,082 - Inference time: 24.412999629974365 seconds
2023-12-17 03:42:09,082 - Precision: 0.9664406049614186
2023-12-17 03:42:09,082 - Recall: 0.9662699991784505
2023-12-17 03:42:09,082 - F-score: 0.9660890739695089
2023-12-17 03:42:09,082 - Accuracy: 0.9658823529411765
2023-12-17 03:42:09,082 - G-mean: 0.9660761566165219
2023-12-17 03:42:09,119 - Model distilbert-base-uncased 2023-12-17_03-15-40 not saved
2023-12-17 03:42:09,119 - ====================================================
2023-12-17 03:42:09,381 - Model: distilbert-base-uncased 2023-12-17_03-42-09 with 7 epochs
2023-12-17 03:45:51,985 - Epoch 1 average train loss: 0.583282
2023-12-17 03:49:35,824 - Epoch 2 average train loss: 0.111984
2023-12-17 03:53:20,012 - Epoch 3 average train loss: 0.056519
2023-12-17 03:57:04,148 - Epoch 4 average train loss: 0.031857
2023-12-17 04:00:48,280 - Epoch 5 average train loss: 0.020561
2023-12-17 04:04:30,960 - Epoch 6 average train loss: 0.012764
2023-12-17 04:08:12,492 - Epoch 7 average train loss: 0.006973
2023-12-17 04:08:36,950 - 2023-12-17_03-42-09 with 7 epochs: Evaluation Results:
2023-12-17 04:08:36,951 - Training time: 1563.0505771636963 seconds
2023-12-17 04:08:36,955 - Inference time: 24.44999933242798 seconds
2023-12-17 04:08:36,955 - Precision: 0.9697974818443666
2023-12-17 04:08:36,956 - Recall: 0.9695644501821297
2023-12-17 04:08:36,956 - F-score: 0.9695632177717716
2023-12-17 04:08:36,956 - Accuracy: 0.9694117647058823
2023-12-17 04:08:36,956 - G-mean: 0.9694881044381859
2023-12-17 04:09:04,696 - distilbert-base-uncased 2023-12-17_03-42-09 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 04:09:04,697 - Training time: 1563.0505771636963 seconds
2023-12-17 04:09:04,698 - Inference time: 24.355002403259277 seconds
2023-12-17 04:09:04,698 - Precision: 0.970194816699955
2023-12-17 04:09:04,698 - Recall: 0.9710599504801272
2023-12-17 04:09:04,698 - F-score: 0.9704964177382742
2023-12-17 04:09:04,698 - Accuracy: 0.9705882352941176
2023-12-17 04:09:04,699 - G-mean: 0.9708240642368214
2023-12-17 04:09:04,745 - Model distilbert-base-uncased 2023-12-17_03-42-09 not saved
2023-12-17 04:09:04,746 - ====================================================
2023-12-17 04:09:05,412 - Model: distilbert-base-uncased 2023-12-17_04-09-05 with 8 epochs
2023-12-17 04:12:45,041 - Epoch 1 average train loss: 0.609512
2023-12-17 04:16:24,541 - Epoch 2 average train loss: 0.120249
2023-12-17 04:20:04,051 - Epoch 3 average train loss: 0.062729
2023-12-17 04:23:43,813 - Epoch 4 average train loss: 0.032762
2023-12-17 04:27:23,771 - Epoch 5 average train loss: 0.018849
2023-12-17 04:31:03,272 - Epoch 6 average train loss: 0.009715
2023-12-17 04:34:42,715 - Epoch 7 average train loss: 0.004530
2023-12-17 04:38:22,161 - Epoch 8 average train loss: 0.017299
2023-12-17 04:38:46,691 - 2023-12-17_04-09-05 with 8 epochs: Evaluation Results:
2023-12-17 04:38:46,691 - Training time: 1756.6958854198456 seconds
2023-12-17 04:38:46,691 - Inference time: 24.52199673652649 seconds
2023-12-17 04:38:46,691 - Precision: 0.9615016228766613
2023-12-17 04:38:46,691 - Recall: 0.9614548585638912
2023-12-17 04:38:46,691 - F-score: 0.9611256231849084
2023-12-17 04:38:46,691 - Accuracy: 0.9611764705882353
2023-12-17 04:38:46,691 - G-mean: 0.9613156544987458
2023-12-17 04:39:14,432 - distilbert-base-uncased 2023-12-17_04-09-05 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 04:39:14,432 - Training time: 1756.6958854198456 seconds
2023-12-17 04:39:14,432 - Inference time: 24.40303325653076 seconds
2023-12-17 04:39:14,432 - Precision: 0.962011793724136
2023-12-17 04:39:14,432 - Recall: 0.9629512333136304
2023-12-17 04:39:14,432 - F-score: 0.9618076435855702
2023-12-17 04:39:14,432 - Accuracy: 0.9623529411764706
2023-12-17 04:39:14,432 - G-mean: 0.9626520407649287
2023-12-17 04:39:16,270 - ====================================================
2023-12-17 04:39:16,530 - Model: distilbert-base-uncased 2023-12-17_04-39-16 with 8 epochs
2023-12-17 04:42:55,973 - Epoch 1 average train loss: 0.586965
2023-12-17 04:46:35,550 - Epoch 2 average train loss: 0.102729
2023-12-17 04:50:15,043 - Epoch 3 average train loss: 0.054149
2023-12-17 04:53:54,463 - Epoch 4 average train loss: 0.032439
2023-12-17 04:57:34,030 - Epoch 5 average train loss: 0.020834
2023-12-17 05:01:13,512 - Epoch 6 average train loss: 0.021736
2023-12-17 05:04:52,925 - Epoch 7 average train loss: 0.010587
2023-12-17 05:08:32,354 - Epoch 8 average train loss: 0.008425
2023-12-17 05:08:56,860 - 2023-12-17_04-39-16 with 8 epochs: Evaluation Results:
2023-12-17 05:08:56,860 - Training time: 1755.7711856365204 seconds
2023-12-17 05:08:56,860 - Inference time: 24.498979568481445 seconds
2023-12-17 05:08:56,860 - Precision: 0.9628893763279729
2023-12-17 05:08:56,860 - Recall: 0.9623274511420086
2023-12-17 05:08:56,860 - F-score: 0.9625404822136483
2023-12-17 05:08:56,860 - Accuracy: 0.9623529411764706
2023-12-17 05:08:56,860 - G-mean: 0.9623401960748436
2023-12-17 05:09:24,533 - distilbert-base-uncased 2023-12-17_04-39-16 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 05:09:24,533 - Training time: 1755.7711856365204 seconds
2023-12-17 05:09:24,533 - Inference time: 24.38800024986267 seconds
2023-12-17 05:09:24,533 - Precision: 0.9651495821263569
2023-12-17 05:09:24,533 - Recall: 0.9664977430341348
2023-12-17 05:09:24,533 - F-score: 0.9656313470111101
2023-12-17 05:09:24,533 - Accuracy: 0.9658823529411765
2023-12-17 05:09:24,533 - G-mean: 0.9661899989930277
2023-12-17 05:09:26,413 - ====================================================
2023-12-17 05:09:26,654 - Model: distilbert-base-uncased 2023-12-17_05-09-26 with 8 epochs
2023-12-17 05:13:05,774 - Epoch 1 average train loss: 0.565900
2023-12-17 05:16:44,851 - Epoch 2 average train loss: 0.108418
2023-12-17 05:20:23,986 - Epoch 3 average train loss: 0.054620
2023-12-17 05:24:03,026 - Epoch 4 average train loss: 0.029974
2023-12-17 05:27:42,216 - Epoch 5 average train loss: 0.020547
2023-12-17 05:31:21,347 - Epoch 6 average train loss: 0.012033
2023-12-17 05:35:00,412 - Epoch 7 average train loss: 0.009743
2023-12-17 05:38:39,533 - Epoch 8 average train loss: 0.004042
2023-12-17 05:39:03,982 - 2023-12-17_05-09-26 with 8 epochs: Evaluation Results:
2023-12-17 05:39:03,982 - Training time: 1752.823698759079 seconds
2023-12-17 05:39:03,982 - Inference time: 24.441001176834106 seconds
2023-12-17 05:39:03,982 - Precision: 0.9617230254224071
2023-12-17 05:39:03,982 - Recall: 0.9611347171788518
2023-12-17 05:39:03,982 - F-score: 0.9613408019362302
2023-12-17 05:39:03,982 - Accuracy: 0.9611764705882353
2023-12-17 05:39:03,982 - G-mean: 0.9611555936568181
2023-12-17 05:39:31,629 - distilbert-base-uncased 2023-12-17_05-09-26 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 05:39:31,629 - Training time: 1752.823698759079 seconds
2023-12-17 05:39:31,629 - Inference time: 24.325963735580444 seconds
2023-12-17 05:39:31,629 - Precision: 0.9664899100202167
2023-12-17 05:39:31,629 - Recall: 0.9674097866022594
2023-12-17 05:39:31,629 - F-score: 0.9668422630855259
2023-12-17 05:39:31,629 - Accuracy: 0.9670588235294117
2023-12-17 05:39:31,630 - G-mean: 0.9672342891473711
2023-12-17 05:39:33,495 - ====================================================
2023-12-17 05:39:33,748 - Model: distilbert-base-uncased 2023-12-17_05-39-33 with 8 epochs
2023-12-17 05:43:12,731 - Epoch 1 average train loss: 0.572090
2023-12-17 05:46:51,819 - Epoch 2 average train loss: 0.107492
2023-12-17 05:50:30,797 - Epoch 3 average train loss: 0.059677
2023-12-17 05:54:09,765 - Epoch 4 average train loss: 0.028923
2023-12-17 05:57:48,768 - Epoch 5 average train loss: 0.020021
2023-12-17 06:01:27,812 - Epoch 6 average train loss: 0.010550
2023-12-17 06:05:06,802 - Epoch 7 average train loss: 0.005386
2023-12-17 06:08:45,775 - Epoch 8 average train loss: 0.009260
2023-12-17 06:09:10,271 - 2023-12-17_05-39-33 with 8 epochs: Evaluation Results:
2023-12-17 06:09:10,271 - Training time: 1751.9702384471893 seconds
2023-12-17 06:09:10,272 - Inference time: 24.490001916885376 seconds
2023-12-17 06:09:10,272 - Precision: 0.9690536677805145
2023-12-17 06:09:10,272 - Recall: 0.9681465957185139
2023-12-17 06:09:10,272 - F-score: 0.9684779500440618
2023-12-17 06:09:10,272 - Accuracy: 0.9682352941176471
2023-12-17 06:09:10,272 - G-mean: 0.9681909439023452
2023-12-17 06:09:37,976 - distilbert-base-uncased 2023-12-17_05-39-33 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 06:09:37,976 - Training time: 1751.9702384471893 seconds
2023-12-17 06:09:37,976 - Inference time: 24.393000841140747 seconds
2023-12-17 06:09:37,976 - Precision: 0.9704701895733239
2023-12-17 06:09:37,976 - Recall: 0.9706764803794732
2023-12-17 06:09:37,976 - F-score: 0.9705588004100651
2023-12-17 06:09:37,976 - Accuracy: 0.9705882352941176
2023-12-17 06:09:37,976 - G-mean: 0.9706323568339447
2023-12-17 06:09:39,819 - ====================================================
2023-12-17 06:09:40,096 - Model: distilbert-base-uncased 2023-12-17_06-09-40 with 8 epochs
2023-12-17 06:13:19,181 - Epoch 1 average train loss: 0.586156
2023-12-17 06:16:58,437 - Epoch 2 average train loss: 0.115347
2023-12-17 06:20:37,685 - Epoch 3 average train loss: 0.053726
2023-12-17 06:24:16,930 - Epoch 4 average train loss: 0.026381
2023-12-17 06:27:56,225 - Epoch 5 average train loss: 0.015030
2023-12-17 06:31:35,637 - Epoch 6 average train loss: 0.009691
2023-12-17 06:35:15,202 - Epoch 7 average train loss: 0.003386
2023-12-17 06:38:55,131 - Epoch 8 average train loss: 0.002823
2023-12-17 06:39:19,542 - 2023-12-17_06-09-40 with 8 epochs: Evaluation Results:
2023-12-17 06:39:19,543 - Training time: 1754.9812641143799 seconds
2023-12-17 06:39:19,543 - Inference time: 24.404034852981567 seconds
2023-12-17 06:39:19,543 - Precision: 0.9676222602099627
2023-12-17 06:39:19,543 - Recall: 0.9674254127489746
2023-12-17 06:39:19,543 - F-score: 0.967454873373941
2023-12-17 06:39:19,543 - Accuracy: 0.9670588235294117
2023-12-17 06:39:19,543 - G-mean: 0.9672421007718176
2023-12-17 06:39:47,221 - distilbert-base-uncased 2023-12-17_06-09-40 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 06:39:47,221 - Training time: 1754.9812641143799 seconds
2023-12-17 06:39:47,221 - Inference time: 24.335999011993408 seconds
2023-12-17 06:39:47,221 - Precision: 0.9690030344570036
2023-12-17 06:39:47,221 - Recall: 0.9698928431460612
2023-12-17 06:39:47,221 - F-score: 0.969368959481448
2023-12-17 06:39:47,221 - Accuracy: 0.9694117647058823
2023-12-17 06:39:47,221 - G-mean: 0.9696522740909901
2023-12-17 06:39:47,255 - Model distilbert-base-uncased 2023-12-17_06-09-40 not saved
2023-12-17 06:39:47,255 - ====================================================
2023-12-17 06:39:47,512 - Model: distilbert-base-uncased 2023-12-17_06-39-47 with 8 epochs
2023-12-17 06:43:26,881 - Epoch 1 average train loss: 0.590776
2023-12-17 06:47:06,925 - Epoch 2 average train loss: 0.111443
2023-12-17 06:50:47,136 - Epoch 3 average train loss: 0.059084
2023-12-17 06:54:27,209 - Epoch 4 average train loss: 0.032559
2023-12-17 06:58:07,286 - Epoch 5 average train loss: 0.025296
2023-12-17 07:01:47,657 - Epoch 6 average train loss: 0.012725
2023-12-17 07:05:27,562 - Epoch 7 average train loss: 0.009164
2023-12-17 07:09:07,484 - Epoch 8 average train loss: 0.004946
2023-12-17 07:09:31,972 - 2023-12-17_06-39-47 with 8 epochs: Evaluation Results:
2023-12-17 07:09:31,972 - Training time: 1759.9134075641632 seconds
2023-12-17 07:09:31,972 - Inference time: 24.479990482330322 seconds
2023-12-17 07:09:31,972 - Precision: 0.9662470482685375
2023-12-17 07:09:31,972 - Recall: 0.9645211840179952
2023-12-17 07:09:31,972 - F-score: 0.9651172802331345
2023-12-17 07:09:31,972 - Accuracy: 0.9647058823529412
2023-12-17 07:09:31,972 - G-mean: 0.964613528764854
2023-12-17 07:09:59,750 - distilbert-base-uncased 2023-12-17_06-39-47 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 07:09:59,750 - Training time: 1759.9134075641632 seconds
2023-12-17 07:09:59,750 - Inference time: 24.423001050949097 seconds
2023-12-17 07:09:59,750 - Precision: 0.972350354054264
2023-12-17 07:09:59,750 - Recall: 0.9726557156331539
2023-12-17 07:09:59,750 - F-score: 0.9722752689268633
2023-12-17 07:09:59,750 - Accuracy: 0.971764705882353
2023-12-17 07:09:59,750 - G-mean: 0.9722101086838386
2023-12-17 07:10:01,604 - ====================================================
2023-12-17 07:10:01,837 - Model: distilbert-base-uncased 2023-12-17_07-10-01 with 8 epochs
2023-12-17 07:13:41,196 - Epoch 1 average train loss: 0.583250
2023-12-17 07:17:21,100 - Epoch 2 average train loss: 0.111965
2023-12-17 07:21:00,867 - Epoch 3 average train loss: 0.053142
2023-12-17 07:24:40,489 - Epoch 4 average train loss: 0.028686
2023-12-17 07:28:20,262 - Epoch 5 average train loss: 0.015094
2023-12-17 07:32:00,177 - Epoch 6 average train loss: 0.007256
2023-12-17 07:35:40,139 - Epoch 7 average train loss: 0.003885
2023-12-17 07:39:20,420 - Epoch 8 average train loss: 0.008071
2023-12-17 07:39:45,008 - 2023-12-17_07-10-01 with 8 epochs: Evaluation Results:
2023-12-17 07:39:45,008 - Training time: 1758.5298743247986 seconds
2023-12-17 07:39:45,008 - Inference time: 24.579998254776 seconds
2023-12-17 07:39:45,008 - Precision: 0.9628316127832985
2023-12-17 07:39:45,008 - Recall: 0.9620910211312255
2023-12-17 07:39:45,009 - F-score: 0.9623622794351283
2023-12-17 07:39:45,009 - Accuracy: 0.9623529411764706
2023-12-17 07:39:45,009 - G-mean: 0.9622219722419089
2023-12-17 07:40:12,889 - distilbert-base-uncased 2023-12-17_07-10-01 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 07:40:12,889 - Training time: 1758.5298743247986 seconds
2023-12-17 07:40:12,889 - Inference time: 24.514000415802002 seconds
2023-12-17 07:40:12,889 - Precision: 0.9731866826986184
2023-12-17 07:40:12,889 - Recall: 0.9734812085536939
2023-12-17 07:40:12,889 - F-score: 0.9732503471692491
2023-12-17 07:40:12,889 - Accuracy: 0.9729411764705882
2023-12-17 07:40:12,889 - G-mean: 0.9732111550543597
2023-12-17 07:40:14,737 - ====================================================
2023-12-17 07:40:14,997 - Model: distilbert-base-uncased 2023-12-17_07-40-14 with 8 epochs
2023-12-17 07:43:54,436 - Epoch 1 average train loss: 0.605221
2023-12-17 07:47:34,271 - Epoch 2 average train loss: 0.116365
2023-12-17 07:51:14,635 - Epoch 3 average train loss: 0.056166
2023-12-17 07:54:54,742 - Epoch 4 average train loss: 0.029843
2023-12-17 07:58:34,879 - Epoch 5 average train loss: 0.023989
2023-12-17 08:02:15,027 - Epoch 6 average train loss: 0.016413
2023-12-17 08:05:55,153 - Epoch 7 average train loss: 0.012998
2023-12-17 08:09:34,987 - Epoch 8 average train loss: 0.008486
2023-12-17 08:09:59,428 - 2023-12-17_07-40-14 with 8 epochs: Evaluation Results:
2023-12-17 08:09:59,428 - Training time: 1759.9344420433044 seconds
2023-12-17 08:09:59,428 - Inference time: 24.43400001525879 seconds
2023-12-17 08:09:59,428 - Precision: 0.9666361908614494
2023-12-17 08:09:59,428 - Recall: 0.965914286304262
2023-12-17 08:09:59,428 - F-score: 0.9661468158571067
2023-12-17 08:09:59,428 - Accuracy: 0.9658823529411765
2023-12-17 08:09:59,428 - G-mean: 0.9658983194907514
2023-12-17 08:10:27,043 - distilbert-base-uncased 2023-12-17_07-40-14 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 08:10:27,043 - Training time: 1759.9344420433044 seconds
2023-12-17 08:10:27,043 - Inference time: 24.311969995498657 seconds
2023-12-17 08:10:27,043 - Precision: 0.9670850840376806
2023-12-17 08:10:27,043 - Recall: 0.9676314649180687
2023-12-17 08:10:27,043 - F-score: 0.9670096063667227
2023-12-17 08:10:27,043 - Accuracy: 0.9670588235294117
2023-12-17 08:10:27,043 - G-mean: 0.9673451018502698
2023-12-17 08:10:27,077 - Model distilbert-base-uncased 2023-12-17_07-40-14 not saved
2023-12-17 08:10:27,078 - ====================================================
2023-12-17 08:10:27,340 - Model: distilbert-base-uncased 2023-12-17_08-10-27 with 8 epochs
2023-12-17 08:14:06,583 - Epoch 1 average train loss: 0.588944
2023-12-17 08:17:46,351 - Epoch 2 average train loss: 0.107986
2023-12-17 08:21:26,019 - Epoch 3 average train loss: 0.057148
2023-12-17 08:25:05,713 - Epoch 4 average train loss: 0.036215
2023-12-17 08:28:45,282 - Epoch 5 average train loss: 0.016958
2023-12-17 08:32:24,975 - Epoch 6 average train loss: 0.007274
2023-12-17 08:36:04,397 - Epoch 7 average train loss: 0.010663
2023-12-17 08:39:43,948 - Epoch 8 average train loss: 0.003553
2023-12-17 08:40:08,371 - 2023-12-17_08-10-27 with 8 epochs: Evaluation Results:
2023-12-17 08:40:08,371 - Training time: 1756.5519626140594 seconds
2023-12-17 08:40:08,371 - Inference time: 24.416000366210938 seconds
2023-12-17 08:40:08,371 - Precision: 0.9695069973966672
2023-12-17 08:40:08,371 - Recall: 0.9695353813683892
2023-12-17 08:40:08,371 - F-score: 0.9693039094767565
2023-12-17 08:40:08,371 - Accuracy: 0.9694117647058823
2023-12-17 08:40:08,372 - G-mean: 0.9694735710668553
2023-12-17 08:40:36,052 - distilbert-base-uncased 2023-12-17_08-10-27 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 08:40:36,052 - Training time: 1756.5519626140594 seconds
2023-12-17 08:40:36,052 - Inference time: 24.34696650505066 seconds
2023-12-17 08:40:36,052 - Precision: 0.965156642526296
2023-12-17 08:40:36,052 - Recall: 0.966404471053038
2023-12-17 08:40:36,052 - F-score: 0.9656176217164134
2023-12-17 08:40:36,052 - Accuracy: 0.9658823529411765
2023-12-17 08:40:36,052 - G-mean: 0.9661433767270681
2023-12-17 08:40:36,090 - Model distilbert-base-uncased 2023-12-17_08-10-27 not saved
2023-12-17 08:40:36,090 - ====================================================
2023-12-17 08:40:36,345 - Model: distilbert-base-uncased 2023-12-17_08-40-36 with 8 epochs
2023-12-17 08:44:15,497 - Epoch 1 average train loss: 0.619606
2023-12-17 08:47:54,839 - Epoch 2 average train loss: 0.123307
2023-12-17 08:51:34,508 - Epoch 3 average train loss: 0.060029
2023-12-17 08:55:14,431 - Epoch 4 average train loss: 0.033675
2023-12-17 08:58:54,586 - Epoch 5 average train loss: 0.026717
2023-12-17 09:02:34,116 - Epoch 6 average train loss: 0.014339
2023-12-17 09:06:13,463 - Epoch 7 average train loss: 0.010215
2023-12-17 09:09:53,333 - Epoch 8 average train loss: 0.003927
2023-12-17 09:10:17,790 - 2023-12-17_08-40-36 with 8 epochs: Evaluation Results:
2023-12-17 09:10:17,790 - Training time: 1756.9323213100433 seconds
2023-12-17 09:10:17,790 - Inference time: 24.449002027511597 seconds
2023-12-17 09:10:17,790 - Precision: 0.9632464890444095
2023-12-17 09:10:17,790 - Recall: 0.9624592996613709
2023-12-17 09:10:17,790 - F-score: 0.9624568781332385
2023-12-17 09:10:17,790 - Accuracy: 0.9623529411764706
2023-12-17 09:10:17,790 - G-mean: 0.96240611894967
2023-12-17 09:10:45,429 - distilbert-base-uncased 2023-12-17_08-40-36 with 8 epochs: Evaluation Results (completely new data):
2023-12-17 09:10:45,429 - Training time: 1756.9323213100433 seconds
2023-12-17 09:10:45,429 - Inference time: 24.3190016746521 seconds
2023-12-17 09:10:45,430 - Precision: 0.9667026509161609
2023-12-17 09:10:45,430 - Recall: 0.9677932567029133
2023-12-17 09:10:45,430 - F-score: 0.9668516209067757
2023-12-17 09:10:45,430 - Accuracy: 0.9670588235294117
2023-12-17 09:10:45,430 - G-mean: 0.9674259704219322
2023-12-17 09:10:45,464 - Model distilbert-base-uncased 2023-12-17_08-40-36 not saved
2023-12-17 09:10:45,465 - ====================================================
2023-12-17 09:10:45,754 - Model: distilbert-base-uncased 2023-12-17_09-10-45 with 9 epochs
2023-12-17 09:14:25,041 - Epoch 1 average train loss: 0.603949
2023-12-17 09:18:05,094 - Epoch 2 average train loss: 0.116659
2023-12-17 09:21:45,134 - Epoch 3 average train loss: 0.051911
2023-12-17 09:25:24,884 - Epoch 4 average train loss: 0.029889
2023-12-17 09:29:04,848 - Epoch 5 average train loss: 0.022914
2023-12-17 09:32:44,600 - Epoch 6 average train loss: 0.013373
2023-12-17 09:36:24,583 - Epoch 7 average train loss: 0.008487
2023-12-17 09:40:04,235 - Epoch 8 average train loss: 0.009877
2023-12-17 09:43:47,836 - Epoch 9 average train loss: 0.003120
2023-12-17 09:44:12,343 - 2023-12-17_09-10-45 with 9 epochs: Evaluation Results:
2023-12-17 09:44:12,343 - Training time: 1982.0296757221222 seconds
2023-12-17 09:44:12,343 - Inference time: 24.499999523162842 seconds
2023-12-17 09:44:12,344 - Precision: 0.9633893079527696
2023-12-17 09:44:12,344 - Recall: 0.9622341791609117
2023-12-17 09:44:12,344 - F-score: 0.9626996670186395
2023-12-17 09:44:12,344 - Accuracy: 0.9623529411764706
2023-12-17 09:44:12,344 - G-mean: 0.9622935583365558
2023-12-17 09:44:40,522 - distilbert-base-uncased 2023-12-17_09-10-45 with 9 epochs: Evaluation Results (completely new data):
2023-12-17 09:44:40,522 - Training time: 1982.0296757221222 seconds
2023-12-17 09:44:40,522 - Inference time: 24.818315744400024 seconds
2023-12-17 09:44:40,522 - Precision: 0.9666431536720907
2023-12-17 09:44:40,522 - Recall: 0.9677581223492977
2023-12-17 09:44:40,523 - F-score: 0.9669962148310776
2023-12-17 09:44:40,523 - Accuracy: 0.9670588235294117
2023-12-17 09:44:40,523 - G-mean: 0.9674084097526465
2023-12-17 09:44:42,406 - ====================================================
2023-12-17 09:44:42,690 - Model: distilbert-base-uncased 2023-12-17_09-44-42 with 9 epochs
2023-12-17 09:48:22,255 - Epoch 1 average train loss: 0.585544
2023-12-17 09:52:02,184 - Epoch 2 average train loss: 0.117424
2023-12-17 09:55:42,059 - Epoch 3 average train loss: 0.054810
2023-12-17 09:59:22,255 - Epoch 4 average train loss: 0.026648
2023-12-17 10:03:01,859 - Epoch 5 average train loss: 0.018539
2023-12-17 10:06:41,684 - Epoch 6 average train loss: 0.010771
2023-12-17 10:10:21,380 - Epoch 7 average train loss: 0.013323
2023-12-17 10:14:01,483 - Epoch 8 average train loss: 0.007870
