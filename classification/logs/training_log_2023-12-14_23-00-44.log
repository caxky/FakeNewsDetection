2023-12-14 23:01:03,722 - ====================================================
2023-12-14 23:01:03,970 - Model: distilbert-base-uncased 2023-12-14_23-01-03 with 4 epochs
2023-12-14 23:04:50,912 - Epoch 1 average train loss: 0.6229832021979724
2023-12-14 23:08:37,627 - Epoch 2 average train loss: 0.11421070005306426
2023-12-14 23:13:42,164 - Epoch 3 average train loss: 0.05227411282960983
2023-12-14 23:17:43,357 - Epoch 4 average train loss: 0.03536553712671294
2023-12-14 23:18:09,428 - 2023-12-14_23-01-03 with 4 epochs: Evaluation Results:
2023-12-14 23:18:09,428 - Training time: 999.3122470378876 seconds
2023-12-14 23:18:09,429 - Inference time: 26.05999779701233 seconds
2023-12-14 23:18:09,433 - Precision: 0.9610622121815382
2023-12-14 23:18:09,433 - Recall: 0.960164317325142
2023-12-14 23:18:09,433 - F-score: 0.9605304116987916
2023-12-14 23:18:09,433 - Accuracy: 0.96
2023-12-14 23:18:09,433 - G-mean: 0.960082155147223
2023-12-14 23:18:39,744 - distilbert-base-uncased 2023-12-14_23-01-03 with 4 epochs: Evaluation Results (completely new data):
2023-12-14 23:18:39,744 - Training time: 999.3122470378876 seconds
2023-12-14 23:18:39,744 - Inference time: 25.70400094985962 seconds
2023-12-14 23:18:39,744 - Precision: 0.9654741270450801
2023-12-14 23:18:39,744 - Recall: 0.9663094501684576
2023-12-14 23:18:39,745 - F-score: 0.9658018868192718
2023-12-14 23:18:39,745 - Accuracy: 0.9658823529411765
2023-12-14 23:18:39,745 - G-mean: 0.9660958779531172
2023-12-14 23:18:44,043 - ====================================================
2023-12-14 23:18:46,011 - Model: distilbert-base-uncased 2023-12-14_23-18-46 with 4 epochs
2023-12-14 23:22:37,555 - Epoch 1 average train loss: 0.5793647526555201
2023-12-14 23:26:22,609 - Epoch 2 average train loss: 0.11412943373269895
2023-12-14 23:30:07,089 - Epoch 3 average train loss: 0.06282715314560954
2023-12-14 23:33:50,844 - Epoch 4 average train loss: 0.02925693491802496
2023-12-14 23:34:15,816 - 2023-12-14_23-18-46 with 4 epochs: Evaluation Results:
2023-12-14 23:34:15,816 - Training time: 904.5500931739807 seconds
2023-12-14 23:34:15,816 - Inference time: 24.965001106262207 seconds
2023-12-14 23:34:15,816 - Precision: 0.9688198650003818
2023-12-14 23:34:15,816 - Recall: 0.9685882034466491
2023-12-14 23:34:15,816 - F-score: 0.968634814376891
2023-12-14 23:34:15,816 - Accuracy: 0.9682352941176471
2023-12-14 23:34:15,816 - G-mean: 0.968411732706213
2023-12-14 23:34:43,953 - distilbert-base-uncased 2023-12-14_23-18-46 with 4 epochs: Evaluation Results (completely new data):
2023-12-14 23:34:43,953 - Training time: 904.5500931739807 seconds
2023-12-14 23:34:43,953 - Inference time: 24.736965894699097 seconds
2023-12-14 23:34:43,953 - Precision: 0.9675586908093905
2023-12-14 23:34:43,953 - Recall: 0.9689851162143283
2023-12-14 23:34:43,953 - F-score: 0.9679380117836885
2023-12-14 23:34:43,953 - Accuracy: 0.9682352941176471
2023-12-14 23:34:43,953 - G-mean: 0.9686101326092984
2023-12-14 23:34:45,823 - ====================================================
2023-12-14 23:34:46,069 - Model: distilbert-base-uncased 2023-12-14_23-34-46 with 4 epochs
2023-12-14 23:38:28,354 - Epoch 1 average train loss: 0.6075332990376389
2023-12-14 23:42:13,683 - Epoch 2 average train loss: 0.11071742120911093
2023-12-14 23:45:55,399 - Epoch 3 average train loss: 0.05264965614632649
2023-12-14 23:49:36,693 - Epoch 4 average train loss: 0.026846742349512436
2023-12-14 23:50:01,384 - 2023-12-14_23-34-46 with 4 epochs: Evaluation Results:
2023-12-14 23:50:01,384 - Training time: 890.5692205429077 seconds
2023-12-14 23:50:01,384 - Inference time: 24.68203091621399 seconds
2023-12-14 23:50:01,384 - Precision: 0.9673826436721186
2023-12-14 23:50:01,384 - Recall: 0.9656616272354416
2023-12-14 23:50:01,384 - F-score: 0.9663096849737848
2023-12-14 23:50:01,384 - Accuracy: 0.9658823529411765
2023-12-14 23:50:01,384 - G-mean: 0.9657719837824938
2023-12-14 23:50:29,253 - distilbert-base-uncased 2023-12-14_23-34-46 with 4 epochs: Evaluation Results (completely new data):
2023-12-14 23:50:29,253 - Training time: 890.5692205429077 seconds
2023-12-14 23:50:29,254 - Inference time: 24.497966051101685 seconds
2023-12-14 23:50:29,254 - Precision: 0.9708032995325814
2023-12-14 23:50:29,254 - Recall: 0.9691575951137192
2023-12-14 23:50:29,254 - F-score: 0.9695645183164997
2023-12-14 23:50:29,254 - Accuracy: 0.9694117647058823
2023-12-14 23:50:29,254 - G-mean: 0.9692846715786335
2023-12-14 23:50:31,270 - ====================================================
2023-12-14 23:50:31,568 - Model: distilbert-base-uncased 2023-12-14_23-50-31 with 4 epochs
2023-12-14 23:54:12,544 - Epoch 1 average train loss: 0.5833534978680751
2023-12-14 23:57:53,911 - Epoch 2 average train loss: 0.11039114602129249
2023-12-15 00:01:36,084 - Epoch 3 average train loss: 0.05415093179025194
2023-12-15 00:05:20,070 - Epoch 4 average train loss: 0.03314999668804162
2023-12-15 00:05:45,042 - 2023-12-14_23-50-31 with 4 epochs: Evaluation Results:
2023-12-15 00:05:45,042 - Training time: 888.4466490745544 seconds
2023-12-15 00:05:45,042 - Inference time: 24.96399974822998 seconds
2023-12-15 00:05:45,042 - Precision: 0.9687081282134731
2023-12-15 00:05:45,042 - Recall: 0.9683031964152253
2023-12-15 00:05:45,042 - F-score: 0.9684483834527867
2023-12-15 00:05:45,042 - Accuracy: 0.9682352941176471
2023-12-15 00:05:45,042 - G-mean: 0.9682692446712089
2023-12-15 00:06:13,402 - distilbert-base-uncased 2023-12-14_23-50-31 with 4 epochs: Evaluation Results (completely new data):
2023-12-15 00:06:13,403 - Training time: 888.4466490745544 seconds
2023-12-15 00:06:13,403 - Inference time: 24.733999252319336 seconds
2023-12-15 00:06:13,403 - Precision: 0.9672425095379793
2023-12-15 00:06:13,403 - Recall: 0.9672462459139313
2023-12-15 00:06:13,403 - F-score: 0.9670909414474895
2023-12-15 00:06:13,403 - Accuracy: 0.9670588235294117
2023-12-15 00:06:13,403 - G-mean: 0.9671525301816495
2023-12-15 00:06:13,444 - Model distilbert-base-uncased 2023-12-14_23-50-31 not saved
2023-12-15 00:06:13,445 - ====================================================
2023-12-15 00:06:13,737 - Model: distilbert-base-uncased 2023-12-15_00-06-13 with 4 epochs
2023-12-15 00:09:57,897 - Epoch 1 average train loss: 0.5897426249086857
2023-12-15 00:13:42,091 - Epoch 2 average train loss: 0.1131471968595596
2023-12-15 00:17:26,166 - Epoch 3 average train loss: 0.05452767299378619
2023-12-15 00:21:09,582 - Epoch 4 average train loss: 0.027221030251616065
2023-12-15 00:21:34,356 - 2023-12-15_00-06-13 with 4 epochs: Evaluation Results:
2023-12-15 00:21:34,357 - Training time: 895.787750005722 seconds
2023-12-15 00:21:34,357 - Inference time: 24.76600432395935 seconds
2023-12-15 00:21:34,357 - Precision: 0.967590777558741
2023-12-15 00:21:34,357 - Recall: 0.9671703489830332
2023-12-15 00:21:34,357 - F-score: 0.9673574907852789
2023-12-15 00:21:34,357 - Accuracy: 0.9670588235294117
2023-12-15 00:21:34,357 - G-mean: 0.9671145846486148
2023-12-15 00:22:02,594 - distilbert-base-uncased 2023-12-15_00-06-13 with 4 epochs: Evaluation Results (completely new data):
2023-12-15 00:22:02,594 - Training time: 895.787750005722 seconds
2023-12-15 00:22:02,594 - Inference time: 24.592000484466553 seconds
2023-12-15 00:22:02,594 - Precision: 0.9673283474614373
2023-12-15 00:22:02,595 - Recall: 0.9672462459139313
2023-12-15 00:22:02,595 - F-score: 0.9671837129338812
2023-12-15 00:22:02,595 - Accuracy: 0.9670588235294117
2023-12-15 00:22:02,595 - G-mean: 0.9671525301816495
2023-12-15 00:22:02,639 - Model distilbert-base-uncased 2023-12-15_00-06-13 not saved
2023-12-15 00:22:02,640 - ====================================================
2023-12-15 00:22:02,923 - Model: distilbert-base-uncased 2023-12-15_00-22-02 with 5 epochs
2023-12-15 00:25:46,103 - Epoch 1 average train loss: 0.6065031982607701
2023-12-15 00:29:29,764 - Epoch 2 average train loss: 0.11215747527120744
2023-12-15 00:33:12,898 - Epoch 3 average train loss: 0.0555233090140802
2023-12-15 00:36:54,933 - Epoch 4 average train loss: 0.02715076493236291
2023-12-15 00:40:36,954 - Epoch 5 average train loss: 0.014592023256152649
2023-12-15 00:41:01,688 - 2023-12-15_00-22-02 with 5 epochs: Evaluation Results:
2023-12-15 00:41:01,689 - Training time: 1113.9758567810059 seconds
2023-12-15 00:41:01,689 - Inference time: 24.72699809074402 seconds
2023-12-15 00:41:01,689 - Precision: 0.9652360303113066
2023-12-15 00:41:01,689 - Recall: 0.9647172357047136
2023-12-15 00:41:01,689 - F-score: 0.9649225533675889
2023-12-15 00:41:01,689 - Accuracy: 0.9647058823529412
2023-12-15 00:41:01,689 - G-mean: 0.9647115590121257
2023-12-15 00:41:29,733 - distilbert-base-uncased 2023-12-15_00-22-02 with 5 epochs: Evaluation Results (completely new data):
2023-12-15 00:41:29,733 - Training time: 1113.9758567810059 seconds
2023-12-15 00:41:29,733 - Inference time: 24.63899278640747 seconds
2023-12-15 00:41:29,733 - Precision: 0.9690960873837279
2023-12-15 00:41:29,733 - Recall: 0.969828639978705
2023-12-15 00:41:29,733 - F-score: 0.969412652442711
2023-12-15 00:41:29,734 - Accuracy: 0.9694117647058823
2023-12-15 00:41:29,734 - G-mean: 0.969620179938548
2023-12-15 00:41:31,586 - ====================================================
2023-12-15 00:41:31,909 - Model: distilbert-base-uncased 2023-12-15_00-41-31 with 5 epochs
2023-12-15 00:45:12,767 - Epoch 1 average train loss: 0.5789151503671618
2023-12-15 00:48:55,066 - Epoch 2 average train loss: 0.1121045224127524
2023-12-15 00:52:38,184 - Epoch 3 average train loss: 0.05132026960306308
2023-12-15 00:56:20,244 - Epoch 4 average train loss: 0.028102535688153962
2023-12-15 01:00:01,939 - Epoch 5 average train loss: 0.017111770311260925
2023-12-15 01:00:26,545 - 2023-12-15_00-41-31 with 5 epochs: Evaluation Results:
2023-12-15 01:00:26,545 - Training time: 1109.9765994548798 seconds
2023-12-15 01:00:26,546 - Inference time: 24.59900665283203 seconds
2023-12-15 01:00:26,546 - Precision: 0.9685812144206827
2023-12-15 01:00:26,546 - Recall: 0.968053323737417
2023-12-15 01:00:26,546 - F-score: 0.9682069063572403
2023-12-15 01:00:26,546 - Accuracy: 0.9682352941176471
2023-12-15 01:00:26,546 - G-mean: 0.9681443046521856
2023-12-15 01:00:54,245 - distilbert-base-uncased 2023-12-15_00-41-31 with 5 epochs: Evaluation Results (completely new data):
2023-12-15 01:00:54,245 - Training time: 1109.9765994548798 seconds
2023-12-15 01:00:54,245 - Inference time: 24.370002031326294 seconds
2023-12-15 01:00:54,245 - Precision: 0.9703120945222246
2023-12-15 01:00:54,245 - Recall: 0.9709623618626388
2023-12-15 01:00:54,245 - F-score: 0.9705945039984536
2023-12-15 01:00:54,245 - Accuracy: 0.9705882352941176
2023-12-15 01:00:54,245 - G-mean: 0.9707752805553235
2023-12-15 01:00:56,090 - ====================================================
2023-12-15 01:00:56,319 - Model: distilbert-base-uncased 2023-12-15_01-00-56 with 5 epochs
2023-12-15 01:04:37,192 - Epoch 1 average train loss: 0.5756345384173533
2023-12-15 01:08:18,364 - Epoch 2 average train loss: 0.11359481911668007
2023-12-15 01:11:59,792 - Epoch 3 average train loss: 0.05883869619053953
2023-12-15 01:15:41,338 - Epoch 4 average train loss: 0.03288914205725579
2023-12-15 01:19:23,405 - Epoch 5 average train loss: 0.0185147697340204
2023-12-15 01:19:48,168 - 2023-12-15_01-00-56 with 5 epochs: Evaluation Results:
2023-12-15 01:19:48,168 - Training time: 1107.0314242839813 seconds
2023-12-15 01:19:48,168 - Inference time: 24.75499939918518 seconds
2023-12-15 01:19:48,168 - Precision: 0.9676281581486152
2023-12-15 01:19:48,168 - Recall: 0.9667287412548979
2023-12-15 01:19:48,168 - F-score: 0.96712264510087
2023-12-15 01:19:48,168 - Accuracy: 0.9670588235294117
2023-12-15 01:19:48,168 - G-mean: 0.966893768306545
2023-12-15 01:20:16,090 - distilbert-base-uncased 2023-12-15_01-00-56 with 5 epochs: Evaluation Results (completely new data):
2023-12-15 01:20:16,090 - Training time: 1107.0314242839813 seconds
2023-12-15 01:20:16,090 - Inference time: 24.574967622756958 seconds
2023-12-15 01:20:16,090 - Precision: 0.9674332706952308
2023-12-15 01:20:16,091 - Recall: 0.9675165012503812
2023-12-15 01:20:16,091 - F-score: 0.9674165693318475
2023-12-15 01:20:16,091 - Accuracy: 0.9670588235294117
2023-12-15 01:20:16,091 - G-mean: 0.9672876353207903
2023-12-15 01:20:16,126 - Model distilbert-base-uncased 2023-12-15_01-00-56 not saved
2023-12-15 01:20:16,126 - ====================================================
2023-12-15 01:20:16,725 - Model: distilbert-base-uncased 2023-12-15_01-20-16 with 5 epochs
2023-12-15 01:23:57,508 - Epoch 1 average train loss: 0.5859713019605945
2023-12-15 01:27:39,294 - Epoch 2 average train loss: 0.10694753024288836
2023-12-15 01:31:20,620 - Epoch 3 average train loss: 0.04638938524486388
2023-12-15 01:35:01,291 - Epoch 4 average train loss: 0.0313603604267187
2023-12-15 01:38:41,872 - Epoch 5 average train loss: 0.019177188983132296
2023-12-15 01:39:06,394 - 2023-12-15_01-20-16 with 5 epochs: Evaluation Results:
2023-12-15 01:39:06,394 - Training time: 1105.089736700058 seconds
2023-12-15 01:39:06,394 - Inference time: 24.514000415802002 seconds
2023-12-15 01:39:06,394 - Precision: 0.9486295518929779
2023-12-15 01:39:06,394 - Recall: 0.9483353862316501
2023-12-15 01:39:06,394 - F-score: 0.9482327641732295
2023-12-15 01:39:06,394 - Accuracy: 0.9482352941176471
2023-12-15 01:39:06,394 - G-mean: 0.9482853388540504
2023-12-15 01:39:34,118 - distilbert-base-uncased 2023-12-15_01-20-16 with 5 epochs: Evaluation Results (completely new data):
2023-12-15 01:39:34,118 - Training time: 1105.089736700058 seconds
2023-12-15 01:39:34,118 - Inference time: 24.37199831008911 seconds
2023-12-15 01:39:34,118 - Precision: 0.9613563755089427
2023-12-15 01:39:34,119 - Recall: 0.9619175047443062
2023-12-15 01:39:34,119 - F-score: 0.9613424572760401
2023-12-15 01:39:34,119 - Accuracy: 0.9611764705882353
2023-12-15 01:39:34,119 - G-mean: 0.9615469162797904
2023-12-15 01:39:34,155 - Model distilbert-base-uncased 2023-12-15_01-20-16 not saved
2023-12-15 01:39:34,155 - ====================================================
2023-12-15 01:39:34,423 - Model: distilbert-base-uncased 2023-12-15_01-39-34 with 5 epochs
2023-12-15 01:43:14,478 - Epoch 1 average train loss: 0.592734387192656
2023-12-15 01:46:54,648 - Epoch 2 average train loss: 0.11563486697919229
2023-12-15 01:50:34,989 - Epoch 3 average train loss: 0.05135021274471108
2023-12-15 01:54:15,404 - Epoch 4 average train loss: 0.030680970182532772
2023-12-15 01:57:56,149 - Epoch 5 average train loss: 0.015274074258940186
2023-12-15 01:58:20,863 - 2023-12-15_01-39-34 with 5 epochs: Evaluation Results:
2023-12-15 01:58:20,864 - Training time: 1101.6674795150757 seconds
2023-12-15 01:58:20,864 - Inference time: 24.707035541534424 seconds
2023-12-15 01:58:20,864 - Precision: 0.9662968801423168
2023-12-15 01:58:20,864 - Recall: 0.9661895140707412
2023-12-15 01:58:20,864 - F-score: 0.9661651824089874
2023-12-15 01:58:20,864 - Accuracy: 0.9658823529411765
2023-12-15 01:58:20,864 - G-mean: 0.966035921297826
2023-12-15 01:58:48,740 - distilbert-base-uncased 2023-12-15_01-39-34 with 5 epochs: Evaluation Results (completely new data):
2023-12-15 01:58:48,740 - Training time: 1101.6674795150757 seconds
2023-12-15 01:58:48,741 - Inference time: 24.59601140022278 seconds
2023-12-15 01:58:48,741 - Precision: 0.9666518974918608
2023-12-15 01:58:48,741 - Recall: 0.9674807111031279
2023-12-15 01:58:48,741 - F-score: 0.9669336619796528
2023-12-15 01:58:48,741 - Accuracy: 0.9670588235294117
2023-12-15 01:58:48,741 - G-mean: 0.9672697443147851
2023-12-15 01:58:48,778 - Model distilbert-base-uncased 2023-12-15_01-39-34 not saved
2023-12-15 01:58:48,779 - ====================================================
2023-12-15 01:58:49,111 - Model: distilbert-base-uncased 2023-12-15_01-58-49 with 6 epochs
2023-12-15 02:02:29,320 - Epoch 1 average train loss: 0.5749545202974011
2023-12-15 02:06:09,746 - Epoch 2 average train loss: 0.10417088404297829
2023-12-15 02:09:50,310 - Epoch 3 average train loss: 0.05114238587591578
2023-12-15 02:13:31,031 - Epoch 4 average train loss: 0.030157756701330928
2023-12-15 02:17:11,826 - Epoch 5 average train loss: 0.0218142124125734
2023-12-15 02:20:52,416 - Epoch 6 average train loss: 0.01030461839715238
2023-12-15 02:21:17,090 - 2023-12-15_01-58-49 with 6 epochs: Evaluation Results:
2023-12-15 02:21:17,090 - Training time: 1323.249912261963 seconds
2023-12-15 02:21:17,090 - Inference time: 24.667031049728394 seconds
2023-12-15 02:21:17,090 - Precision: 0.9709370638439585
2023-12-15 02:21:17,090 - Recall: 0.9703789051327659
2023-12-15 02:21:17,090 - F-score: 0.9706118165412756
2023-12-15 02:21:17,090 - Accuracy: 0.9705882352941176
2023-12-15 02:21:17,091 - G-mean: 0.970483564569462
2023-12-15 02:21:44,964 - distilbert-base-uncased 2023-12-15_01-58-49 with 6 epochs: Evaluation Results (completely new data):
2023-12-15 02:21:44,964 - Training time: 1323.249912261963 seconds
2023-12-15 02:21:44,964 - Inference time: 24.570000410079956 seconds
2023-12-15 02:21:44,964 - Precision: 0.9702875885547637
2023-12-15 02:21:44,964 - Recall: 0.9711840401784482
2023-12-15 02:21:44,964 - F-score: 0.9705610601209281
2023-12-15 02:21:44,964 - Accuracy: 0.9705882352941176
2023-12-15 02:21:44,964 - G-mean: 0.9708860920327428
2023-12-15 02:21:46,812 - ====================================================
2023-12-15 02:21:47,106 - Model: distilbert-base-uncased 2023-12-15_02-21-47 with 6 epochs
2023-12-15 02:25:26,980 - Epoch 1 average train loss: 0.5965522047176081
2023-12-15 02:29:07,819 - Epoch 2 average train loss: 0.11026593698517365
2023-12-15 02:32:48,261 - Epoch 3 average train loss: 0.05010622736723984
2023-12-15 02:36:28,886 - Epoch 4 average train loss: 0.03329564775723745
2023-12-15 02:40:09,527 - Epoch 5 average train loss: 0.019691687948618305
2023-12-15 02:43:49,900 - Epoch 6 average train loss: 0.011564976004061893
2023-12-15 02:44:14,441 - 2023-12-15_02-21-47 with 6 epochs: Evaluation Results:
2023-12-15 02:44:14,441 - Training time: 1322.7379114627838 seconds
2023-12-15 02:44:14,441 - Inference time: 24.533031225204468 seconds
2023-12-15 02:44:14,441 - Precision: 0.968715784245733
2023-12-15 02:44:14,441 - Recall: 0.9680823925511577
2023-12-15 02:44:14,441 - F-score: 0.9683462976277214
2023-12-15 02:44:14,441 - Accuracy: 0.9682352941176471
2023-12-15 02:44:14,441 - G-mean: 0.9681588403159296
2023-12-15 02:44:42,090 - distilbert-base-uncased 2023-12-15_02-21-47 with 6 epochs: Evaluation Results (completely new data):
2023-12-15 02:44:42,090 - Training time: 1322.7379114627838 seconds
2023-12-15 02:44:42,090 - Inference time: 24.341028928756714 seconds
2023-12-15 02:44:42,090 - Precision: 0.9737794358836716
2023-12-15 02:44:42,090 - Recall: 0.9747990697027005
2023-12-15 02:44:42,091 - F-score: 0.9741891805650809
2023-12-15 02:44:42,091 - Accuracy: 0.9741176470588235
2023-12-15 02:44:42,091 - G-mean: 0.9744582988173094
2023-12-15 02:44:43,934 - ====================================================
2023-12-15 02:44:44,208 - Model: distilbert-base-uncased 2023-12-15_02-44-44 with 6 epochs
2023-12-15 02:48:24,144 - Epoch 1 average train loss: 0.5959681622158078
2023-12-15 02:52:04,519 - Epoch 2 average train loss: 0.11545977268148871
2023-12-15 02:55:44,997 - Epoch 3 average train loss: 0.055415567540947126
2023-12-15 02:59:25,507 - Epoch 4 average train loss: 0.027303856309393748
2023-12-15 03:03:05,629 - Epoch 5 average train loss: 0.014737271912916398
2023-12-15 03:06:45,414 - Epoch 6 average train loss: 0.013980667165220331
2023-12-15 03:07:09,984 - 2023-12-15_02-44-44 with 6 epochs: Evaluation Results:
2023-12-15 03:07:09,984 - Training time: 1321.1482090950012 seconds
2023-12-15 03:07:09,984 - Inference time: 24.56299877166748 seconds
2023-12-15 03:07:09,984 - Precision: 0.9687360940366005
2023-12-15 03:07:09,984 - Recall: 0.9683083875033585
2023-12-15 03:07:09,984 - F-score: 0.9682849638274833
2023-12-15 03:07:09,984 - Accuracy: 0.9682352941176471
2023-12-15 03:07:09,984 - G-mean: 0.968271840120789
2023-12-15 03:07:37,841 - distilbert-base-uncased 2023-12-15_02-44-44 with 6 epochs: Evaluation Results (completely new data):
2023-12-15 03:07:37,841 - Training time: 1321.1482090950012 seconds
2023-12-15 03:07:37,841 - Inference time: 24.480000734329224 seconds
2023-12-15 03:07:37,841 - Precision: 0.9643266637086493
2023-12-15 03:07:37,841 - Recall: 0.9651260608940386
2023-12-15 03:07:37,841 - F-score: 0.9644030916533166
2023-12-15 03:07:37,842 - Accuracy: 0.9647058823529412
2023-12-15 03:07:37,842 - G-mean: 0.9649159487523263
2023-12-15 03:07:37,877 - Model distilbert-base-uncased 2023-12-15_02-44-44 not saved
2023-12-15 03:07:37,877 - ====================================================
2023-12-15 03:07:38,160 - Model: distilbert-base-uncased 2023-12-15_03-07-38 with 6 epochs
2023-12-15 03:11:17,353 - Epoch 1 average train loss: 0.6276652409311603
2023-12-15 03:14:57,080 - Epoch 2 average train loss: 0.11100963446147302
2023-12-15 03:18:36,843 - Epoch 3 average train loss: 0.05925164185354815
2023-12-15 03:22:16,583 - Epoch 4 average train loss: 0.028725709756836294
2023-12-15 03:25:56,622 - Epoch 5 average train loss: 0.02428366407423335
2023-12-15 03:29:36,957 - Epoch 6 average train loss: 0.01804551845060333
2023-12-15 03:30:01,445 - 2023-12-15_03-07-38 with 6 epochs: Evaluation Results:
2023-12-15 03:30:01,445 - Training time: 1318.7399151325226 seconds
2023-12-15 03:30:01,445 - Inference time: 24.479999542236328 seconds
2023-12-15 03:30:01,445 - Precision: 0.9689851031972457
2023-12-15 03:30:01,445 - Recall: 0.9683031964152253
2023-12-15 03:30:01,445 - F-score: 0.9685746574418257
2023-12-15 03:30:01,445 - Accuracy: 0.9682352941176471
2023-12-15 03:30:01,445 - G-mean: 0.9682692446712089
2023-12-15 03:30:29,175 - distilbert-base-uncased 2023-12-15_03-07-38 with 6 epochs: Evaluation Results (completely new data):
2023-12-15 03:30:29,175 - Training time: 1318.7399151325226 seconds
2023-12-15 03:30:29,175 - Inference time: 24.413032293319702 seconds
2023-12-15 03:30:29,175 - Precision: 0.9699492244025599
2023-12-15 03:30:29,175 - Recall: 0.9710265650299951
2023-12-15 03:30:29,175 - F-score: 0.9703551925628631
2023-12-15 03:30:29,175 - Accuracy: 0.9705882352941176
2023-12-15 03:30:29,175 - G-mean: 0.9708073754232462
2023-12-15 03:30:29,212 - Model distilbert-base-uncased 2023-12-15_03-07-38 not saved
2023-12-15 03:30:29,212 - ====================================================
2023-12-15 03:30:29,558 - Model: distilbert-base-uncased 2023-12-15_03-30-29 with 6 epochs
2023-12-15 03:34:08,896 - Epoch 1 average train loss: 0.5960868502890363
2023-12-15 03:37:48,806 - Epoch 2 average train loss: 0.10733772601494018
2023-12-15 03:41:28,769 - Epoch 3 average train loss: 0.050092736872680045
2023-12-15 03:45:08,889 - Epoch 4 average train loss: 0.027090480760945116
2023-12-15 03:48:48,860 - Epoch 5 average train loss: 0.013545521031380356
2023-12-15 03:52:28,394 - Epoch 6 average train loss: 0.006511595038327334
2023-12-15 03:52:52,887 - 2023-12-15_03-30-29 with 6 epochs: Evaluation Results:
2023-12-15 03:52:52,887 - Training time: 1318.7809989452362 seconds
2023-12-15 03:52:52,887 - Inference time: 24.484999895095825 seconds
2023-12-15 03:52:52,887 - Precision: 0.9635560643577357
2023-12-15 03:52:52,887 - Recall: 0.9640303126370483
2023-12-15 03:52:52,887 - F-score: 0.9637405039888153
2023-12-15 03:52:52,887 - Accuracy: 0.9635294117647059
2023-12-15 03:52:52,887 - G-mean: 0.9637798296595134
2023-12-15 03:53:20,792 - distilbert-base-uncased 2023-12-15_03-30-29 with 6 epochs: Evaluation Results (completely new data):
2023-12-15 03:53:20,792 - Training time: 1318.7809989452362 seconds
2023-12-15 03:53:20,792 - Inference time: 24.419999837875366 seconds
2023-12-15 03:53:20,792 - Precision: 0.972733948623549
2023-12-15 03:53:20,792 - Recall: 0.9736362790050259
2023-12-15 03:53:20,792 - F-score: 0.9731118611963214
2023-12-15 03:53:20,792 - Accuracy: 0.9729411764705882
2023-12-15 03:53:20,792 - G-mean: 0.9732886656843363
2023-12-15 03:53:20,829 - Model distilbert-base-uncased 2023-12-15_03-30-29 not saved
2023-12-15 03:53:20,830 - ====================================================
2023-12-15 03:53:21,253 - Model: distilbert-base-uncased 2023-12-15_03-53-21 with 7 epochs
2023-12-15 03:57:01,191 - Epoch 1 average train loss: 0.6146279285234564
2023-12-15 04:00:41,864 - Epoch 2 average train loss: 0.10949537886635345
2023-12-15 04:04:21,777 - Epoch 3 average train loss: 0.05549537279807469
2023-12-15 04:08:01,555 - Epoch 4 average train loss: 0.03330197000656934
2023-12-15 04:11:41,170 - Epoch 5 average train loss: 0.020818307629841215
2023-12-15 04:15:20,871 - Epoch 6 average train loss: 0.011316905693315407
2023-12-15 04:19:00,610 - Epoch 7 average train loss: 0.01519459212640547
2023-12-15 04:19:25,154 - 2023-12-15_03-53-21 with 7 epochs: Evaluation Results:
2023-12-15 04:19:25,154 - Training time: 1539.3009278774261 seconds
2023-12-15 04:19:25,154 - Inference time: 24.536000967025757 seconds
2023-12-15 04:19:25,154 - Precision: 0.9674449727025513
2023-12-15 04:19:25,154 - Recall: 0.9671755400711666
2023-12-15 04:19:25,154 - F-score: 0.9670847838936062
2023-12-15 04:19:25,154 - Accuracy: 0.9670588235294117
2023-12-15 04:19:25,154 - G-mean: 0.967117180039547
2023-12-15 04:19:52,880 - distilbert-base-uncased 2023-12-15_03-53-21 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 04:19:52,881 - Training time: 1539.3009278774261 seconds
2023-12-15 04:19:52,881 - Inference time: 24.398970127105713 seconds
2023-12-15 04:19:52,881 - Precision: 0.9713803960319305
2023-12-15 04:19:52,881 - Recall: 0.9726018946420641
2023-12-15 04:19:52,881 - F-score: 0.9717555970687062
2023-12-15 04:19:52,881 - Accuracy: 0.971764705882353
2023-12-15 04:19:52,881 - G-mean: 0.9721832101448085
2023-12-15 04:19:54,694 - ====================================================
2023-12-15 04:19:55,042 - Model: distilbert-base-uncased 2023-12-15_04-19-55 with 7 epochs
2023-12-15 04:23:34,683 - Epoch 1 average train loss: 0.5894374214726336
2023-12-15 04:27:14,820 - Epoch 2 average train loss: 0.11557129578774467
2023-12-15 04:30:54,983 - Epoch 3 average train loss: 0.058003345461671844
2023-12-15 04:34:35,106 - Epoch 4 average train loss: 0.03412566716892316
2023-12-15 04:38:15,462 - Epoch 5 average train loss: 0.02017106998794
2023-12-15 04:41:55,996 - Epoch 6 average train loss: 0.00873690324356122
2023-12-15 04:45:36,458 - Epoch 7 average train loss: 0.008886002801317612
2023-12-15 04:46:01,175 - 2023-12-15_04-19-55 with 7 epochs: Evaluation Results:
2023-12-15 04:46:01,175 - Training time: 1541.3605420589447 seconds
2023-12-15 04:46:01,175 - Inference time: 24.709129333496094 seconds
2023-12-15 04:46:01,175 - Precision: 0.9608695163225729
2023-12-15 04:46:01,175 - Recall: 0.9594626734010362
2023-12-15 04:46:01,175 - F-score: 0.9600414147544072
2023-12-15 04:46:01,175 - Accuracy: 0.96
2023-12-15 04:46:01,176 - G-mean: 0.9597312990962599
2023-12-15 04:46:29,048 - distilbert-base-uncased 2023-12-15_04-19-55 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 04:46:29,048 - Training time: 1541.3605420589447 seconds
2023-12-15 04:46:29,049 - Inference time: 24.55897307395935 seconds
2023-12-15 04:46:29,049 - Precision: 0.9716554558757222
2023-12-15 04:46:29,049 - Recall: 0.9724401028572196
2023-12-15 04:46:29,049 - F-score: 0.971971917455891
2023-12-15 04:46:29,049 - Accuracy: 0.971764705882353
2023-12-15 04:46:29,049 - G-mean: 0.9721023457132747
2023-12-15 04:46:30,982 - ====================================================
2023-12-15 04:46:31,313 - Model: distilbert-base-uncased 2023-12-15_04-46-31 with 7 epochs
2023-12-15 04:50:11,017 - Epoch 1 average train loss: 0.5743257478756063
2023-12-15 04:53:51,139 - Epoch 2 average train loss: 0.11048662150388255
2023-12-15 04:57:31,168 - Epoch 3 average train loss: 0.05071571193306761
2023-12-15 05:01:10,942 - Epoch 4 average train loss: 0.025881121461005772
2023-12-15 05:04:50,508 - Epoch 5 average train loss: 0.018752682151628987
2023-12-15 05:08:30,250 - Epoch 6 average train loss: 0.011928245369961267
2023-12-15 05:12:10,066 - Epoch 7 average train loss: 0.01394260771209648
2023-12-15 05:12:34,792 - 2023-12-15_04-46-31 with 7 epochs: Evaluation Results:
2023-12-15 05:12:34,792 - Training time: 1538.6964230537415 seconds
2023-12-15 05:12:34,792 - Inference time: 24.71998357772827 seconds
2023-12-15 05:12:34,792 - Precision: 0.9671509850868002
2023-12-15 05:12:34,792 - Recall: 0.967204608884907
2023-12-15 05:12:34,792 - F-score: 0.9670747348932391
2023-12-15 05:12:34,792 - Accuracy: 0.9670588235294117
2023-12-15 05:12:34,793 - G-mean: 0.9671317134602003
2023-12-15 05:13:02,708 - distilbert-base-uncased 2023-12-15_04-46-31 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 05:13:02,708 - Training time: 1538.6964230537415 seconds
2023-12-15 05:13:02,709 - Inference time: 24.652968645095825 seconds
2023-12-15 05:13:02,709 - Precision: 0.9677528738286382
2023-12-15 05:13:02,709 - Recall: 0.9687009836346462
2023-12-15 05:13:02,709 - F-score: 0.9680205368190556
2023-12-15 05:13:02,709 - Accuracy: 0.9682352941176471
2023-12-15 05:13:02,709 - G-mean: 0.9684681108851988
2023-12-15 05:13:02,744 - Model distilbert-base-uncased 2023-12-15_04-46-31 not saved
2023-12-15 05:13:02,744 - ====================================================
2023-12-15 05:13:02,992 - Model: distilbert-base-uncased 2023-12-15_05-13-02 with 7 epochs
2023-12-15 05:16:42,115 - Epoch 1 average train loss: 0.5935879158798386
2023-12-15 05:20:21,620 - Epoch 2 average train loss: 0.11210685374982217
2023-12-15 05:24:00,935 - Epoch 3 average train loss: 0.056179844093892504
2023-12-15 05:27:40,321 - Epoch 4 average train loss: 0.03340646888732034
2023-12-15 05:31:19,520 - Epoch 5 average train loss: 0.020395648520412472
2023-12-15 05:34:58,606 - Epoch 6 average train loss: 0.010509652784573572
2023-12-15 05:38:37,891 - Epoch 7 average train loss: 0.009451006444698365
2023-12-15 05:39:02,291 - 2023-12-15_05-13-02 with 7 epochs: Evaluation Results:
2023-12-15 05:39:02,291 - Training time: 1534.8435153961182 seconds
2023-12-15 05:39:02,291 - Inference time: 24.391998767852783 seconds
2023-12-15 05:39:02,292 - Precision: 0.9662090928141959
2023-12-15 05:39:02,292 - Recall: 0.9657867544212912
2023-12-15 05:39:02,292 - F-score: 0.9659586402808908
2023-12-15 05:39:02,292 - Accuracy: 0.9658823529411765
2023-12-15 05:39:02,292 - G-mean: 0.9658345524984385
2023-12-15 05:39:29,994 - distilbert-base-uncased 2023-12-15_05-13-02 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 05:39:29,994 - Training time: 1534.8435153961182 seconds
2023-12-15 05:39:29,994 - Inference time: 24.246999502182007 seconds
2023-12-15 05:39:29,994 - Precision: 0.9738685165451292
2023-12-15 05:39:29,994 - Recall: 0.975178223166963
2023-12-15 05:39:29,994 - F-score: 0.974246966238389
2023-12-15 05:39:29,994 - Accuracy: 0.9741176470588235
2023-12-15 05:39:29,995 - G-mean: 0.9746477908528836
2023-12-15 05:39:31,999 - ====================================================
2023-12-15 05:39:32,336 - Model: distilbert-base-uncased 2023-12-15_05-39-32 with 7 epochs
2023-12-15 05:43:12,645 - Epoch 1 average train loss: 0.6189500312419499
2023-12-15 05:46:53,289 - Epoch 2 average train loss: 0.12370054639875888
2023-12-15 05:50:33,575 - Epoch 3 average train loss: 0.06089143844869207
2023-12-15 05:54:13,308 - Epoch 4 average train loss: 0.03450939608025638
2023-12-15 05:57:52,903 - Epoch 5 average train loss: 0.02542705706512446
2023-12-15 06:01:32,547 - Epoch 6 average train loss: 0.01614438530509634
2023-12-15 06:05:11,868 - Epoch 7 average train loss: 0.007464769490898642
2023-12-15 06:05:36,246 - 2023-12-15_05-39-32 with 7 epochs: Evaluation Results:
2023-12-15 06:05:36,247 - Training time: 1539.478348016739 seconds
2023-12-15 06:05:36,247 - Inference time: 24.37100100517273 seconds
2023-12-15 06:05:36,247 - Precision: 0.9719843411130025
2023-12-15 06:05:36,247 - Recall: 0.9716759490469237
2023-12-15 06:05:36,247 - F-score: 0.971782282395391
2023-12-15 06:05:36,247 - Accuracy: 0.971764705882353
2023-12-15 06:05:36,247 - G-mean: 0.9717203264512583
2023-12-15 06:06:03,909 - distilbert-base-uncased 2023-12-15_05-39-32 with 7 epochs: Evaluation Results (completely new data):
2023-12-15 06:06:03,909 - Training time: 1539.478348016739 seconds
2023-12-15 06:06:03,909 - Inference time: 24.331971406936646 seconds
2023-12-15 06:06:03,909 - Precision: 0.9677275495104952
2023-12-15 06:06:03,909 - Recall: 0.9692067945301378
2023-12-15 06:06:03,910 - F-score: 0.9680279953351063
2023-12-15 06:06:03,910 - Accuracy: 0.9682352941176471
2023-12-15 06:06:03,910 - G-mean: 0.9687209225379154
2023-12-15 06:06:03,945 - Model distilbert-base-uncased 2023-12-15_05-39-32 not saved
2023-12-15 06:06:03,946 - ====================================================
2023-12-15 06:06:04,296 - Model: distilbert-base-uncased 2023-12-15_06-06-04 with 8 epochs
2023-12-15 06:09:43,517 - Epoch 1 average train loss: 0.5808430241924875
2023-12-15 06:13:22,920 - Epoch 2 average train loss: 0.11800470520468319
2023-12-15 06:17:02,253 - Epoch 3 average train loss: 0.060397547883365085
2023-12-15 06:20:41,547 - Epoch 4 average train loss: 0.037981181412606556
2023-12-15 06:24:20,941 - Epoch 5 average train loss: 0.016613850557957503
2023-12-15 06:28:00,274 - Epoch 6 average train loss: 0.008770214086124564
2023-12-15 06:31:39,602 - Epoch 7 average train loss: 0.004890390980495688
2023-12-15 06:35:19,025 - Epoch 8 average train loss: 0.0036006774964591707
2023-12-15 06:35:43,514 - 2023-12-15_06-06-04 with 8 epochs: Evaluation Results:
2023-12-15 06:35:43,514 - Training time: 1754.6760246753693 seconds
2023-12-15 06:35:43,514 - Inference time: 24.48199772834778 seconds
2023-12-15 06:35:43,514 - Precision: 0.9664645563760079
2023-12-15 06:35:43,514 - Recall: 0.9661008302664558
2023-12-15 06:35:43,514 - F-score: 0.9662652797013684
2023-12-15 06:35:43,514 - Accuracy: 0.9658823529411765
2023-12-15 06:35:43,514 - G-mean: 0.9659915854272171
2023-12-15 06:36:11,212 - distilbert-base-uncased 2023-12-15_06-06-04 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 06:36:11,212 - Training time: 1754.6760246753693 seconds
2023-12-15 06:36:11,213 - Inference time: 24.35002326965332 seconds
2023-12-15 06:36:11,213 - Precision: 0.9679842497151782
2023-12-15 06:36:11,213 - Recall: 0.9688857786933565
2023-12-15 06:36:11,213 - F-score: 0.96832336803788
2023-12-15 06:36:11,213 - Accuracy: 0.9682352941176471
2023-12-15 06:36:11,213 - G-mean: 0.9685604817973773
2023-12-15 06:36:13,027 - ====================================================
2023-12-15 06:36:13,269 - Model: distilbert-base-uncased 2023-12-15_06-36-13 with 8 epochs
2023-12-15 06:39:52,198 - Epoch 1 average train loss: 0.563035954999573
2023-12-15 06:43:31,522 - Epoch 2 average train loss: 0.10770532334552092
2023-12-15 06:47:10,744 - Epoch 3 average train loss: 0.053993189880514846
2023-12-15 06:50:50,330 - Epoch 4 average train loss: 0.036329266026287395
2023-12-15 06:54:29,885 - Epoch 5 average train loss: 0.017345515107325113
2023-12-15 06:58:10,090 - Epoch 6 average train loss: 0.010669212845455417
2023-12-15 07:01:50,120 - Epoch 7 average train loss: 0.003969289557289277
2023-12-15 07:05:30,002 - Epoch 8 average train loss: 0.004869028737405589
2023-12-15 07:05:54,500 - 2023-12-15_06-36-13 with 8 epochs: Evaluation Results:
2023-12-15 07:05:54,500 - Training time: 1756.6779894828796 seconds
2023-12-15 07:05:54,500 - Inference time: 24.490999698638916 seconds
2023-12-15 07:05:54,500 - Precision: 0.9698750345340734
2023-12-15 07:05:54,500 - Recall: 0.9694659871128997
2023-12-15 07:05:54,500 - F-score: 0.969647431811173
2023-12-15 07:05:54,501 - Accuracy: 0.9694117647058823
2023-12-15 07:05:54,501 - G-mean: 0.9694388755302967
2023-12-15 07:06:22,106 - distilbert-base-uncased 2023-12-15_06-36-13 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 07:06:22,107 - Training time: 1756.6779894828796 seconds
2023-12-15 07:06:22,107 - Inference time: 24.29596734046936 seconds
2023-12-15 07:06:22,107 - Precision: 0.9717238672074917
2023-12-15 07:06:22,107 - Recall: 0.9721318738938256
2023-12-15 07:06:22,107 - F-score: 0.9718677704797727
2023-12-15 07:06:22,107 - Accuracy: 0.971764705882353
2023-12-15 07:06:22,107 - G-mean: 0.9719482725501878
2023-12-15 07:06:23,943 - ====================================================
2023-12-15 07:06:24,215 - Model: distilbert-base-uncased 2023-12-15_07-06-24 with 8 epochs
2023-12-15 07:10:03,953 - Epoch 1 average train loss: 0.5718669950523797
2023-12-15 07:13:43,814 - Epoch 2 average train loss: 0.1109886620062239
2023-12-15 07:17:23,527 - Epoch 3 average train loss: 0.05747755086487707
2023-12-15 07:21:03,675 - Epoch 4 average train loss: 0.03189330486525946
2023-12-15 07:24:43,579 - Epoch 5 average train loss: 0.01607890447382541
2023-12-15 07:28:23,495 - Epoch 6 average train loss: 0.007511218712076216
2023-12-15 07:32:03,528 - Epoch 7 average train loss: 0.007177579595334143
2023-12-15 07:35:43,569 - Epoch 8 average train loss: 0.013197939228985513
2023-12-15 07:36:08,038 - 2023-12-15_07-06-24 with 8 epochs: Evaluation Results:
2023-12-15 07:36:08,038 - Training time: 1759.300511598587 seconds
2023-12-15 07:36:08,038 - Inference time: 24.460999965667725 seconds
2023-12-15 07:36:08,038 - Precision: 0.9615291267914763
2023-12-15 07:36:08,038 - Recall: 0.9616695968880838
2023-12-15 07:36:08,038 - F-score: 0.9615213143969352
2023-12-15 07:36:08,038 - Accuracy: 0.9611764705882353
2023-12-15 07:36:08,038 - G-mean: 0.9614230021218024
2023-12-15 07:36:35,730 - distilbert-base-uncased 2023-12-15_07-06-24 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 07:36:35,730 - Training time: 1759.300511598587 seconds
2023-12-15 07:36:35,730 - Inference time: 24.369965314865112 seconds
2023-12-15 07:36:35,730 - Precision: 0.9595610190754881
2023-12-15 07:36:35,730 - Recall: 0.9607178307895324
2023-12-15 07:36:35,730 - F-score: 0.9599243730712299
2023-12-15 07:36:35,730 - Accuracy: 0.96
2023-12-15 07:36:35,730 - G-mean: 0.9603588483259531
2023-12-15 07:36:35,767 - Model distilbert-base-uncased 2023-12-15_07-06-24 not saved
2023-12-15 07:36:35,768 - ====================================================
2023-12-15 07:36:36,030 - Model: distilbert-base-uncased 2023-12-15_07-36-36 with 8 epochs
2023-12-15 07:40:15,276 - Epoch 1 average train loss: 0.5900895098815946
2023-12-15 07:43:54,814 - Epoch 2 average train loss: 0.11542427512871868
2023-12-15 07:47:34,338 - Epoch 3 average train loss: 0.05845154653905946
2023-12-15 07:51:13,721 - Epoch 4 average train loss: 0.03216397293240709
2023-12-15 07:54:53,049 - Epoch 5 average train loss: 0.02091788664624533
2023-12-15 07:58:32,235 - Epoch 6 average train loss: 0.012292274967356421
2023-12-15 08:02:11,511 - Epoch 7 average train loss: 0.008080076692173916
2023-12-15 08:05:50,943 - Epoch 8 average train loss: 0.005152594737095676
2023-12-15 08:06:15,526 - 2023-12-15_07-36-36 with 8 epochs: Evaluation Results:
2023-12-15 08:06:15,526 - Training time: 1754.856256723404 seconds
2023-12-15 08:06:15,526 - Inference time: 24.575000286102295 seconds
2023-12-15 08:06:15,526 - Precision: 0.9659107598978288
2023-12-15 08:06:15,526 - Recall: 0.9660760780891066
2023-12-15 08:06:15,526 - F-score: 0.9657418632843774
2023-12-15 08:06:15,526 - Accuracy: 0.9658823529411765
2023-12-15 08:06:15,526 - G-mean: 0.9659792106587439
2023-12-15 08:06:43,340 - distilbert-base-uncased 2023-12-15_07-36-36 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 08:06:43,340 - Training time: 1754.856256723404 seconds
2023-12-15 08:06:43,340 - Inference time: 24.52803683280945 seconds
2023-12-15 08:06:43,340 - Precision: 0.9690200363596391
2023-12-15 08:06:43,340 - Recall: 0.9701188380982622
2023-12-15 08:06:43,340 - F-score: 0.9692771298633259
2023-12-15 08:06:43,340 - Accuracy: 0.9694117647058823
2023-12-15 08:06:43,340 - G-mean: 0.9697652369595728
2023-12-15 08:06:43,375 - Model distilbert-base-uncased 2023-12-15_07-36-36 not saved
2023-12-15 08:06:43,375 - ====================================================
2023-12-15 08:06:43,633 - Model: distilbert-base-uncased 2023-12-15_08-06-43 with 8 epochs
2023-12-15 08:10:22,703 - Epoch 1 average train loss: 0.5878498246827546
2023-12-15 08:14:02,036 - Epoch 2 average train loss: 0.11744538452257128
2023-12-15 08:17:41,434 - Epoch 3 average train loss: 0.0607282679944354
2023-12-15 08:21:21,018 - Epoch 4 average train loss: 0.028719533016247783
2023-12-15 08:25:00,209 - Epoch 5 average train loss: 0.01090267569730606
2023-12-15 08:28:39,526 - Epoch 6 average train loss: 0.011012770626762444
2023-12-15 08:32:18,775 - Epoch 7 average train loss: 0.005425340759273454
2023-12-15 08:35:57,986 - Epoch 8 average train loss: 0.010270230703326354
2023-12-15 08:36:22,566 - 2023-12-15_08-06-43 with 8 epochs: Evaluation Results:
2023-12-15 08:36:22,566 - Training time: 1754.2963135242462 seconds
2023-12-15 08:36:22,566 - Inference time: 24.571999549865723 seconds
2023-12-15 08:36:22,566 - Precision: 0.9583300966626593
2023-12-15 08:36:22,566 - Recall: 0.9576693483596941
2023-12-15 08:36:22,566 - F-score: 0.9577970622045223
2023-12-15 08:36:22,566 - Accuracy: 0.9576470588235294
2023-12-15 08:36:22,566 - G-mean: 0.957658203526763
2023-12-15 08:36:50,282 - distilbert-base-uncased 2023-12-15_08-06-43 with 8 epochs: Evaluation Results (completely new data):
2023-12-15 08:36:50,282 - Training time: 1754.2963135242462 seconds
2023-12-15 08:36:50,282 - Inference time: 24.449000358581543 seconds
2023-12-15 08:36:50,282 - Precision: 0.9672396259851492
2023-12-15 08:36:50,282 - Recall: 0.9676013029944821
2023-12-15 08:36:50,283 - F-score: 0.9671610034226183
2023-12-15 08:36:50,283 - Accuracy: 0.9670588235294117
2023-12-15 08:36:50,283 - G-mean: 0.9673300252340821
2023-12-15 08:36:50,316 - Model distilbert-base-uncased 2023-12-15_08-06-43 not saved
2023-12-15 08:36:50,317 - ====================================================
2023-12-15 08:36:50,618 - Model: distilbert-base-uncased 2023-12-15_08-36-50 with 9 epochs
2023-12-15 08:40:29,966 - Epoch 1 average train loss: 0.5664451593423591
2023-12-15 08:44:09,478 - Epoch 2 average train loss: 0.10614220121984973
2023-12-15 08:47:48,629 - Epoch 3 average train loss: 0.04817861151059761
2023-12-15 08:51:28,386 - Epoch 4 average train loss: 0.02120162726736025
2023-12-15 08:55:08,016 - Epoch 5 average train loss: 0.016316531531731873
2023-12-15 08:58:47,618 - Epoch 6 average train loss: 0.005872615701039596
2023-12-15 09:02:27,194 - Epoch 7 average train loss: 0.00530538373724352
2023-12-15 09:06:06,801 - Epoch 8 average train loss: 0.0069935335071846455
2023-12-15 09:09:46,440 - Epoch 9 average train loss: 0.006717644311652025
2023-12-15 09:10:10,995 - 2023-12-15_08-36-50 with 9 epochs: Evaluation Results:
2023-12-15 09:10:10,995 - Training time: 1975.7675414085388 seconds
2023-12-15 09:10:10,995 - Inference time: 24.54799175262451 seconds
2023-12-15 09:10:10,995 - Precision: 0.9689886348741954
2023-12-15 09:10:10,995 - Recall: 0.9679848039336694
2023-12-15 09:10:10,995 - F-score: 0.9684277338444243
2023-12-15 09:10:10,995 - Accuracy: 0.9682352941176471
2023-12-15 09:10:10,995 - G-mean: 0.9681100409241344
2023-12-15 09:10:38,732 - distilbert-base-uncased 2023-12-15_08-36-50 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 09:10:38,732 - Training time: 1975.7675414085388 seconds
2023-12-15 09:10:38,732 - Inference time: 24.42496943473816 seconds
2023-12-15 09:10:38,732 - Precision: 0.9755610926190016
2023-12-15 09:10:38,732 - Recall: 0.9758642717828865
2023-12-15 09:10:38,732 - F-score: 0.9756289433380483
2023-12-15 09:10:38,732 - Accuracy: 0.9752941176470589
2023-12-15 09:10:38,733 - G-mean: 0.9755791530633381
2023-12-15 09:10:40,576 - ====================================================
2023-12-15 09:10:40,804 - Model: distilbert-base-uncased 2023-12-15_09-10-40 with 9 epochs
2023-12-15 09:14:19,649 - Epoch 1 average train loss: 0.5970144224955755
2023-12-15 09:17:58,608 - Epoch 2 average train loss: 0.1126477878803716
2023-12-15 09:21:37,731 - Epoch 3 average train loss: 0.05640806643511442
2023-12-15 09:25:16,779 - Epoch 4 average train loss: 0.035320998051149
2023-12-15 09:28:56,042 - Epoch 5 average train loss: 0.02116124019437634
2023-12-15 09:32:35,496 - Epoch 6 average train loss: 0.012831801052533967
2023-12-15 09:36:14,807 - Epoch 7 average train loss: 0.010626846023232621
2023-12-15 09:39:54,257 - Epoch 8 average train loss: 0.00465545836433782
2023-12-15 09:43:33,342 - Epoch 9 average train loss: 0.005728763178930796
2023-12-15 09:43:57,851 - 2023-12-15_09-10-40 with 9 epochs: Evaluation Results:
2023-12-15 09:43:57,851 - Training time: 1972.4840631484985 seconds
2023-12-15 09:43:57,851 - Inference time: 24.501999139785767 seconds
2023-12-15 09:43:57,851 - Precision: 0.9712352323965341
2023-12-15 09:43:57,851 - Recall: 0.9706587210760566
2023-12-15 09:43:57,851 - F-score: 0.970915659173556
2023-12-15 09:43:57,851 - Accuracy: 0.9705882352941176
2023-12-15 09:43:57,851 - G-mean: 0.9706234775452606
2023-12-15 09:44:25,530 - distilbert-base-uncased 2023-12-15_09-10-40 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 09:44:25,530 - Training time: 1972.4840631484985 seconds
2023-12-15 09:44:25,530 - Inference time: 24.40297245979309 seconds
2023-12-15 09:44:25,531 - Precision: 0.9676863842996566
2023-12-15 09:44:25,531 - Recall: 0.9674104423958967
2023-12-15 09:44:25,531 - F-score: 0.9674008010604274
2023-12-15 09:44:25,531 - Accuracy: 0.9670588235294117
2023-12-15 09:44:25,531 - G-mean: 0.9672346169846505
2023-12-15 09:44:25,567 - Model distilbert-base-uncased 2023-12-15_09-10-40 not saved
2023-12-15 09:44:25,567 - ====================================================
2023-12-15 09:44:25,823 - Model: distilbert-base-uncased 2023-12-15_09-44-25 with 9 epochs
2023-12-15 09:48:04,849 - Epoch 1 average train loss: 0.5826783278496827
2023-12-15 09:51:43,980 - Epoch 2 average train loss: 0.11303220229113803
2023-12-15 09:55:23,090 - Epoch 3 average train loss: 0.058541424278827275
2023-12-15 09:59:02,226 - Epoch 4 average train loss: 0.030424827086355757
2023-12-15 10:02:41,303 - Epoch 5 average train loss: 0.01846886082415414
2023-12-15 10:06:39,299 - Epoch 6 average train loss: 0.015560591195107384
2023-12-15 10:11:00,093 - Epoch 7 average train loss: 0.009231153922840296
2023-12-15 10:15:31,743 - Epoch 8 average train loss: 0.010735079262042693
2023-12-15 10:19:33,124 - Epoch 9 average train loss: 0.0025096072167284816
2023-12-15 10:19:58,509 - 2023-12-15_09-44-25 with 9 epochs: Evaluation Results:
2023-12-15 10:19:58,509 - Training time: 2107.2398064136505 seconds
2023-12-15 10:19:58,509 - Inference time: 25.37799859046936 seconds
2023-12-15 10:19:58,509 - Precision: 0.9643764874944966
2023-12-15 10:19:58,509 - Recall: 0.9615331910304435
2023-12-15 10:19:58,510 - F-score: 0.9625956861220611
2023-12-15 10:19:58,510 - Accuracy: 0.9623529411764706
2023-12-15 10:19:58,510 - G-mean: 0.9619429787814581
2023-12-15 10:20:27,285 - distilbert-base-uncased 2023-12-15_09-44-25 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 10:20:27,285 - Training time: 2107.2398064136505 seconds
2023-12-15 10:20:27,285 - Inference time: 24.578996181488037 seconds
2023-12-15 10:20:27,285 - Precision: 0.9680108162200216
2023-12-15 10:20:27,285 - Recall: 0.9653533982720479
2023-12-15 10:20:27,285 - F-score: 0.9663306673982224
2023-12-15 10:20:27,286 - Accuracy: 0.9658823529411765
2023-12-15 10:20:27,286 - G-mean: 0.9656178393871803
2023-12-15 10:20:27,337 - Model distilbert-base-uncased 2023-12-15_09-44-25 not saved
2023-12-15 10:20:27,337 - ====================================================
2023-12-15 10:20:27,579 - Model: distilbert-base-uncased 2023-12-15_10-20-27 with 9 epochs
2023-12-15 10:24:16,435 - Epoch 1 average train loss: 0.5759518711356556
2023-12-15 10:28:04,773 - Epoch 2 average train loss: 0.11141825188170461
2023-12-15 10:31:53,911 - Epoch 3 average train loss: 0.05433050091542742
2023-12-15 10:35:42,231 - Epoch 4 average train loss: 0.029936680452459875
2023-12-15 10:39:30,649 - Epoch 5 average train loss: 0.01565265906125526
2023-12-15 10:43:18,946 - Epoch 6 average train loss: 0.007235477322863196
2023-12-15 10:47:07,226 - Epoch 7 average train loss: 0.008776207814225927
2023-12-15 10:50:55,562 - Epoch 8 average train loss: 0.013230556679225307
2023-12-15 10:54:44,729 - Epoch 9 average train loss: 0.008612003841742134
2023-12-15 10:55:09,352 - 2023-12-15_10-20-27 with 9 epochs: Evaluation Results:
2023-12-15 10:55:09,352 - Training time: 2057.0895483493805 seconds
2023-12-15 10:55:09,352 - Inference time: 24.614001512527466 seconds
2023-12-15 10:55:09,352 - Precision: 0.9639309281958791
2023-12-15 10:55:09,352 - Recall: 0.9633670265931038
2023-12-15 10:55:09,353 - F-score: 0.9635288405454085
2023-12-15 10:55:09,353 - Accuracy: 0.9635294117647059
2023-12-15 10:55:09,353 - G-mean: 0.9634482157577371
2023-12-15 10:55:37,355 - distilbert-base-uncased 2023-12-15_10-20-27 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 10:55:37,355 - Training time: 2057.0895483493805 seconds
2023-12-15 10:55:37,355 - Inference time: 24.62499976158142 seconds
2023-12-15 10:55:37,355 - Precision: 0.964093376396891
2023-12-15 10:55:37,355 - Recall: 0.9652108626381395
2023-12-15 10:55:37,355 - F-score: 0.9644609078223599
2023-12-15 10:55:37,355 - Accuracy: 0.9647058823529412
2023-12-15 10:55:37,355 - G-mean: 0.9649583394623676
2023-12-15 10:55:37,394 - Model distilbert-base-uncased 2023-12-15_10-20-27 not saved
2023-12-15 10:55:37,394 - ====================================================
2023-12-15 10:55:37,754 - Model: distilbert-base-uncased 2023-12-15_10-55-37 with 9 epochs
2023-12-15 10:59:18,509 - Epoch 1 average train loss: 0.60075224813293
2023-12-15 11:02:59,547 - Epoch 2 average train loss: 0.11447733522776295
2023-12-15 11:06:41,386 - Epoch 3 average train loss: 0.05543300858424867
2023-12-15 11:10:49,186 - Epoch 4 average train loss: 0.03087779581108514
2023-12-15 11:14:51,658 - Epoch 5 average train loss: 0.019489937430412014
2023-12-15 11:18:46,135 - Epoch 6 average train loss: 0.013763454859144986
2023-12-15 11:22:48,310 - Epoch 7 average train loss: 0.009250271685236571
2023-12-15 11:27:10,523 - Epoch 8 average train loss: 0.008591194491806056
2023-12-15 11:31:28,630 - Epoch 9 average train loss: 0.009168761715259547
2023-12-15 11:31:56,179 - 2023-12-15_10-55-37 with 9 epochs: Evaluation Results:
2023-12-15 11:31:56,179 - Training time: 2150.8094158172607 seconds
2023-12-15 11:31:56,179 - Inference time: 27.541024923324585 seconds
2023-12-15 11:31:56,179 - Precision: 0.967274486124832
2023-12-15 11:31:56,179 - Recall: 0.9670119993828383
2023-12-15 11:31:56,179 - F-score: 0.9670747295893161
2023-12-15 11:31:56,179 - Accuracy: 0.9670588235294117
2023-12-15 11:31:56,179 - G-mean: 0.9670354111727202
2023-12-15 11:32:27,200 - distilbert-base-uncased 2023-12-15_10-55-37 with 9 epochs: Evaluation Results (completely new data):
2023-12-15 11:32:27,200 - Training time: 2150.8094158172607 seconds
2023-12-15 11:32:27,201 - Inference time: 27.388994216918945 seconds
2023-12-15 11:32:27,201 - Precision: 0.9665515266523972
2023-12-15 11:32:27,201 - Recall: 0.9677871911630384
2023-12-15 11:32:27,201 - F-score: 0.9669952068722069
2023-12-15 11:32:27,201 - Accuracy: 0.9670588235294117
2023-12-15 11:32:27,201 - G-mean: 0.9674229387982083
2023-12-15 11:32:27,249 - Model distilbert-base-uncased 2023-12-15_10-55-37 not saved
2023-12-15 11:32:27,249 - ====================================================
2023-12-15 11:32:27,584 - Model: distilbert-base-uncased 2023-12-15_11-32-27 with 10 epochs
2023-12-15 11:36:33,115 - Epoch 1 average train loss: 0.5884760637143079
2023-12-15 11:40:38,862 - Epoch 2 average train loss: 0.11236985540127054
2023-12-15 11:44:45,664 - Epoch 3 average train loss: 0.05661254660609891
2023-12-15 11:48:54,236 - Epoch 4 average train loss: 0.03250516485368066
2023-12-15 11:53:03,777 - Epoch 5 average train loss: 0.018266350623384556
2023-12-15 11:57:00,018 - Epoch 6 average train loss: 0.011566257846870405
2023-12-15 12:01:02,936 - Epoch 7 average train loss: 0.006600252688615857
2023-12-15 12:05:08,993 - Epoch 8 average train loss: 0.005299840611850788
2023-12-15 12:09:13,713 - Epoch 9 average train loss: 0.0009401270614591126
2023-12-15 12:12:56,418 - Epoch 10 average train loss: 0.005093268549377064
2023-12-15 12:13:23,723 - 2023-12-15_11-32-27 with 10 epochs: Evaluation Results:
2023-12-15 12:13:23,723 - Training time: 2428.7727024555206 seconds
2023-12-15 12:13:23,723 - Inference time: 27.29859471321106 seconds
2023-12-15 12:13:23,723 - Precision: 0.967462836862971
2023-12-15 12:13:23,723 - Recall: 0.9673030719541373
2023-12-15 12:13:23,723 - F-score: 0.9671163976428799
2023-12-15 12:13:23,723 - Accuracy: 0.9670588235294117
2023-12-15 12:13:23,723 - G-mean: 0.9671809400315713
2023-12-15 12:13:52,251 - distilbert-base-uncased 2023-12-15_11-32-27 with 10 epochs: Evaluation Results (completely new data):
2023-12-15 12:13:52,251 - Training time: 2428.7727024555206 seconds
2023-12-15 12:13:52,251 - Inference time: 24.759000778198242 seconds
2023-12-15 12:13:52,251 - Precision: 0.9659534234146975
2023-12-15 12:13:52,251 - Recall: 0.9665970805551067
2023-12-15 12:13:52,251 - F-score: 0.965796571647755
2023-12-15 12:13:52,251 - Accuracy: 0.9658823529411765
2023-12-15 12:13:52,251 - G-mean: 0.9662396506626285
2023-12-15 12:13:54,097 - ====================================================
2023-12-15 12:13:54,348 - Model: distilbert-base-uncased 2023-12-15_12-13-54 with 10 epochs
2023-12-15 12:17:36,601 - Epoch 1 average train loss: 0.6024641481918447
2023-12-15 12:21:18,348 - Epoch 2 average train loss: 0.11279089583850958
2023-12-15 12:25:01,267 - Epoch 3 average train loss: 0.053938211830442444
2023-12-15 12:28:44,650 - Epoch 4 average train loss: 0.028990326477214695
2023-12-15 12:32:26,270 - Epoch 5 average train loss: 0.015312168170040583
2023-12-15 12:36:08,310 - Epoch 6 average train loss: 0.012453386276371449
2023-12-15 12:39:50,554 - Epoch 7 average train loss: 0.009399005026857862
2023-12-15 12:43:32,425 - Epoch 8 average train loss: 0.00653566134699812
2023-12-15 12:47:19,339 - Epoch 9 average train loss: 0.005767349333503208
2023-12-15 12:51:20,342 - Epoch 10 average train loss: 0.006490293956519642
2023-12-15 12:51:44,738 - 2023-12-15_12-13-54 with 10 epochs: Evaluation Results:
2023-12-15 12:51:44,738 - Training time: 2245.9395627975464 seconds
2023-12-15 12:51:44,738 - Inference time: 24.38700032234192 seconds
2023-12-15 12:51:44,738 - Precision: 0.9707068841389018
2023-12-15 12:51:44,738 - Recall: 0.9703789051327659
2023-12-15 12:51:44,738 - F-score: 0.9704916975012836
2023-12-15 12:51:44,738 - Accuracy: 0.9705882352941176
2023-12-15 12:51:44,738 - G-mean: 0.970483564569462
2023-12-15 12:52:12,389 - distilbert-base-uncased 2023-12-15_12-13-54 with 10 epochs: Evaluation Results (completely new data):
2023-12-15 12:52:12,389 - Training time: 2245.9395627975464 seconds
2023-12-15 12:52:12,389 - Inference time: 24.27500009536743 seconds
2023-12-15 12:52:12,389 - Precision: 0.972672712045631
2023-12-15 12:52:12,389 - Recall: 0.9737937541534791
2023-12-15 12:52:12,389 - F-score: 0.9731078262799207
2023-12-15 12:52:12,389 - Accuracy: 0.9729411764705882
2023-12-15 12:52:12,389 - G-mean: 0.9733673719648696
2023-12-15 12:52:14,215 - ====================================================
2023-12-15 12:52:14,481 - Model: distilbert-base-uncased 2023-12-15_12-52-14 with 10 epochs
2023-12-15 12:55:53,832 - Epoch 1 average train loss: 0.618624698659953
2023-12-15 12:59:33,263 - Epoch 2 average train loss: 0.12142788015305996
2023-12-15 13:03:13,413 - Epoch 3 average train loss: 0.05928600730922292
2023-12-15 13:08:12,849 - Epoch 4 average train loss: 0.032113917702808976
2023-12-15 13:12:56,690 - Epoch 5 average train loss: 0.015621712913681918
2023-12-15 13:17:28,866 - Epoch 6 average train loss: 0.011746505039251025
2023-12-15 13:22:22,114 - Epoch 7 average train loss: 0.008661655112347729
2023-12-15 13:26:44,107 - Epoch 8 average train loss: 0.003587236974426025
2023-12-15 13:31:06,762 - Epoch 9 average train loss: 0.00452744920120441
2023-12-15 13:35:20,396 - Epoch 10 average train loss: 0.006052547621220583
2023-12-15 13:35:48,587 - 2023-12-15_12-52-14 with 10 epochs: Evaluation Results:
2023-12-15 13:35:48,587 - Training time: 2585.862292766571 seconds
2023-12-15 13:35:48,587 - Inference time: 28.181999444961548 seconds
2023-12-15 13:35:48,587 - Precision: 0.9646438090960588
2023-12-15 13:35:48,587 - Recall: 0.9630828940134217
2023-12-15 13:35:48,587 - F-score: 0.9636404326940762
2023-12-15 13:35:48,587 - Accuracy: 0.9635294117647059
2023-12-15 13:35:48,587 - G-mean: 0.9633061270174724
2023-12-15 13:36:19,185 - distilbert-base-uncased 2023-12-15_12-52-14 with 10 epochs: Evaluation Results (completely new data):
2023-12-15 13:36:19,185 - Training time: 2585.862292766571 seconds
2023-12-15 13:36:19,185 - Inference time: 27.049028873443604 seconds
2023-12-15 13:36:19,185 - Precision: 0.9669094476282719
2023-12-15 13:36:19,185 - Recall: 0.9671239051190936
2023-12-15 13:36:19,185 - F-score: 0.9669788323167527
2023-12-15 13:36:19,185 - Accuracy: 0.9670588235294117
2023-12-15 13:36:19,185 - G-mean: 0.9670913637767846
2023-12-15 13:36:19,229 - Model distilbert-base-uncased 2023-12-15_12-52-14 not saved
2023-12-15 13:36:19,230 - ====================================================
2023-12-15 13:36:19,594 - Model: distilbert-base-uncased 2023-12-15_13-36-19 with 10 epochs
