2023-12-13 17:42:42,746 - ====================================================
2023-12-13 17:42:42,995 - Model: distilbert-base-uncased 2023-12-13_17-42-42 with 5 epochs
2023-12-13 17:46:51,022 - Epoch 1 average train loss: 0.5934280015791163
2023-12-13 17:50:33,142 - Epoch 2 average train loss: 0.11406666719080771
2023-12-13 17:54:20,998 - Epoch 3 average train loss: 0.05576713295872597
2023-12-13 17:58:19,139 - Epoch 4 average train loss: 0.03229552897350753
2023-12-13 18:02:26,484 - Epoch 5 average train loss: 0.018423950101577623
2023-12-13 18:02:54,249 - 2023-12-13_17-42-42 with 5 epochs: Evaluation Results:
2023-12-13 18:02:54,249 - Training time: 1183.4224004745483 seconds
2023-12-13 18:02:54,254 - Inference time: 27.74400281906128 seconds
2023-12-13 18:02:54,254 - Precision: 0.9611499860144093
2023-12-13 18:02:54,254 - Recall: 0.9595036546364225
2023-12-13 18:02:54,254 - F-score: 0.9601513935164954
2023-12-13 18:02:54,254 - Accuracy: 0.96
2023-12-13 18:02:54,254 - G-mean: 0.9597517952319576
2023-12-13 18:03:25,994 - distilbert-base-uncased 2023-12-13_17-42-42 with 5 epochs: Evaluation Results (completely new data):
2023-12-13 18:03:25,994 - Training time: 1183.4224004745483 seconds
2023-12-13 18:03:25,994 - Inference time: 26.62999653816223 seconds
2023-12-13 18:03:25,995 - Precision: 0.9734141219329262
2023-12-13 18:03:25,995 - Recall: 0.9734753616719232
2023-12-13 18:03:25,995 - F-score: 0.9733483098989064
2023-12-13 18:03:25,995 - Accuracy: 0.9729411764705882
2023-12-13 18:03:25,995 - G-mean: 0.9732082324200778
2023-12-13 18:03:27,876 - ====================================================
2023-12-13 18:03:28,113 - Model: distilbert-base-uncased 2023-12-13_18-03-28 with 5 epochs
2023-12-13 18:07:23,779 - Epoch 1 average train loss: 0.5931759953148225
2023-12-13 18:11:32,156 - Epoch 2 average train loss: 0.10804693759364241
2023-12-13 18:15:48,565 - Epoch 3 average train loss: 0.05154644954182646
2023-12-13 18:19:50,570 - Epoch 4 average train loss: 0.026397050788133022
2023-12-13 18:24:04,599 - Epoch 5 average train loss: 0.018558258264709044
2023-12-13 18:24:34,191 - 2023-12-13_18-03-28 with 5 epochs: Evaluation Results:
2023-12-13 18:24:34,191 - Training time: 1236.4324107170105 seconds
2023-12-13 18:24:34,191 - Inference time: 29.583000421524048 seconds
2023-12-13 18:24:34,191 - Precision: 0.9676134336968923
2023-12-13 18:24:34,191 - Recall: 0.9669838050208395
2023-12-13 18:24:34,192 - F-score: 0.9671883129776122
2023-12-13 18:24:34,192 - Accuracy: 0.9670588235294117
2023-12-13 18:24:34,192 - G-mean: 0.9670213135476627
2023-12-13 18:25:06,670 - distilbert-base-uncased 2023-12-13_18-03-28 with 5 epochs: Evaluation Results (completely new data):
2023-12-13 18:25:06,670 - Training time: 1236.4324107170105 seconds
2023-12-13 18:25:06,671 - Inference time: 28.212989568710327 seconds
2023-12-13 18:25:06,671 - Precision: 0.9703858800219308
2023-12-13 18:25:06,671 - Recall: 0.9712174256285803
2023-12-13 18:25:06,671 - F-score: 0.9706931972296793
2023-12-13 18:25:06,671 - Accuracy: 0.9705882352941176
2023-12-13 18:25:06,671 - G-mean: 0.970902779493261
2023-12-13 18:25:06,736 - Model distilbert-base-uncased 2023-12-13_18-03-28 not saved
2023-12-13 18:25:06,736 - ====================================================
2023-12-13 18:25:07,089 - Model: distilbert-base-uncased 2023-12-13_18-25-07 with 5 epochs
2023-12-13 18:29:45,297 - Epoch 1 average train loss: 0.6269910712715457
2023-12-13 18:34:06,974 - Epoch 2 average train loss: 0.11533780415706775
2023-12-13 18:38:06,842 - Epoch 3 average train loss: 0.052358112952069324
2023-12-13 18:42:00,521 - Epoch 4 average train loss: 0.029405003084034168
2023-12-13 18:46:49,002 - Epoch 5 average train loss: 0.022353806999950286
2023-12-13 18:47:20,315 - 2023-12-13_18-25-07 with 5 epochs: Evaluation Results:
2023-12-13 18:47:20,315 - Training time: 1301.8144381046295 seconds
2023-12-13 18:47:20,315 - Inference time: 31.305001497268677 seconds
2023-12-13 18:47:20,315 - Precision: 0.9641244140753944
2023-12-13 18:47:20,315 - Recall: 0.9617539948945112
2023-12-13 18:47:20,315 - F-score: 0.9626131072422158
2023-12-13 18:47:20,316 - Accuracy: 0.9623529411764706
2023-12-13 18:47:20,316 - G-mean: 0.9620534214246905
2023-12-13 18:47:49,164 - distilbert-base-uncased 2023-12-13_18-25-07 with 5 epochs: Evaluation Results (completely new data):
2023-12-13 18:47:49,165 - Training time: 1301.8144381046295 seconds
2023-12-13 18:47:49,165 - Inference time: 25.304999589920044 seconds
2023-12-13 18:47:49,165 - Precision: 0.9633404506199177
2023-12-13 18:47:49,165 - Recall: 0.9619591726118634
2023-12-13 18:47:49,165 - F-score: 0.9624323676564414
2023-12-13 18:47:49,165 - Accuracy: 0.9623529411764706
2023-12-13 18:47:49,165 - G-mean: 0.9621560367501266
2023-12-13 18:47:49,206 - Model distilbert-base-uncased 2023-12-13_18-25-07 not saved
2023-12-13 18:47:49,207 - ====================================================
2023-12-13 18:47:49,447 - Model: distilbert-base-uncased 2023-12-13_18-47-49 with 6 epochs
2023-12-13 18:52:22,489 - Epoch 1 average train loss: 0.5787822256631711
2023-12-13 18:56:47,306 - Epoch 2 average train loss: 0.10719402404173332
2023-12-13 19:01:12,921 - Epoch 3 average train loss: 0.05900447072022978
2023-12-13 19:05:50,797 - Epoch 4 average train loss: 0.03271101636483389
2023-12-13 19:10:28,345 - Epoch 5 average train loss: 0.015093481263748425
2023-12-13 19:15:10,198 - Epoch 6 average train loss: 0.01593947530573453
2023-12-13 19:15:41,894 - 2023-12-13_18-47-49 with 6 epochs: Evaluation Results:
2023-12-13 19:15:41,894 - Training time: 1640.6853952407837 seconds
2023-12-13 19:15:41,894 - Inference time: 31.684998989105225 seconds
2023-12-13 19:15:41,894 - Precision: 0.968424683086821
2023-12-13 19:15:41,894 - Recall: 0.9682107988858701
2023-12-13 19:15:41,894 - F-score: 0.9682309191487815
2023-12-13 19:15:41,894 - Accuracy: 0.9682352941176471
2023-12-13 19:15:41,894 - G-mean: 0.968223046424295
2023-12-13 19:16:17,342 - distilbert-base-uncased 2023-12-13_18-47-49 with 6 epochs: Evaluation Results (completely new data):
2023-12-13 19:16:17,343 - Training time: 1640.6853952407837 seconds
2023-12-13 19:16:17,343 - Inference time: 31.18600106239319 seconds
2023-12-13 19:16:17,343 - Precision: 0.9689761844224087
2023-12-13 19:16:17,343 - Recall: 0.9698620254288371
2023-12-13 19:16:17,343 - F-score: 0.9693007303667269
2023-12-13 19:16:17,343 - Accuracy: 0.9694117647058823
2023-12-13 19:16:17,343 - G-mean: 0.969636868931968
2023-12-13 19:16:17,421 - Model distilbert-base-uncased 2023-12-13_18-47-49 not saved
2023-12-13 19:16:17,422 - ====================================================
2023-12-13 19:16:17,756 - Model: distilbert-base-uncased 2023-12-13_19-16-17 with 6 epochs
2023-12-13 19:21:07,010 - Epoch 1 average train loss: 0.6040168573751169
2023-12-13 19:25:51,166 - Epoch 2 average train loss: 0.11587740514208289
2023-12-13 19:30:33,270 - Epoch 3 average train loss: 0.056392657813780445
2023-12-13 19:34:54,727 - Epoch 4 average train loss: 0.03131630329004324
2023-12-13 19:38:48,688 - Epoch 5 average train loss: 0.015202329859740156
2023-12-13 19:42:37,104 - Epoch 6 average train loss: 0.011497348380987258
2023-12-13 19:43:02,492 - 2023-12-13_19-16-17 with 6 epochs: Evaluation Results:
2023-12-13 19:43:02,493 - Training time: 1579.2305591106415 seconds
2023-12-13 19:43:02,493 - Inference time: 25.382000207901 seconds
2023-12-13 19:43:02,493 - Precision: 0.9700396733017935
2023-12-13 19:43:02,493 - Recall: 0.9691527857194769
2023-12-13 19:43:02,493 - F-score: 0.9695017157252097
2023-12-13 19:43:02,493 - Accuracy: 0.9694117647058823
2023-12-13 19:43:02,493 - G-mean: 0.9692822665632236
2023-12-13 19:43:30,919 - distilbert-base-uncased 2023-12-13_19-16-17 with 6 epochs: Evaluation Results (completely new data):
2023-12-13 19:43:30,919 - Training time: 1579.2305591106415 seconds
2023-12-13 19:43:30,919 - Inference time: 24.97003984451294 seconds
2023-12-13 19:43:30,919 - Precision: 0.9691603347637251
2023-12-13 19:43:30,919 - Recall: 0.9697995711649643
2023-12-13 19:43:30,919 - F-score: 0.9694246957641754
2023-12-13 19:43:30,919 - Accuracy: 0.9694117647058823
2023-12-13 19:43:30,919 - G-mean: 0.9696056485468904
2023-12-13 19:43:30,956 - Model distilbert-base-uncased 2023-12-13_19-16-17 not saved
2023-12-13 19:43:30,957 - ====================================================
2023-12-13 19:43:31,242 - Model: distilbert-base-uncased 2023-12-13_19-43-31 with 6 epochs
2023-12-13 19:47:18,594 - Epoch 1 average train loss: 0.5813563622797236
2023-12-13 19:51:06,287 - Epoch 2 average train loss: 0.11267618199481684
2023-12-13 19:54:54,272 - Epoch 3 average train loss: 0.05182864544365336
2023-12-13 19:58:42,019 - Epoch 4 average train loss: 0.031307340198579954
2023-12-13 20:02:29,574 - Epoch 5 average train loss: 0.024076536479953895
2023-12-13 20:06:16,949 - Epoch 6 average train loss: 0.014904134294209892
2023-12-13 20:06:42,201 - 2023-12-13_19-43-31 with 6 epochs: Evaluation Results:
2023-12-13 20:06:42,201 - Training time: 1365.6413373947144 seconds
2023-12-13 20:06:42,202 - Inference time: 25.246038675308228 seconds
2023-12-13 20:06:42,202 - Precision: 0.9672921989006547
2023-12-13 20:06:42,202 - Recall: 0.9654366697707694
2023-12-13 20:06:42,202 - F-score: 0.9661891758588131
2023-12-13 20:06:42,202 - Accuracy: 0.9658823529411765
2023-12-13 20:06:42,202 - G-mean: 0.9656594856438186
2023-12-13 20:07:10,731 - distilbert-base-uncased 2023-12-13_19-43-31 with 6 epochs: Evaluation Results (completely new data):
2023-12-13 20:07:10,731 - Training time: 1365.6413373947144 seconds
2023-12-13 20:07:10,731 - Inference time: 24.951040029525757 seconds
2023-12-13 20:07:10,731 - Precision: 0.9713159283506883
2023-12-13 20:07:10,731 - Recall: 0.9705771428585013
2023-12-13 20:07:10,731 - F-score: 0.9707406891028679
2023-12-13 20:07:10,731 - Accuracy: 0.9705882352941176
2023-12-13 20:07:10,731 - G-mean: 0.9705826890604631
2023-12-13 20:07:10,770 - Model distilbert-base-uncased 2023-12-13_19-43-31 not saved
2023-12-13 20:07:10,770 - ====================================================
2023-12-13 20:07:11,028 - Model: distilbert-base-uncased 2023-12-13_20-07-11 with 7 epochs
2023-12-13 20:11:08,453 - Epoch 1 average train loss: 0.5748698670811513
2023-12-13 20:15:21,056 - Epoch 2 average train loss: 0.11511042501120006
2023-12-13 20:19:22,219 - Epoch 3 average train loss: 0.0553016444701044
2023-12-13 20:23:17,303 - Epoch 4 average train loss: 0.02895753859788837
2023-12-13 20:27:12,467 - Epoch 5 average train loss: 0.019620727720560834
2023-12-13 20:30:59,594 - Epoch 6 average train loss: 0.016473909680498763
2023-12-13 20:34:44,976 - Epoch 7 average train loss: 0.00471519323925296
2023-12-13 20:35:10,012 - 2023-12-13_20-07-11 with 7 epochs: Evaluation Results:
2023-12-13 20:35:10,012 - Training time: 1653.8848707675934 seconds
2023-12-13 20:35:10,012 - Inference time: 25.02700138092041 seconds
2023-12-13 20:35:10,012 - Precision: 0.9714841510182228
2023-12-13 20:35:10,012 - Recall: 0.9705945179087003
2023-12-13 20:35:10,012 - F-score: 0.9709609550696591
2023-12-13 20:35:10,012 - Accuracy: 0.9705882352941176
2023-12-13 20:35:10,012 - G-mean: 0.9705913765963256
2023-12-13 20:35:38,300 - distilbert-base-uncased 2023-12-13_20-07-11 with 7 epochs: Evaluation Results (completely new data):
2023-12-13 20:35:38,300 - Training time: 1653.8848707675934 seconds
2023-12-13 20:35:38,300 - Inference time: 24.835997343063354 seconds
2023-12-13 20:35:38,300 - Precision: 0.9674573877059623
2023-12-13 20:35:38,300 - Recall: 0.9675611962108375
2023-12-13 20:35:38,300 - F-score: 0.9674026222092019
2023-12-13 20:35:38,300 - Accuracy: 0.9670588235294117
2023-12-13 20:35:38,300 - G-mean: 0.9673099772567028
2023-12-13 20:35:38,338 - Model distilbert-base-uncased 2023-12-13_20-07-11 not saved
2023-12-13 20:35:38,338 - ====================================================
2023-12-13 20:35:38,579 - Model: distilbert-base-uncased 2023-12-13_20-35-38 with 7 epochs
2023-12-13 20:39:22,697 - Epoch 1 average train loss: 0.5856373226029031
2023-12-13 20:43:07,152 - Epoch 2 average train loss: 0.11176728720393252
2023-12-13 20:46:51,320 - Epoch 3 average train loss: 0.0543411510437727
2023-12-13 20:50:35,315 - Epoch 4 average train loss: 0.029037131971734412
2023-12-13 20:54:19,135 - Epoch 5 average train loss: 0.01697712146649685
2023-12-13 20:58:02,952 - Epoch 6 average train loss: 0.012472371393814683
2023-12-13 21:01:46,759 - Epoch 7 average train loss: 0.008380375832442524
2023-12-13 21:02:11,586 - 2023-12-13_20-35-38 with 7 epochs: Evaluation Results:
2023-12-13 21:02:11,587 - Training time: 1568.116630077362 seconds
2023-12-13 21:02:11,587 - Inference time: 24.81899905204773 seconds
2023-12-13 21:02:11,587 - Precision: 0.9672915061731784
2023-12-13 21:02:11,587 - Recall: 0.9674254127489746
2023-12-13 21:02:11,587 - F-score: 0.9673041002563775
2023-12-13 21:02:11,587 - Accuracy: 0.9670588235294117
2023-12-13 21:02:11,587 - G-mean: 0.9672421007718176
2023-12-13 21:02:39,654 - distilbert-base-uncased 2023-12-13_20-35-38 with 7 epochs: Evaluation Results (completely new data):
2023-12-13 21:02:39,655 - Training time: 1568.116630077362 seconds
2023-12-13 21:02:39,655 - Inference time: 24.69696831703186 seconds
2023-12-13 21:02:39,655 - Precision: 0.9702049941710141
2023-12-13 21:02:39,655 - Recall: 0.9715323759254865
2023-12-13 21:02:39,655 - F-score: 0.9705510734951559
2023-12-13 21:02:39,655 - Accuracy: 0.9705882352941176
2023-12-13 21:02:39,655 - G-mean: 0.9710601908638925
2023-12-13 21:02:39,694 - Model distilbert-base-uncased 2023-12-13_20-35-38 not saved
2023-12-13 21:02:39,694 - ====================================================
2023-12-13 21:02:39,933 - Model: distilbert-base-uncased 2023-12-13_21-02-39 with 7 epochs
2023-12-13 21:06:23,280 - Epoch 1 average train loss: 0.5895738822922987
2023-12-13 21:10:07,021 - Epoch 2 average train loss: 0.11045995850773419
2023-12-13 21:13:51,031 - Epoch 3 average train loss: 0.052342197957503445
2023-12-13 21:17:34,803 - Epoch 4 average train loss: 0.02859937261132633
2023-12-13 21:21:18,559 - Epoch 5 average train loss: 0.014686676095075468
2023-12-13 21:25:01,883 - Epoch 6 average train loss: 0.012698902750100173
2023-12-13 21:28:52,649 - Epoch 7 average train loss: 0.00929695164469783
2023-12-13 21:29:18,804 - 2023-12-13_21-02-39 with 7 epochs: Evaluation Results:
2023-12-13 21:29:18,804 - Training time: 1572.6532788276672 seconds
2023-12-13 21:29:18,804 - Inference time: 26.147284030914307 seconds
2023-12-13 21:29:18,804 - Precision: 0.9695599064755605
2023-12-13 21:29:18,804 - Recall: 0.9695943934476121
2023-12-13 21:29:18,804 - F-score: 0.9695227195574173
2023-12-13 21:29:18,804 - Accuracy: 0.9694117647058823
2023-12-13 21:29:18,804 - G-mean: 0.969503074776444
2023-12-13 21:29:47,561 - distilbert-base-uncased 2023-12-13_21-02-39 with 7 epochs: Evaluation Results (completely new data):
2023-12-13 21:29:47,561 - Training time: 1572.6532788276672 seconds
2023-12-13 21:29:47,561 - Inference time: 25.210999488830566 seconds
2023-12-13 21:29:47,561 - Precision: 0.9690343115290638
2023-12-13 21:29:47,561 - Recall: 0.9700503182945145
2023-12-13 21:29:47,561 - F-score: 0.9694378747968605
2023-12-13 21:29:47,561 - Accuracy: 0.9694117647058823
2023-12-13 21:29:47,561 - G-mean: 0.9697309889404319
2023-12-13 21:29:47,611 - Model distilbert-base-uncased 2023-12-13_21-02-39 not saved
2023-12-13 21:29:47,611 - ====================================================
2023-12-13 21:29:47,872 - Model: distilbert-base-uncased 2023-12-13_21-29-47 with 8 epochs
2023-12-13 21:33:41,740 - Epoch 1 average train loss: 0.5701154424951357
2023-12-13 21:37:43,254 - Epoch 2 average train loss: 0.10918362553505337
2023-12-13 21:41:44,852 - Epoch 3 average train loss: 0.05001829150571104
2023-12-13 21:45:46,870 - Epoch 4 average train loss: 0.027841881081900176
2023-12-13 21:49:43,344 - Epoch 5 average train loss: 0.016277077024090376
2023-12-13 21:53:38,676 - Epoch 6 average train loss: 0.008295619777499643
2023-12-13 21:57:23,814 - Epoch 7 average train loss: 0.008295876112670693
2023-12-13 22:01:16,755 - Epoch 8 average train loss: 0.003002980601073325
2023-12-13 22:01:42,116 - 2023-12-13_21-29-47 with 8 epochs: Evaluation Results:
2023-12-13 22:01:42,117 - Training time: 1888.814341545105 seconds
2023-12-13 22:01:42,117 - Inference time: 25.353996753692627 seconds
2023-12-13 22:01:42,117 - Precision: 0.9618628654278039
2023-12-13 22:01:42,117 - Recall: 0.9602321813352521
2023-12-13 22:01:42,117 - F-score: 0.960043670531169
2023-12-13 22:01:42,117 - Accuracy: 0.96
2023-12-13 22:01:42,117 - G-mean: 0.9601160836491814
2023-12-13 22:02:10,068 - distilbert-base-uncased 2023-12-13_21-29-47 with 8 epochs: Evaluation Results (completely new data):
2023-12-13 22:02:10,068 - Training time: 1888.814341545105 seconds
2023-12-13 22:02:10,068 - Inference time: 24.551000595092773 seconds
2023-12-13 22:02:10,068 - Precision: 0.9591278386379637
2023-12-13 22:02:10,068 - Recall: 0.9597469938002892
2023-12-13 22:02:10,069 - F-score: 0.9587892622361129
2023-12-13 22:02:10,069 - Accuracy: 0.9588235294117647
2023-12-13 22:02:10,069 - G-mean: 0.9592851504833817
2023-12-13 22:02:10,112 - Model distilbert-base-uncased 2023-12-13_21-29-47 not saved
2023-12-13 22:02:10,112 - ====================================================
2023-12-13 22:02:10,347 - Model: distilbert-base-uncased 2023-12-13_22-02-10 with 8 epochs
2023-12-13 22:06:09,149 - Epoch 1 average train loss: 0.6122429225900594
2023-12-13 22:10:07,306 - Epoch 2 average train loss: 0.11325723727617194
2023-12-13 22:14:01,302 - Epoch 3 average train loss: 0.05389070141074412
2023-12-13 22:17:55,197 - Epoch 4 average train loss: 0.026417070969610528
2023-12-13 22:21:47,439 - Epoch 5 average train loss: 0.012472098099807387
2023-12-13 22:25:38,838 - Epoch 6 average train loss: 0.009173496575950754
2023-12-13 22:29:35,680 - Epoch 7 average train loss: 0.005693267577478443
2023-12-13 22:33:31,097 - Epoch 8 average train loss: 0.009243187551551006
2023-12-13 22:33:56,901 - 2023-12-13_22-02-10 with 8 epochs: Evaluation Results:
2023-12-13 22:33:56,901 - Training time: 1880.6868426799774 seconds
2023-12-13 22:33:56,902 - Inference time: 25.79699945449829 seconds
2023-12-13 22:33:56,902 - Precision: 0.9734283006749648
2023-12-13 22:33:56,902 - Recall: 0.9729595502940562
2023-12-13 22:33:56,902 - F-score: 0.9730923991458951
2023-12-13 22:33:56,902 - Accuracy: 0.9729411764705882
2023-12-13 22:33:56,902 - G-mean: 0.9729503633389494
2023-12-13 22:34:26,987 - distilbert-base-uncased 2023-12-13_22-02-10 with 8 epochs: Evaluation Results (completely new data):
2023-12-13 22:34:26,988 - Training time: 1880.6868426799774 seconds
2023-12-13 22:34:26,988 - Inference time: 26.678972244262695 seconds
2023-12-13 22:34:26,988 - Precision: 0.9679659952990581
2023-12-13 22:34:26,988 - Recall: 0.9688276410658752
2023-12-13 22:34:26,988 - F-score: 0.9682687716447408
2023-12-13 22:34:26,988 - Accuracy: 0.9682352941176471
2023-12-13 22:34:26,988 - G-mean: 0.9685314223073631
2023-12-13 22:34:27,088 - Model distilbert-base-uncased 2023-12-13_22-02-10 not saved
2023-12-13 22:34:27,088 - ====================================================
2023-12-13 22:34:27,545 - Model: distilbert-base-uncased 2023-12-13_22-34-27 with 8 epochs
2023-12-13 22:38:25,664 - Epoch 1 average train loss: 0.5886057078926002
2023-12-13 22:42:24,635 - Epoch 2 average train loss: 0.10725117337177781
2023-12-13 22:46:31,326 - Epoch 3 average train loss: 0.05144950720207656
2023-12-13 22:50:31,408 - Epoch 4 average train loss: 0.028735910798149075
2023-12-13 22:54:21,853 - Epoch 5 average train loss: 0.018175438509925323
2023-12-13 22:58:23,583 - Epoch 6 average train loss: 0.01174820774632013
2023-12-13 23:02:24,586 - Epoch 7 average train loss: 0.008306360419065802
2023-12-13 23:06:33,717 - Epoch 8 average train loss: 0.007758893069318112
2023-12-13 23:07:02,190 - 2023-12-13_22-34-27 with 8 epochs: Evaluation Results:
2023-12-13 23:07:02,191 - Training time: 1925.9923739433289 seconds
2023-12-13 23:07:02,191 - Inference time: 28.46500039100647 seconds
2023-12-13 23:07:02,191 - Precision: 0.9660528149642399
2023-12-13 23:07:02,191 - Recall: 0.9641780393590901
2023-12-13 23:07:02,191 - F-score: 0.9649074097331811
2023-12-13 23:07:02,191 - Accuracy: 0.9647058823529412
2023-12-13 23:07:02,191 - G-mean: 0.9644419247446887
2023-12-13 23:07:34,148 - distilbert-base-uncased 2023-12-13_22-34-27 with 8 epochs: Evaluation Results (completely new data):
2023-12-13 23:07:34,148 - Training time: 1925.9923739433289 seconds
2023-12-13 23:07:34,148 - Inference time: 28.11300039291382 seconds
2023-12-13 23:07:34,148 - Precision: 0.9631709019294498
2023-12-13 23:07:34,148 - Recall: 0.9605712614137298
2023-12-13 23:07:34,148 - F-score: 0.9613433268110321
2023-12-13 23:07:34,148 - Accuracy: 0.9611764705882353
2023-12-13 23:07:34,148 - G-mean: 0.9608738183518885
2023-12-13 23:07:34,197 - Model distilbert-base-uncased 2023-12-13_22-34-27 not saved
2023-12-13 23:07:34,197 - Total program time: 19507.53178858757 seconds
