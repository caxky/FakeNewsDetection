2023-12-17 17:03:44,987 - ====================================================
2023-12-17 17:03:45,181 - Model: distilbert-base-uncased 2023-12-17_17-03-45 with 6 epochs
2023-12-17 17:07:33,768 - Epoch 1 average train loss: 0.600657
2023-12-17 17:11:27,726 - Epoch 2 average train loss: 0.119751
2023-12-17 17:15:24,538 - Epoch 3 average train loss: 0.052728
2023-12-17 17:19:20,665 - Epoch 4 average train loss: 0.030059
2023-12-17 17:23:17,628 - Epoch 5 average train loss: 0.009548
2023-12-17 17:27:14,290 - Epoch 6 average train loss: 0.008908
2023-12-17 17:27:40,670 - 2023-12-17_17-03-45 with 6 epochs: Evaluation Results:
2023-12-17 17:27:40,671 - Training time: 1409.0390005111694 seconds
2023-12-17 17:27:40,675 - Inference time: 26.372999668121338 seconds
2023-12-17 17:27:40,675 - Precision: 0.9609621536725422
2023-12-17 17:27:40,675 - Recall: 0.9579480491493563
2023-12-17 17:27:40,675 - F-score: 0.9589968234501679
2023-12-17 17:27:40,675 - Accuracy: 0.9588235294117647
2023-12-17 17:27:40,675 - G-mean: 0.9583856893122416
2023-12-17 17:28:10,260 - distilbert-base-uncased 2023-12-17_17-03-45 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 17:28:10,260 - Training time: 1409.0390005111694 seconds
2023-12-17 17:28:10,260 - Inference time: 26.03686785697937 seconds
2023-12-17 17:28:10,260 - Precision: 0.9614502461839292
2023-12-17 17:28:10,260 - Recall: 0.9567293068632174
2023-12-17 17:28:10,260 - F-score: 0.9580220982710579
2023-12-17 17:28:10,260 - Accuracy: 0.9576470588235294
2023-12-17 17:28:10,260 - G-mean: 0.957188072850803
2023-12-17 17:28:12,683 - ====================================================
2023-12-17 17:28:12,927 - Model: distilbert-base-uncased 2023-12-17_17-28-12 with 6 epochs
2023-12-17 17:32:08,265 - Epoch 1 average train loss: 0.578508
2023-12-17 17:36:04,392 - Epoch 2 average train loss: 0.109944
2023-12-17 17:40:00,503 - Epoch 3 average train loss: 0.057247
2023-12-17 17:43:55,864 - Epoch 4 average train loss: 0.029429
2023-12-17 17:48:04,290 - Epoch 5 average train loss: 0.011018
2023-12-17 17:52:25,053 - Epoch 6 average train loss: 0.006152
2023-12-17 17:52:53,945 - 2023-12-17_17-28-12 with 6 epochs: Evaluation Results:
2023-12-17 17:52:53,945 - Training time: 1452.0658564567566 seconds
2023-12-17 17:52:53,945 - Inference time: 28.883997917175293 seconds
2023-12-17 17:52:53,945 - Precision: 0.9688214643977611
2023-12-17 17:52:53,945 - Recall: 0.9681465957185139
2023-12-17 17:52:53,945 - F-score: 0.9684147525324928
2023-12-17 17:52:53,945 - Accuracy: 0.9682352941176471
2023-12-17 17:52:53,946 - G-mean: 0.9681909439023452
2023-12-17 17:53:24,446 - distilbert-base-uncased 2023-12-17_17-28-12 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 17:53:24,446 - Training time: 1452.0658564567566 seconds
2023-12-17 17:53:24,446 - Inference time: 27.049999952316284 seconds
2023-12-17 17:53:24,446 - Precision: 0.969174897313367
2023-12-17 17:53:24,446 - Recall: 0.9696420960165113
2023-12-17 17:53:24,446 - F-score: 0.9693632939004357
2023-12-17 17:53:24,446 - Accuracy: 0.9694117647058823
2023-12-17 17:53:24,446 - G-mean: 0.969526923521197
2023-12-17 17:53:26,383 - ====================================================
2023-12-17 17:53:26,637 - Model: distilbert-base-uncased 2023-12-17_17-53-26 with 6 epochs
2023-12-17 17:57:29,261 - Epoch 1 average train loss: 0.584322
2023-12-17 18:01:28,724 - Epoch 2 average train loss: 0.111575
2023-12-17 18:05:27,293 - Epoch 3 average train loss: 0.052667
2023-12-17 18:09:25,667 - Epoch 4 average train loss: 0.025093
2023-12-17 18:13:23,382 - Epoch 5 average train loss: 0.016884
2023-12-17 18:17:17,013 - Epoch 6 average train loss: 0.012073
2023-12-17 18:17:41,765 - 2023-12-17_17-53-26 with 6 epochs: Evaluation Results:
2023-12-17 18:17:41,765 - Training time: 1430.317518234253 seconds
2023-12-17 18:17:41,765 - Inference time: 24.74400019645691 seconds
2023-12-17 18:17:41,765 - Precision: 0.9657011268018122
2023-12-17 18:17:41,765 - Recall: 0.9665133691808505
2023-12-17 18:17:41,765 - F-score: 0.9660132221022797
2023-12-17 18:17:41,765 - Accuracy: 0.9658823529411765
2023-12-17 18:17:41,765 - G-mean: 0.9661978095470429
2023-12-17 18:18:09,816 - distilbert-base-uncased 2023-12-17_17-53-26 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 18:18:09,816 - Training time: 1430.317518234253 seconds
2023-12-17 18:18:09,816 - Inference time: 24.630966663360596 seconds
2023-12-17 18:18:09,816 - Precision: 0.9583674978867286
2023-12-17 18:18:09,816 - Recall: 0.9594925858279852
2023-12-17 18:18:09,816 - F-score: 0.9587540111294063
2023-12-17 18:18:09,816 - Accuracy: 0.9588235294117647
2023-12-17 18:18:09,817 - G-mean: 0.9591579992827092
2023-12-17 18:18:09,859 - Model distilbert-base-uncased 2023-12-17_17-53-26 not saved
2023-12-17 18:18:09,859 - ====================================================
2023-12-17 18:18:10,149 - Model: distilbert-base-uncased 2023-12-17_18-18-10 with 6 epochs
2023-12-17 18:21:58,416 - Epoch 1 average train loss: 0.589309
2023-12-17 18:25:54,649 - Epoch 2 average train loss: 0.104836
2023-12-17 18:29:50,857 - Epoch 3 average train loss: 0.050520
2023-12-17 18:33:46,904 - Epoch 4 average train loss: 0.031166
2023-12-17 18:37:42,757 - Epoch 5 average train loss: 0.022619
2023-12-17 18:41:52,136 - Epoch 6 average train loss: 0.009781
2023-12-17 18:42:18,468 - 2023-12-17_18-18-10 with 6 epochs: Evaluation Results:
2023-12-17 18:42:18,468 - Training time: 1421.9240481853485 seconds
2023-12-17 18:42:18,468 - Inference time: 26.322964906692505 seconds
2023-12-17 18:42:18,468 - Precision: 0.970084428221653
2023-12-17 18:42:18,468 - Recall: 0.9696286533494861
2023-12-17 18:42:18,468 - F-score: 0.9696655780591763
2023-12-17 18:42:18,468 - Accuracy: 0.9694117647058823
2023-12-17 18:42:18,468 - G-mean: 0.9695202029627404
2023-12-17 18:42:47,833 - distilbert-base-uncased 2023-12-17_18-18-10 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 18:42:47,833 - Training time: 1421.9240481853485 seconds
2023-12-17 18:42:47,833 - Inference time: 25.908002853393555 seconds
2023-12-17 18:42:47,833 - Precision: 0.9621132932776453
2023-12-17 18:42:47,833 - Recall: 0.9628870301462742
2023-12-17 18:42:47,834 - F-score: 0.9622120913559302
2023-12-17 18:42:47,834 - Accuracy: 0.9623529411764706
2023-12-17 18:42:47,834 - G-mean: 0.9626199486204012
2023-12-17 18:42:47,885 - Model distilbert-base-uncased 2023-12-17_18-18-10 not saved
2023-12-17 18:42:47,885 - ====================================================
2023-12-17 18:42:48,158 - Model: distilbert-base-uncased 2023-12-17_18-42-48 with 6 epochs
2023-12-17 18:46:59,114 - Epoch 1 average train loss: 0.591335
2023-12-17 18:51:23,482 - Epoch 2 average train loss: 0.112174
2023-12-17 18:56:13,354 - Epoch 3 average train loss: 0.051954
2023-12-17 19:00:27,939 - Epoch 4 average train loss: 0.024320
2023-12-17 19:04:33,052 - Epoch 5 average train loss: 0.015777
2023-12-17 19:08:40,638 - Epoch 6 average train loss: 0.008097
2023-12-17 19:09:07,337 - 2023-12-17_18-42-48 with 6 epochs: Evaluation Results:
2023-12-17 19:09:07,337 - Training time: 1552.415958404541 seconds
2023-12-17 19:09:07,337 - Inference time: 26.6910343170166 seconds
2023-12-17 19:09:07,337 - Precision: 0.9676633736685918
2023-12-17 19:09:07,337 - Recall: 0.9670180649227132
2023-12-17 19:09:07,337 - F-score: 0.9670859536647569
2023-12-17 19:09:07,337 - Accuracy: 0.9670588235294117
2023-12-17 19:09:07,337 - G-mean: 0.9670384440113264
2023-12-17 19:09:37,364 - distilbert-base-uncased 2023-12-17_18-42-48 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 19:09:37,364 - Training time: 1552.415958404541 seconds
2023-12-17 19:09:37,364 - Inference time: 26.220030069351196 seconds
2023-12-17 19:09:37,365 - Precision: 0.9680854268253254
2023-12-17 19:09:37,365 - Recall: 0.9685725772999337
2023-12-17 19:09:37,365 - F-score: 0.9682966921799462
2023-12-17 19:09:37,365 - Accuracy: 0.9682352941176471
2023-12-17 19:09:37,365 - G-mean: 0.9684039210248422
2023-12-17 19:09:37,573 - Model distilbert-base-uncased 2023-12-17_18-42-48 not saved
2023-12-17 19:09:37,574 - ====================================================
2023-12-17 19:09:37,808 - Model: distilbert-base-uncased 2023-12-17_19-09-37 with 6 epochs
2023-12-17 19:13:36,814 - Epoch 1 average train loss: 0.592566
2023-12-17 19:17:47,027 - Epoch 2 average train loss: 0.107859
2023-12-17 19:21:52,857 - Epoch 3 average train loss: 0.048718
2023-12-17 19:26:10,048 - Epoch 4 average train loss: 0.028180
2023-12-17 19:30:14,386 - Epoch 5 average train loss: 0.015123
2023-12-17 19:34:20,941 - Epoch 6 average train loss: 0.013198
2023-12-17 19:34:47,742 - 2023-12-17_19-09-37 with 6 epochs: Evaluation Results:
2023-12-17 19:34:47,742 - Training time: 1483.0626776218414 seconds
2023-12-17 19:34:47,742 - Inference time: 26.79200053215027 seconds
2023-12-17 19:34:47,742 - Precision: 0.967568074552753
2023-12-17 19:34:47,742 - Recall: 0.9669838050208395
2023-12-17 19:34:47,742 - F-score: 0.9671902776334192
2023-12-17 19:34:47,742 - Accuracy: 0.9670588235294117
2023-12-17 19:34:47,742 - G-mean: 0.9670213135476627
2023-12-17 19:35:17,963 - distilbert-base-uncased 2023-12-17_19-09-37 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 19:35:17,963 - Training time: 1483.0626776218414 seconds
2023-12-17 19:35:17,963 - Inference time: 26.50299835205078 seconds
2023-12-17 19:35:17,963 - Precision: 0.9737462127933766
2023-12-17 19:35:17,963 - Recall: 0.9746433434577308
2023-12-17 19:35:17,963 - F-score: 0.9740594522207303
2023-12-17 19:35:17,964 - Accuracy: 0.9741176470588235
2023-12-17 19:35:17,964 - G-mean: 0.9743804598054036
2023-12-17 19:35:19,889 - ====================================================
2023-12-17 19:35:20,141 - Model: distilbert-base-uncased 2023-12-17_19-35-20 with 6 epochs
2023-12-17 19:39:24,147 - Epoch 1 average train loss: 0.594195
2023-12-17 19:43:25,355 - Epoch 2 average train loss: 0.116568
2023-12-17 19:47:28,699 - Epoch 3 average train loss: 0.056912
2023-12-17 19:51:30,697 - Epoch 4 average train loss: 0.031653
2023-12-17 19:55:39,891 - Epoch 5 average train loss: 0.020976
2023-12-17 20:00:16,508 - Epoch 6 average train loss: 0.021361
2023-12-17 20:00:43,157 - 2023-12-17_19-35-20 with 6 epochs: Evaluation Results:
2023-12-17 20:00:43,158 - Training time: 1496.310940027237 seconds
2023-12-17 20:00:43,158 - Inference time: 26.64199161529541 seconds
2023-12-17 20:00:43,158 - Precision: 0.9621358740672257
2023-12-17 20:00:43,158 - Recall: 0.9609737998457488
2023-12-17 20:00:43,158 - F-score: 0.9614795535237788
2023-12-17 20:00:43,158 - Accuracy: 0.9611764705882353
2023-12-17 20:00:43,158 - G-mean: 0.961075129874612
2023-12-17 20:01:12,842 - distilbert-base-uncased 2023-12-17_19-35-20 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 20:01:12,842 - Training time: 1496.310940027237 seconds
2023-12-17 20:01:12,842 - Inference time: 26.18533992767334 seconds
2023-12-17 20:01:12,842 - Precision: 0.9740233135206878
2023-12-17 20:01:12,842 - Recall: 0.9745791402903745
2023-12-17 20:01:12,842 - F-score: 0.9742253850830924
2023-12-17 20:01:12,842 - Accuracy: 0.9741176470588235
2023-12-17 20:01:12,843 - G-mean: 0.9743483663517226
2023-12-17 20:01:14,919 - ====================================================
2023-12-17 20:01:15,184 - Model: distilbert-base-uncased 2023-12-17_20-01-15 with 6 epochs
2023-12-17 20:05:44,035 - Epoch 1 average train loss: 0.624322
2023-12-17 20:10:14,647 - Epoch 2 average train loss: 0.113943
2023-12-17 20:14:29,811 - Epoch 3 average train loss: 0.057410
2023-12-17 20:18:38,371 - Epoch 4 average train loss: 0.030794
2023-12-17 20:22:29,520 - Epoch 5 average train loss: 0.016677
2023-12-17 20:26:20,429 - Epoch 6 average train loss: 0.007580
2023-12-17 20:26:46,089 - 2023-12-17_20-01-15 with 6 epochs: Evaluation Results:
2023-12-17 20:26:46,089 - Training time: 1505.1830582618713 seconds
2023-12-17 20:26:46,089 - Inference time: 25.652981281280518 seconds
2023-12-17 20:26:46,089 - Precision: 0.9684966149469192
2023-12-17 20:26:46,089 - Recall: 0.9648614646197883
2023-12-17 20:26:46,090 - F-score: 0.9660181328473737
2023-12-17 20:26:46,090 - Accuracy: 0.9658823529411765
2023-12-17 20:26:46,090 - G-mean: 0.965371773830803
2023-12-17 20:27:14,803 - distilbert-base-uncased 2023-12-17_20-01-15 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 20:27:14,803 - Training time: 1505.1830582618713 seconds
2023-12-17 20:27:14,803 - Inference time: 25.322999715805054 seconds
2023-12-17 20:27:14,803 - Precision: 0.9643613855554246
2023-12-17 20:27:14,803 - Recall: 0.9604111629100514
2023-12-17 20:27:14,804 - F-score: 0.9615811408827483
2023-12-17 20:27:14,804 - Accuracy: 0.9611764705882353
2023-12-17 20:27:14,804 - G-mean: 0.9607937405496696
2023-12-17 20:27:14,857 - Model distilbert-base-uncased 2023-12-17_20-01-15 not saved
2023-12-17 20:27:14,857 - ====================================================
2023-12-17 20:27:15,113 - Model: distilbert-base-uncased 2023-12-17_20-27-15 with 6 epochs
2023-12-17 20:31:05,232 - Epoch 1 average train loss: 0.578432
2023-12-17 20:35:01,330 - Epoch 2 average train loss: 0.120752
2023-12-17 20:39:36,410 - Epoch 3 average train loss: 0.060134
2023-12-17 20:44:30,807 - Epoch 4 average train loss: 0.037003
2023-12-17 20:51:06,035 - Epoch 5 average train loss: 0.021741
2023-12-17 20:55:39,378 - Epoch 6 average train loss: 0.011687
2023-12-17 20:56:09,573 - 2023-12-17_20-27-15 with 6 epochs: Evaluation Results:
2023-12-17 20:56:09,573 - Training time: 1704.2069857120514 seconds
2023-12-17 20:56:09,573 - Inference time: 30.186999320983887 seconds
2023-12-17 20:56:09,573 - Precision: 0.9687719173628292
2023-12-17 20:56:09,573 - Recall: 0.9683374563170991
2023-12-17 20:56:09,573 - F-score: 0.9684206935864081
2023-12-17 20:56:09,573 - Accuracy: 0.9682352941176471
2023-12-17 20:56:09,573 - G-mean: 0.9682863738700038
2023-12-17 20:56:39,734 - distilbert-base-uncased 2023-12-17_20-27-15 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 20:56:39,734 - Training time: 1704.2069857120514 seconds
2023-12-17 20:56:39,734 - Inference time: 26.63400411605835 seconds
2023-12-17 20:56:39,734 - Precision: 0.9724217742880581
2023-12-17 20:56:39,734 - Recall: 0.9734163495927003
2023-12-17 20:56:39,734 - F-score: 0.9727972992740478
2023-12-17 20:56:39,734 - Accuracy: 0.9729411764705882
2023-12-17 20:56:39,734 - G-mean: 0.9731787340300996
2023-12-17 20:56:39,784 - Model distilbert-base-uncased 2023-12-17_20-27-15 not saved
2023-12-17 20:56:39,784 - ====================================================
2023-12-17 20:56:40,119 - Model: distilbert-base-uncased 2023-12-17_20-56-40 with 6 epochs
2023-12-17 21:02:51,956 - Epoch 1 average train loss: 0.587009
2023-12-17 21:15:03,792 - Epoch 2 average train loss: 0.114890
2023-12-17 21:35:52,901 - Epoch 3 average train loss: 0.058776
2023-12-17 21:52:28,666 - Epoch 4 average train loss: 0.034578
2023-12-17 22:02:12,940 - Epoch 5 average train loss: 0.012452
2023-12-17 22:10:52,402 - Epoch 6 average train loss: 0.011840
2023-12-17 22:12:42,733 - 2023-12-17_20-56-40 with 6 epochs: Evaluation Results:
2023-12-17 22:12:42,733 - Training time: 4452.216361284256 seconds
2023-12-17 22:12:42,733 - Inference time: 110.32311344146729 seconds
2023-12-17 22:12:42,733 - Precision: 0.9649435739751097
2023-12-17 22:12:42,733 - Recall: 0.9644673630269054
2023-12-17 22:12:42,733 - F-score: 0.964611411665594
2023-12-17 22:12:42,733 - Accuracy: 0.9647058823529412
2023-12-17 22:12:42,734 - G-mean: 0.9645866153174039
2023-12-17 22:14:30,753 - distilbert-base-uncased 2023-12-17_20-56-40 with 6 epochs: Evaluation Results (completely new data):
2023-12-17 22:14:30,753 - Training time: 4452.216361284256 seconds
2023-12-17 22:14:30,753 - Inference time: 104.35099697113037 seconds
2023-12-17 22:14:30,753 - Precision: 0.9643818972369331
2023-12-17 22:14:30,754 - Recall: 0.9651774771880074
2023-12-17 22:14:30,754 - F-score: 0.9646991811492814
2023-12-17 22:14:30,754 - Accuracy: 0.9647058823529412
2023-12-17 22:14:30,754 - G-mean: 0.9649416509602239
2023-12-17 22:14:30,932 - Model distilbert-base-uncased 2023-12-17_20-56-40 not saved
2023-12-17 22:14:30,935 - ====================================================
2023-12-17 22:14:31,238 - Model: distilbert-base-uncased 2023-12-17_22-14-31 with 7 epochs
2023-12-17 22:18:37,336 - Epoch 1 average train loss: 0.577396
2023-12-17 22:22:50,838 - Epoch 2 average train loss: 0.108685
2023-12-17 22:27:08,928 - Epoch 3 average train loss: 0.052785
2023-12-17 22:31:19,118 - Epoch 4 average train loss: 0.031281
2023-12-17 22:35:40,274 - Epoch 5 average train loss: 0.016480
2023-12-17 22:40:34,378 - Epoch 6 average train loss: 0.011568
2023-12-17 22:45:22,754 - Epoch 7 average train loss: 0.015378
2023-12-17 22:45:55,687 - 2023-12-17_22-14-31 with 7 epochs: Evaluation Results:
2023-12-17 22:45:55,687 - Training time: 1851.4557585716248 seconds
2023-12-17 22:45:55,687 - Inference time: 32.9239981174469 seconds
2023-12-17 22:45:55,687 - Precision: 0.9699929352011207
2023-12-17 22:45:55,687 - Recall: 0.9693727151318028
2023-12-17 22:45:55,687 - F-score: 0.9696019480313616
2023-12-17 22:45:55,687 - Accuracy: 0.9694117647058823
2023-12-17 22:45:55,687 - G-mean: 0.9693922397222156
2023-12-17 22:46:30,186 - distilbert-base-uncased 2023-12-17_22-14-31 with 7 epochs: Evaluation Results (completely new data):
2023-12-17 22:46:30,186 - Training time: 1851.4557585716248 seconds
2023-12-17 22:46:30,186 - Inference time: 30.663999795913696 seconds
2023-12-17 22:46:30,186 - Precision: 0.9670892551188356
2023-12-17 22:46:30,186 - Recall: 0.9674371065125165
2023-12-17 22:46:30,186 - F-score: 0.9670968596773102
2023-12-17 22:46:30,186 - Accuracy: 0.9670588235294117
2023-12-17 22:46:30,186 - G-mean: 0.9672479465280308
2023-12-17 22:46:32,356 - ====================================================
2023-12-17 22:46:32,694 - Model: distilbert-base-uncased 2023-12-17_22-46-32 with 7 epochs
2023-12-17 22:58:43,259 - Epoch 1 average train loss: 0.591221
2023-12-17 23:10:55,853 - Epoch 2 average train loss: 0.108856
2023-12-17 23:22:48,258 - Epoch 3 average train loss: 0.051197
2023-12-17 23:33:44,539 - Epoch 4 average train loss: 0.024237
2023-12-17 23:46:47,013 - Epoch 5 average train loss: 0.020148
2023-12-18 00:07:34,584 - Epoch 6 average train loss: 0.012823
2023-12-18 00:22:06,454 - Epoch 7 average train loss: 0.007368
2023-12-18 00:22:35,095 - 2023-12-17_22-46-32 with 7 epochs: Evaluation Results:
2023-12-18 00:22:35,095 - Training time: 5733.696055173874 seconds
2023-12-18 00:22:35,095 - Inference time: 28.63299870491028 seconds
2023-12-18 00:22:35,095 - Precision: 0.9649345937128692
2023-12-18 00:22:35,095 - Recall: 0.9648438931359425
2023-12-18 00:22:35,095 - F-score: 0.9648610479181965
2023-12-18 00:22:35,095 - Accuracy: 0.9647058823529412
2023-12-18 00:22:35,095 - G-mean: 0.9647748852766412
2023-12-18 00:23:06,662 - distilbert-base-uncased 2023-12-17_22-46-32 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 00:23:06,662 - Training time: 5733.696055173874 seconds
2023-12-18 00:23:06,662 - Inference time: 28.1709988117218 seconds
2023-12-18 00:23:06,662 - Precision: 0.9694842220912365
2023-12-18 00:23:06,662 - Recall: 0.9699219119598019
2023-12-18 00:23:06,662 - F-score: 0.9696417630843011
2023-12-18 00:23:06,662 - Accuracy: 0.9694117647058823
2023-12-18 00:23:06,662 - G-mean: 0.9696668047839191
2023-12-18 00:23:08,609 - ====================================================
2023-12-18 00:23:08,855 - Model: distilbert-base-uncased 2023-12-18_00-23-08 with 7 epochs
2023-12-18 00:29:03,135 - Epoch 1 average train loss: 0.580196
2023-12-18 00:34:57,601 - Epoch 2 average train loss: 0.114356
2023-12-18 00:40:51,774 - Epoch 3 average train loss: 0.053048
2023-12-18 00:46:49,709 - Epoch 4 average train loss: 0.032330
2023-12-18 00:54:09,917 - Epoch 5 average train loss: 0.018160
2023-12-18 00:58:05,879 - Epoch 6 average train loss: 0.007247
2023-12-18 01:02:03,926 - Epoch 7 average train loss: 0.006587
2023-12-18 01:02:29,001 - 2023-12-18_00-23-08 with 7 epochs: Evaluation Results:
2023-12-18 01:02:29,001 - Training time: 2334.9960663318634 seconds
2023-12-18 01:02:29,001 - Inference time: 25.067001581192017 seconds
2023-12-18 01:02:29,001 - Precision: 0.9644419389412382
2023-12-18 01:02:29,001 - Recall: 0.9634893673879412
2023-12-18 01:02:29,001 - F-score: 0.9638967734528692
2023-12-18 01:02:29,001 - Accuracy: 0.9635294117647059
2023-12-18 01:02:29,001 - G-mean: 0.9635093893682881
2023-12-18 01:02:57,304 - distilbert-base-uncased 2023-12-18_00-23-08 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 01:02:57,304 - Training time: 2334.9960663318634 seconds
2023-12-18 01:02:57,304 - Inference time: 24.85397243499756 seconds
2023-12-18 01:02:57,304 - Precision: 0.9757492014499892
2023-12-18 01:02:57,304 - Recall: 0.975485118318624
2023-12-18 01:02:57,304 - F-score: 0.975539796464757
2023-12-18 01:02:57,304 - Accuracy: 0.9752941176470589
2023-12-18 01:02:57,304 - G-mean: 0.9753896133076255
2023-12-18 01:02:59,149 - ====================================================
2023-12-18 01:02:59,378 - Model: distilbert-base-uncased 2023-12-18_01-02-59 with 7 epochs
2023-12-18 01:06:50,530 - Epoch 1 average train loss: 0.591405
2023-12-18 01:10:42,089 - Epoch 2 average train loss: 0.117080
2023-12-18 01:14:33,241 - Epoch 3 average train loss: 0.053740
2023-12-18 01:18:24,261 - Epoch 4 average train loss: 0.033745
2023-12-18 01:22:15,471 - Epoch 5 average train loss: 0.017959
2023-12-18 01:26:06,885 - Epoch 6 average train loss: 0.010176
2023-12-18 01:29:58,379 - Epoch 7 average train loss: 0.010226
2023-12-18 01:30:23,225 - 2023-12-18_01-02-59 with 7 epochs: Evaluation Results:
2023-12-18 01:30:23,225 - Training time: 1618.9469211101532 seconds
2023-12-18 01:30:23,225 - Inference time: 24.837990045547485 seconds
2023-12-18 01:30:23,225 - Precision: 0.9662719728221141
2023-12-18 01:30:23,225 - Recall: 0.9659494206578776
2023-12-18 01:30:23,225 - F-score: 0.9658764720104337
2023-12-18 01:30:23,225 - Accuracy: 0.9658823529411765
2023-12-18 01:30:23,225 - G-mean: 0.9659158862174269
2023-12-18 01:30:51,187 - distilbert-base-uncased 2023-12-18_01-02-59 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 01:30:51,187 - Training time: 1618.9469211101532 seconds
2023-12-18 01:30:51,187 - Inference time: 24.660999536514282 seconds
2023-12-18 01:30:51,187 - Precision: 0.9690933550670897
2023-12-18 01:30:51,187 - Recall: 0.9699570463134176
2023-12-18 01:30:51,187 - F-score: 0.9694342418798788
2023-12-18 01:30:51,187 - Accuracy: 0.9694117647058823
2023-12-18 01:30:51,187 - G-mean: 0.9696843671811954
2023-12-18 01:30:51,275 - Model distilbert-base-uncased 2023-12-18_01-02-59 not saved
2023-12-18 01:30:51,275 - ====================================================
2023-12-18 01:30:51,524 - Model: distilbert-base-uncased 2023-12-18_01-30-51 with 7 epochs
2023-12-18 01:34:42,983 - Epoch 1 average train loss: 0.604337
2023-12-18 01:38:35,058 - Epoch 2 average train loss: 0.105344
2023-12-18 01:42:26,660 - Epoch 3 average train loss: 0.053327
2023-12-18 01:46:18,344 - Epoch 4 average train loss: 0.031653
2023-12-18 01:50:10,351 - Epoch 5 average train loss: 0.020527
2023-12-18 01:54:02,654 - Epoch 6 average train loss: 0.011703
2023-12-18 01:57:54,480 - Epoch 7 average train loss: 0.007041
2023-12-18 01:58:20,249 - 2023-12-18_01-30-51 with 7 epochs: Evaluation Results:
2023-12-18 01:58:20,250 - Training time: 1622.8905501365662 seconds
2023-12-18 01:58:20,250 - Inference time: 25.762001037597656 seconds
2023-12-18 01:58:20,250 - Precision: 0.9701119547426025
2023-12-18 01:58:20,250 - Recall: 0.9693436463180621
2023-12-18 01:58:20,250 - F-score: 0.9695784172303986
2023-12-18 01:58:20,250 - Accuracy: 0.9694117647058823
2023-12-18 01:58:20,250 - G-mean: 0.9693777049136354
2023-12-18 01:58:49,258 - distilbert-base-uncased 2023-12-18_01-30-51 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 01:58:49,259 - Training time: 1622.8905501365662 seconds
2023-12-18 01:58:49,259 - Inference time: 25.681971311569214 seconds
2023-12-18 01:58:49,259 - Precision: 0.9668056742605229
2023-12-18 01:58:49,259 - Recall: 0.9674097866022594
2023-12-18 01:58:49,259 - F-score: 0.9670662712363629
2023-12-18 01:58:49,259 - Accuracy: 0.9670588235294117
2023-12-18 01:58:49,259 - G-mean: 0.9672342891473711
2023-12-18 01:58:49,301 - Model distilbert-base-uncased 2023-12-18_01-30-51 not saved
2023-12-18 01:58:49,301 - ====================================================
2023-12-18 01:58:49,569 - Model: distilbert-base-uncased 2023-12-18_01-58-49 with 7 epochs
2023-12-18 02:02:40,853 - Epoch 1 average train loss: 0.585312
2023-12-18 02:06:33,005 - Epoch 2 average train loss: 0.115037
2023-12-18 02:10:24,996 - Epoch 3 average train loss: 0.049014
2023-12-18 02:14:17,054 - Epoch 4 average train loss: 0.025438
2023-12-18 02:18:09,626 - Epoch 5 average train loss: 0.015651
2023-12-18 02:22:02,827 - Epoch 6 average train loss: 0.008285
2023-12-18 02:25:56,288 - Epoch 7 average train loss: 0.006369
2023-12-18 02:26:22,230 - 2023-12-18_01-58-49 with 7 epochs: Evaluation Results:
2023-12-18 02:26:22,230 - Training time: 1626.6630647182465 seconds
2023-12-18 02:26:22,230 - Inference time: 25.935014009475708 seconds
2023-12-18 02:26:22,231 - Precision: 0.9723443377843164
2023-12-18 02:26:22,231 - Recall: 0.9717334308807672
2023-12-18 02:26:22,231 - F-score: 0.9719041005277098
2023-12-18 02:26:22,231 - Accuracy: 0.971764705882353
2023-12-18 02:26:22,231 - G-mean: 0.9717490682557399
2023-12-18 02:26:51,185 - distilbert-base-uncased 2023-12-18_01-58-49 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 02:26:51,185 - Training time: 1626.6630647182465 seconds
2023-12-18 02:26:51,185 - Inference time: 25.57196593284607 seconds
2023-12-18 02:26:51,185 - Precision: 0.9644045529205523
2023-12-18 02:26:51,185 - Recall: 0.9651484083742666
2023-12-18 02:26:51,185 - F-score: 0.9645714021652404
2023-12-18 02:26:51,185 - Accuracy: 0.9647058823529412
2023-12-18 02:26:51,185 - G-mean: 0.9649271199952014
2023-12-18 02:26:51,223 - Model distilbert-base-uncased 2023-12-18_01-58-49 not saved
2023-12-18 02:26:51,224 - ====================================================
2023-12-18 02:26:51,493 - Model: distilbert-base-uncased 2023-12-18_02-26-51 with 7 epochs
2023-12-18 02:30:44,442 - Epoch 1 average train loss: 0.583568
2023-12-18 02:34:38,224 - Epoch 2 average train loss: 0.115681
2023-12-18 02:38:32,694 - Epoch 3 average train loss: 0.058498
2023-12-18 02:42:26,562 - Epoch 4 average train loss: 0.031145
2023-12-18 02:46:20,151 - Epoch 5 average train loss: 0.019177
2023-12-18 02:50:14,285 - Epoch 6 average train loss: 0.012040
2023-12-18 02:54:08,638 - Epoch 7 average train loss: 0.006337
2023-12-18 02:54:34,536 - 2023-12-18_02-26-51 with 7 epochs: Evaluation Results:
2023-12-18 02:54:34,536 - Training time: 1637.0887446403503 seconds
2023-12-18 02:54:34,537 - Inference time: 25.889999628067017 seconds
2023-12-18 02:54:34,537 - Precision: 0.9661140523565068
2023-12-18 02:54:34,537 - Recall: 0.9661702245219452
2023-12-18 02:54:34,537 - F-score: 0.9658856475327386
2023-12-18 02:54:34,537 - Accuracy: 0.9658823529411765
2023-12-18 02:54:34,537 - G-mean: 0.9660262780085028
2023-12-18 02:55:03,505 - distilbert-base-uncased 2023-12-18_02-26-51 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 02:55:03,505 - Training time: 1637.0887446403503 seconds
2023-12-18 02:55:03,505 - Inference time: 25.59699845314026 seconds
2023-12-18 02:55:03,505 - Precision: 0.9620291537908564
2023-12-18 02:55:03,505 - Recall: 0.9628870301462742
2023-12-18 02:55:03,505 - F-score: 0.9620415180479078
2023-12-18 02:55:03,505 - Accuracy: 0.9623529411764706
2023-12-18 02:55:03,505 - G-mean: 0.9626199486204012
2023-12-18 02:55:03,541 - Model distilbert-base-uncased 2023-12-18_02-26-51 not saved
2023-12-18 02:55:03,542 - ====================================================
2023-12-18 02:55:03,830 - Model: distilbert-base-uncased 2023-12-18_02-55-03 with 7 epochs
2023-12-18 02:58:57,493 - Epoch 1 average train loss: 0.586392
2023-12-18 03:02:52,104 - Epoch 2 average train loss: 0.112914
2023-12-18 03:06:46,683 - Epoch 3 average train loss: 0.046148
2023-12-18 03:10:41,023 - Epoch 4 average train loss: 0.029149
2023-12-18 03:14:35,795 - Epoch 5 average train loss: 0.015464
2023-12-18 03:18:30,362 - Epoch 6 average train loss: 0.008300
2023-12-18 03:22:25,269 - Epoch 7 average train loss: 0.005228
2023-12-18 03:22:51,302 - 2023-12-18_02-55-03 with 7 epochs: Evaluation Results:
2023-12-18 03:22:51,302 - Training time: 1641.3808283805847 seconds
2023-12-18 03:22:51,302 - Inference time: 26.025996923446655 seconds
2023-12-18 03:22:51,302 - Precision: 0.9674217777869378
2023-12-18 03:22:51,303 - Recall: 0.9673612095816185
2023-12-18 03:22:51,303 - F-score: 0.9673532349865974
2023-12-18 03:22:51,303 - Accuracy: 0.9670588235294117
2023-12-18 03:22:51,303 - G-mean: 0.9672100047383654
2023-12-18 03:23:20,298 - distilbert-base-uncased 2023-12-18_02-55-03 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 03:23:20,298 - Training time: 1641.3808283805847 seconds
2023-12-18 03:23:20,298 - Inference time: 25.636971473693848 seconds
2023-12-18 03:23:20,298 - Precision: 0.9695669970844708
2023-12-18 03:23:20,298 - Recall: 0.9697002336439926
2023-12-18 03:23:20,298 - F-score: 0.9695405544568793
2023-12-18 03:23:20,298 - Accuracy: 0.9694117647058823
2023-12-18 03:23:20,298 - G-mean: 0.9695559884465308
2023-12-18 03:23:20,336 - Model distilbert-base-uncased 2023-12-18_02-55-03 not saved
2023-12-18 03:23:20,337 - ====================================================
2023-12-18 03:23:20,582 - Model: distilbert-base-uncased 2023-12-18_03-23-20 with 7 epochs
2023-12-18 03:27:15,265 - Epoch 1 average train loss: 0.591176
2023-12-18 03:31:10,424 - Epoch 2 average train loss: 0.115501
2023-12-18 03:35:05,992 - Epoch 3 average train loss: 0.058021
2023-12-18 03:39:01,316 - Epoch 4 average train loss: 0.032716
2023-12-18 03:42:56,715 - Epoch 5 average train loss: 0.017754
2023-12-18 03:46:52,239 - Epoch 6 average train loss: 0.009132
2023-12-18 03:50:48,010 - Epoch 7 average train loss: 0.006704
2023-12-18 03:51:14,084 - 2023-12-18_03-23-20 with 7 epochs: Evaluation Results:
2023-12-18 03:51:14,084 - Training time: 1647.369967699051 seconds
2023-12-18 03:51:14,084 - Inference time: 26.065988063812256 seconds
2023-12-18 03:51:14,084 - Precision: 0.9577008765503221
2023-12-18 03:51:14,084 - Recall: 0.9560059378546615
2023-12-18 03:51:14,084 - F-score: 0.9564781073977503
2023-12-18 03:51:14,084 - Accuracy: 0.9564705882352941
2023-12-18 03:51:14,084 - G-mean: 0.9562382348224119
2023-12-18 03:51:43,291 - distilbert-base-uncased 2023-12-18_03-23-20 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 03:51:43,291 - Training time: 1647.369967699051 seconds
2023-12-18 03:51:43,291 - Inference time: 25.795000314712524 seconds
2023-12-18 03:51:43,291 - Precision: 0.9658900656781277
2023-12-18 03:51:43,291 - Recall: 0.9661185895698724
2023-12-18 03:51:43,291 - F-score: 0.9659482194232061
2023-12-18 03:51:43,291 - Accuracy: 0.9658823529411765
2023-12-18 03:51:43,291 - G-mean: 0.966000464034029
2023-12-18 03:51:43,327 - Model distilbert-base-uncased 2023-12-18_03-23-20 not saved
2023-12-18 03:51:43,327 - ====================================================
2023-12-18 03:51:43,596 - Model: distilbert-base-uncased 2023-12-18_03-51-43 with 7 epochs
2023-12-18 03:55:38,811 - Epoch 1 average train loss: 0.581732
2023-12-18 03:59:34,333 - Epoch 2 average train loss: 0.102900
2023-12-18 04:03:29,974 - Epoch 3 average train loss: 0.053432
2023-12-18 04:07:25,622 - Epoch 4 average train loss: 0.027467
2023-12-18 04:11:21,579 - Epoch 5 average train loss: 0.016818
2023-12-18 04:15:17,371 - Epoch 6 average train loss: 0.012861
2023-12-18 04:19:13,192 - Epoch 7 average train loss: 0.009547
2023-12-18 04:19:39,284 - 2023-12-18_03-51-43 with 7 epochs: Evaluation Results:
2023-12-18 04:19:39,284 - Training time: 1649.539166212082 seconds
2023-12-18 04:19:39,284 - Inference time: 26.084999084472656 seconds
2023-12-18 04:19:39,284 - Precision: 0.9697596101102226
2023-12-18 04:19:39,285 - Recall: 0.969436918299159
2023-12-18 04:19:39,285 - F-score: 0.9695473484354924
2023-12-18 04:19:39,285 - Accuracy: 0.9694117647058823
2023-12-18 04:19:39,285 - G-mean: 0.9694243414209384
2023-12-18 04:20:08,580 - distilbert-base-uncased 2023-12-18_03-51-43 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 04:20:08,580 - Training time: 1649.539166212082 seconds
2023-12-18 04:20:08,580 - Inference time: 25.826998472213745 seconds
2023-12-18 04:20:08,580 - Precision: 0.9682057802032151
2023-12-18 04:20:08,580 - Recall: 0.9687211476351839
2023-12-18 04:20:08,580 - F-score: 0.9683696781054959
2023-12-18 04:20:08,580 - Accuracy: 0.9682352941176471
2023-12-18 04:20:08,580 - G-mean: 0.9684781904093334
2023-12-18 04:20:08,616 - Model distilbert-base-uncased 2023-12-18_03-51-43 not saved
2023-12-18 04:20:08,617 - Total program time: 40597.35263299942 seconds
