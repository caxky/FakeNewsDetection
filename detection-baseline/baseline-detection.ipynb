{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: torchvision in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (0.16.1+cu121)\n",
      "Requirement already satisfied: torchaudio in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: numpy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: requests in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (4.36.0)\n",
      "Requirement already satisfied: pandas in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pyarrow in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (14.0.1)\n",
      "Requirement already satisfied: accelerate in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: ipywidgets in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (8.1.1)\n",
      "Requirement already satisfied: tqdm in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: filelock in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: psutil in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from accelerate) (2.1.1+cu121)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (5.14.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: colorama in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: decorator in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install transformers pandas scikit-learn pyarrow accelerate transformers[torch] ipywidgets tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, DistilBertTokenizer, DistilBertForSequenceClassification, AlbertTokenizer, AlbertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = f'logs/training_log_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log'\n",
    "logging.basicConfig(filename=log_filename, filemode='w', format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_category_data(file_path: str, category: str):\n",
    "    df = pd.read_feather(file_path)\n",
    "    # add a column for the category\n",
    "    df['category'] = category\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "  'bert-base-uncased': {\n",
    "    'model': BertForSequenceClassification,\n",
    "    'tokenizer': BertTokenizer,\n",
    "  },\n",
    "  'distilbert-base-uncased': {\n",
    "    'model': DistilBertForSequenceClassification,\n",
    "    'tokenizer': DistilBertTokenizer,\n",
    "  },\n",
    "  'distilbert-base-uncased-finetuned-sst-2-english': {\n",
    "    'model': DistilBertForSequenceClassification,\n",
    "    'tokenizer': DistilBertTokenizer,\n",
    "  },\n",
    "  'albert-base-v2': {\n",
    "    'model': AlbertForSequenceClassification,\n",
    "    'tokenizer': AlbertTokenizer,\n",
    "  },\n",
    "  'roberta-base': {\n",
    "    'model': RobertaForSequenceClassification,\n",
    "    'tokenizer': RobertaTokenizer,\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datasets = {\n",
    "  'crime': ['FA-KES-Dataset'],\n",
    "  'health': ['covid_FNIR', 'covid_fake_news_dataset'],\n",
    "  'politics': ['pheme', 'liar_dataset', ],\n",
    "  'science': ['climate_dataset'],\n",
    "  'social_media': ['gossipcop']\n",
    "}\n",
    "\n",
    "testing_datasets = {\n",
    "  'crime': ['snope'],\n",
    "  'health': ['covid_claims'],\n",
    "  'politics': ['politifact_dataset', 'fake_news_dataset'],\n",
    "  'science': ['climate_dataset'],\n",
    "  'social_media': ['isot_dataset']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (53818, 6)\n",
      "Testing data: (90893, 5)\n"
     ]
    }
   ],
   "source": [
    "# show info on the categories and datasets\n",
    "training_data = pd.concat([load_category_data(os.path.join(os.path.realpath('.'), f'..\\data\\{category}\\{dataset}.feather'), category) for category, datasets in training_datasets.items() for dataset in datasets])\n",
    "testing_data = pd.concat([load_category_data(os.path.join(os.path.realpath('.'), f'..\\data\\{category}\\{dataset}.feather'), category) for category, datasets in testing_datasets.items() for dataset in datasets])\n",
    "print(f'Training data: {training_data.shape}')\n",
    "print(f'Testing data: {testing_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the text, category, and label columns\n",
    "training_data = training_data[['text', 'category', 'label']]\n",
    "training_data.dropna(inplace=True)\n",
    "testing_data = testing_data[['text', 'category', 'label']]\n",
    "testing_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social_media</th>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text  label\n",
       "category                 \n",
       "crime          600    600\n",
       "health         600    600\n",
       "politics       600    600\n",
       "science        600    600\n",
       "social_media   600    600"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match the number of samples for each category\n",
    "df = training_data.groupby('label').apply(lambda x: x.sample(n=1000, random_state=42, replace=True)).groupby('category').apply(lambda x: x.sample(n=600, random_state=42, replace=True))\n",
    "df = df.reset_index(drop=True)\n",
    "df.groupby('category').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433</td>\n",
       "      <td>1433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  category\n",
       "label                \n",
       "0      1567      1567\n",
       "1      1433      1433"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = df['text'].to_list()\n",
    "df_labels = df['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df_texts, df_labels, test_size=0.2, random_state=7623)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "model_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging.info('====================================================')\n",
    "# load the tokenizer and model\n",
    "tokenizer = model_map[model_name]['tokenizer'].from_pretrained(model_name)\n",
    "model = model_map[model_name]['model'].from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the encodings\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encodings to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create a DataLoader for training and testing\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=8)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "data_test_dataloader = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model and data to the GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "train_inputs, train_masks, train_labels = train_inputs.to(device), train_masks.to(device), train_labels.to(device)\n",
    "test_inputs, test_masks, test_labels = test_inputs.to(device), test_masks.to(device), test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 300/300 [02:44<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average train loss: 0.4847223972032468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 1\n",
    "\n",
    "# Fine-tune the pre-trained BERT model\n",
    "train_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, masks, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "    logging.info(f\"Epoch {epoch+1} average train loss: {avg_train_loss}\")\n",
    "train_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/{model_name}-{model_time}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_start_time = time.time()\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in data_test_dataloader:\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': None}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "eval_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "precision = precision_score(test_labels.cpu(), predictions)\n",
    "recall = recall_score(test_labels.cpu(), predictions)\n",
    "f1 = f1_score(test_labels.cpu(), predictions)\n",
    "accuracy = accuracy_score(test_labels.cpu(), predictions)\n",
    "g_mean = (recall*accuracy)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased 2023-12-21_15-16-58 with 1 epochs: Evaluation Results:\n",
      "Training time: 164.36883211135864 seconds\n",
      "Inference time: 17.819432258605957 seconds\n",
      "Precision: 0.8493150684931506\n",
      "Recall: 0.8671328671328671\n",
      "F-score: 0.8581314878892733\n",
      "Accuracy: 0.8633333333333333\n",
      "G-mean: 0.8652310145994008\n"
     ]
    }
   ],
   "source": [
    "# Log the results\n",
    "logging.info(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results:\")\n",
    "logging.info(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "logging.info(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F-score: {f1}\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"G-mean: {g_mean}\")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results:\")\n",
    "print(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "print(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"G-mean: {g_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased 2023-12-21_15-16-58 with 1 epochs: Evaluation Results (completely new data):\n",
      "Training time: 164.36883211135864 seconds\n",
      "Inference time: 24.223811149597168 seconds\n",
      "Precision: 0.3878504672897196\n",
      "Recall: 0.4368421052631579\n",
      "F-score: 0.41089108910891087\n",
      "Accuracy: 0.405\n",
      "G-mean: 0.42061984336402747\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model further by testing it with ones that are not in df but are in data\n",
    "# get the data that is not in df but is in data\n",
    "\n",
    "data_only = testing_data.groupby('label').apply(lambda x: x.sample(n=1000, random_state=42, replace=True)).groupby('category').apply(lambda x: x.sample(n=160, random_state=42, replace=True))\n",
    "\n",
    "# Load the data\n",
    "data_texts = data_only['text'].to_list()\n",
    "data_labels = data_only['label'].to_list()\n",
    "\n",
    "data_test_labels = torch.tensor(data_labels).to(device)\n",
    "\n",
    "# Tokenize the data\n",
    "data_test_encodings = tokenizer(list(data_texts), truncation=True, padding=True)\n",
    "\n",
    "# Convert the encodings to PyTorch tensors\n",
    "\n",
    "data_test_inputs = torch.tensor(data_test_encodings['input_ids'])\n",
    "data_test_masks = torch.tensor(data_test_encodings['attention_mask'])\n",
    "\n",
    "# Create a DataLoader for testing\n",
    "\n",
    "data_test_data = TensorDataset(data_test_inputs, data_test_masks)\n",
    "data_test_dataloader = DataLoader(data_test_data, batch_size=8)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_start_time = time.time()\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in data_test_dataloader:\n",
    "    inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': None}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "eval_end_time = time.time()\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(data_test_labels.cpu(), predictions)\n",
    "recall = recall_score(data_test_labels.cpu(), predictions)\n",
    "f1 = f1_score(data_test_labels.cpu(), predictions)\n",
    "accuracy = accuracy_score(data_test_labels.cpu(), predictions)\n",
    "g_mean = (recall*accuracy)**0.5\n",
    "\n",
    "# Log the results\n",
    "logging.info(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results (completely new data):\")\n",
    "logging.info(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "logging.info(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F-score: {f1}\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"G-mean: {g_mean}\")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"{model_name} {model_time} with {epochs} epochs: Evaluation Results (completely new data):\")\n",
    "print(f\"Training time: {train_end_time - train_start_time} seconds\")\n",
    "print(f\"Inference time: {eval_end_time - eval_start_time} seconds\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"G-mean: {g_mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
