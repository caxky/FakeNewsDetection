2023-12-18 02:46:44,686 - ====================================================
2023-12-18 02:46:45,294 - Model: bert-base-uncased 2023-12-18_02-46-45 with 1 epochs
2023-12-18 03:01:35,732 - Epoch 1 average train loss: 0.580325
2023-12-18 03:01:44,445 - 2023-12-18_02-46-45 with 1 epochs: Evaluation Results:
2023-12-18 03:01:44,445 - Training time: 890.3558838367462 seconds
2023-12-18 03:01:44,445 - Inference time: 8.693742275238037 seconds
2023-12-18 03:01:44,445 - Precision: 0.7162162162162162
2023-12-18 03:01:44,445 - Recall: 0.4818181818181818
2023-12-18 03:01:44,445 - F-score: 0.576086956521739
2023-12-18 03:01:44,445 - Accuracy: 0.74
2023-12-18 03:01:44,445 - G-mean: 0.5971142726023676
2023-12-18 03:01:59,327 - bert-base-uncased 2023-12-18_02-46-45 with 1 epochs: Evaluation Results (completely new data):
2023-12-18 03:01:59,327 - Training time: 890.3558838367462 seconds
2023-12-18 03:01:59,327 - Inference time: 11.79974627494812 seconds
2023-12-18 03:01:59,327 - Precision: 0.6538461538461539
2023-12-18 03:01:59,327 - Recall: 0.1630695443645084
2023-12-18 03:01:59,327 - F-score: 0.2610364683301344
2023-12-18 03:01:59,327 - Accuracy: 0.51875
2023-12-18 03:01:59,327 - G-mean: 0.29084759950717964
2023-12-18 03:01:59,733 - ====================================================
2023-12-18 03:02:00,453 - Model: bert-base-uncased 2023-12-18_03-02-00 with 2 epochs
2023-12-18 03:19:36,856 - Epoch 1 average train loss: 0.581117
2023-12-18 03:37:13,209 - Epoch 2 average train loss: 0.467212
2023-12-18 03:37:21,920 - 2023-12-18_03-02-00 with 2 epochs: Evaluation Results:
2023-12-18 03:37:21,920 - Training time: 2112.703540802002 seconds
2023-12-18 03:37:21,920 - Inference time: 8.707075834274292 seconds
2023-12-18 03:37:21,921 - Precision: 0.6521739130434783
2023-12-18 03:37:21,921 - Recall: 0.6136363636363636
2023-12-18 03:37:21,921 - F-score: 0.6323185011709602
2023-12-18 03:37:21,921 - Accuracy: 0.7383333333333333
2023-12-18 03:37:21,921 - G-mean: 0.6731033960827875
2023-12-18 03:37:36,825 - bert-base-uncased 2023-12-18_03-02-00 with 2 epochs: Evaluation Results (completely new data):
2023-12-18 03:37:36,825 - Training time: 2112.703540802002 seconds
2023-12-18 03:37:36,825 - Inference time: 11.805768013000488 seconds
2023-12-18 03:37:36,825 - Precision: 0.5129310344827587
2023-12-18 03:37:36,825 - Recall: 0.5707434052757794
2023-12-18 03:37:36,826 - F-score: 0.540295119182747
2023-12-18 03:37:36,826 - Accuracy: 0.49375
2023-12-18 03:37:36,826 - G-mean: 0.5308526691605836
2023-12-18 03:37:37,180 - ====================================================
2023-12-18 03:37:38,015 - Model: bert-base-uncased 2023-12-18_03-37-38 with 3 epochs
2023-12-18 03:55:14,461 - Epoch 1 average train loss: 0.591054
2023-12-18 04:12:52,159 - Epoch 2 average train loss: 0.464908
2023-12-18 04:30:28,634 - Epoch 3 average train loss: 0.333263
2023-12-18 04:30:37,343 - 2023-12-18_03-37-38 with 3 epochs: Evaluation Results:
2023-12-18 04:30:37,343 - Training time: 3170.568157672882 seconds
2023-12-18 04:30:37,344 - Inference time: 8.704623937606812 seconds
2023-12-18 04:30:37,344 - Precision: 0.6363636363636364
2023-12-18 04:30:37,344 - Recall: 0.6363636363636364
2023-12-18 04:30:37,344 - F-score: 0.6363636363636364
2023-12-18 04:30:37,344 - Accuracy: 0.7333333333333333
2023-12-18 04:30:37,344 - G-mean: 0.6831300510639732
2023-12-18 04:30:52,392 - bert-base-uncased 2023-12-18_03-37-38 with 3 epochs: Evaluation Results (completely new data):
2023-12-18 04:30:52,392 - Training time: 3170.568157672882 seconds
2023-12-18 04:30:52,392 - Inference time: 11.840064764022827 seconds
2023-12-18 04:30:52,392 - Precision: 0.51138353765324
2023-12-18 04:30:52,392 - Recall: 0.7002398081534772
2023-12-18 04:30:52,392 - F-score: 0.5910931174089069
2023-12-18 04:30:52,392 - Accuracy: 0.495
2023-12-18 04:30:52,392 - G-mean: 0.5887433269566383
2023-12-18 04:30:52,757 - ====================================================
2023-12-18 04:30:53,543 - Model: bert-base-uncased 2023-12-18_04-30-53 with 4 epochs
2023-12-18 04:48:32,579 - Epoch 1 average train loss: 0.578613
2023-12-18 05:06:06,408 - Epoch 2 average train loss: 0.435385
2023-12-18 05:23:33,839 - Epoch 3 average train loss: 0.293042
2023-12-18 05:41:00,713 - Epoch 4 average train loss: 0.160625
2023-12-18 05:41:09,416 - 2023-12-18_04-30-53 with 4 epochs: Evaluation Results:
2023-12-18 05:41:09,416 - Training time: 4207.115457057953 seconds
2023-12-18 05:41:09,416 - Inference time: 8.698769330978394 seconds
2023-12-18 05:41:09,416 - Precision: 0.5609756097560976
2023-12-18 05:41:09,416 - Recall: 0.8363636363636363
2023-12-18 05:41:09,416 - F-score: 0.6715328467153284
2023-12-18 05:41:09,417 - Accuracy: 0.7
2023-12-18 05:41:09,417 - G-mean: 0.7651500150000294
2023-12-18 05:41:24,322 - bert-base-uncased 2023-12-18_04-30-53 with 4 epochs: Evaluation Results (completely new data):
2023-12-18 05:41:24,322 - Training time: 4207.115457057953 seconds
2023-12-18 05:41:24,323 - Inference time: 11.857557773590088 seconds
2023-12-18 05:41:24,323 - Precision: 0.5344311377245509
2023-12-18 05:41:24,323 - Recall: 0.8561151079136691
2023-12-18 05:41:24,323 - F-score: 0.6580645161290322
2023-12-18 05:41:24,323 - Accuracy: 0.53625
2023-12-18 05:41:24,323 - G-mean: 0.6775630794388852
2023-12-18 05:41:24,659 - ====================================================
2023-12-18 05:41:25,797 - Model: bert-base-uncased 2023-12-18_05-41-25 with 5 epochs
2023-12-18 05:58:53,366 - Epoch 1 average train loss: 0.594599
2023-12-18 06:16:20,331 - Epoch 2 average train loss: 0.470667
2023-12-18 06:33:47,979 - Epoch 3 average train loss: 0.332404
2023-12-18 06:51:15,659 - Epoch 4 average train loss: 0.223290
2023-12-18 07:08:42,894 - Epoch 5 average train loss: 0.134622
2023-12-18 07:08:51,507 - 2023-12-18_05-41-25 with 5 epochs: Evaluation Results:
2023-12-18 07:08:51,507 - Training time: 5237.043351650238 seconds
2023-12-18 07:08:51,507 - Inference time: 8.60900068283081 seconds
2023-12-18 07:08:51,507 - Precision: 0.6007194244604317
2023-12-18 07:08:51,508 - Recall: 0.759090909090909
2023-12-18 07:08:51,508 - F-score: 0.6706827309236948
2023-12-18 07:08:51,508 - Accuracy: 0.7266666666666667
2023-12-18 07:08:51,508 - G-mean: 0.7427018652232271
2023-12-18 07:09:06,401 - bert-base-uncased 2023-12-18_05-41-25 with 5 epochs: Evaluation Results (completely new data):
2023-12-18 07:09:06,401 - Training time: 5237.043351650238 seconds
2023-12-18 07:09:06,401 - Inference time: 11.852010250091553 seconds
2023-12-18 07:09:06,401 - Precision: 0.5055467511885895
2023-12-18 07:09:06,401 - Recall: 0.7649880095923262
2023-12-18 07:09:06,401 - F-score: 0.6087786259541985
2023-12-18 07:09:06,401 - Accuracy: 0.4875
2023-12-18 07:09:06,401 - G-mean: 0.6106813036897879
2023-12-18 07:09:06,745 - ====================================================
2023-12-18 07:09:07,566 - Model: bert-base-uncased 2023-12-18_07-09-07 with 6 epochs
2023-12-18 07:26:34,772 - Epoch 1 average train loss: 0.582726
2023-12-18 07:44:01,809 - Epoch 2 average train loss: 0.457509
2023-12-18 08:01:29,110 - Epoch 3 average train loss: 0.308132
2023-12-18 08:18:56,647 - Epoch 4 average train loss: 0.188636
2023-12-18 08:36:23,432 - Epoch 5 average train loss: 0.125578
2023-12-18 08:53:50,776 - Epoch 6 average train loss: 0.077589
2023-12-18 08:53:59,485 - 2023-12-18_07-09-07 with 6 epochs: Evaluation Results:
2023-12-18 08:53:59,486 - Training time: 6283.157334804535 seconds
2023-12-18 08:53:59,486 - Inference time: 8.704882383346558 seconds
2023-12-18 08:53:59,486 - Precision: 0.5857142857142857
2023-12-18 08:53:59,486 - Recall: 0.7454545454545455
2023-12-18 08:53:59,486 - F-score: 0.656
2023-12-18 08:53:59,486 - Accuracy: 0.7133333333333334
2023-12-18 08:53:59,486 - G-mean: 0.7292170978231214
2023-12-18 08:54:14,318 - bert-base-uncased 2023-12-18_07-09-07 with 6 epochs: Evaluation Results (completely new data):
2023-12-18 08:54:14,319 - Training time: 6283.157334804535 seconds
2023-12-18 08:54:14,319 - Inference time: 11.790384769439697 seconds
2023-12-18 08:54:14,319 - Precision: 0.5260736196319018
2023-12-18 08:54:14,319 - Recall: 0.8225419664268585
2023-12-18 08:54:14,319 - F-score: 0.6417212347988774
2023-12-18 08:54:14,319 - Accuracy: 0.52125
2023-12-18 08:54:14,319 - G-mean: 0.6547900426854397
2023-12-18 08:54:14,681 - ====================================================
2023-12-18 08:54:15,419 - Model: bert-base-uncased 2023-12-18_08-54-15 with 7 epochs
2023-12-18 09:11:42,680 - Epoch 1 average train loss: 0.597120
2023-12-18 09:29:12,702 - Epoch 2 average train loss: 0.517795
2023-12-18 09:46:40,025 - Epoch 3 average train loss: 0.386982
2023-12-18 10:04:07,521 - Epoch 4 average train loss: 0.245820
2023-12-18 10:21:35,256 - Epoch 5 average train loss: 0.141591
2023-12-18 10:39:02,055 - Epoch 6 average train loss: 0.084181
2023-12-18 10:56:29,733 - Epoch 7 average train loss: 0.060800
2023-12-18 10:56:38,448 - 2023-12-18_08-54-15 with 7 epochs: Evaluation Results:
2023-12-18 10:56:38,448 - Training time: 7334.26228761673 seconds
2023-12-18 10:56:38,448 - Inference time: 8.71172833442688 seconds
2023-12-18 10:56:38,448 - Precision: 0.6167883211678832
2023-12-18 10:56:38,448 - Recall: 0.7681818181818182
2023-12-18 10:56:38,448 - F-score: 0.6842105263157895
2023-12-18 10:56:38,449 - Accuracy: 0.74
2023-12-18 10:56:38,449 - G-mean: 0.7539592465475474
2023-12-18 10:56:53,289 - bert-base-uncased 2023-12-18_08-54-15 with 7 epochs: Evaluation Results (completely new data):
2023-12-18 10:56:53,289 - Training time: 7334.26228761673 seconds
2023-12-18 10:56:53,289 - Inference time: 11.8058762550354 seconds
2023-12-18 10:56:53,289 - Precision: 0.551219512195122
2023-12-18 10:56:53,289 - Recall: 0.8129496402877698
2023-12-18 10:56:53,289 - F-score: 0.6569767441860466
2023-12-18 10:56:53,289 - Accuracy: 0.5575
2023-12-18 10:56:53,289 - G-mean: 0.673215733966781
2023-12-18 10:56:53,647 - ====================================================
2023-12-18 10:56:54,411 - Model: bert-base-uncased 2023-12-18_10-56-54 with 8 epochs
2023-12-18 11:14:22,260 - Epoch 1 average train loss: 0.590512
2023-12-18 11:31:49,430 - Epoch 2 average train loss: 0.458977
2023-12-18 11:49:15,256 - Epoch 3 average train loss: 0.309564
2023-12-18 12:06:26,843 - Epoch 4 average train loss: 0.187900
2023-12-18 12:23:38,113 - Epoch 5 average train loss: 0.121546
2023-12-18 12:40:49,174 - Epoch 6 average train loss: 0.073460
2023-12-18 12:58:00,885 - Epoch 7 average train loss: 0.058623
2023-12-18 13:15:27,675 - Epoch 8 average train loss: 0.049055
2023-12-18 13:15:36,715 - 2023-12-18_10-56-54 with 8 epochs: Evaluation Results:
2023-12-18 13:15:36,716 - Training time: 8313.212953567505 seconds
2023-12-18 13:15:36,716 - Inference time: 9.034228086471558 seconds
2023-12-18 13:15:36,716 - Precision: 0.5929824561403508
2023-12-18 13:15:36,716 - Recall: 0.7681818181818182
2023-12-18 13:15:36,716 - F-score: 0.6693069306930693
2023-12-18 13:15:36,716 - Accuracy: 0.7216666666666667
2023-12-18 13:15:36,716 - G-mean: 0.74456108689698
2023-12-18 13:15:51,780 - bert-base-uncased 2023-12-18_10-56-54 with 8 epochs: Evaluation Results (completely new data):
2023-12-18 13:15:51,780 - Training time: 8313.212953567505 seconds
2023-12-18 13:15:51,780 - Inference time: 11.864268064498901 seconds
2023-12-18 13:15:51,780 - Precision: 0.5263929618768328
2023-12-18 13:15:51,780 - Recall: 0.8609112709832134
2023-12-18 13:15:51,780 - F-score: 0.6533212010919017
2023-12-18 13:15:51,780 - Accuracy: 0.52375
2023-12-18 13:15:51,780 - G-mean: 0.671492574923549