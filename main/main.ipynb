{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (1.24.1)\n",
      "Requirement already satisfied: transformers in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (4.36.0)\n",
      "Requirement already satisfied: pandas in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: torch in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pyarrow in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (14.0.1)\n",
      "Requirement already satisfied: accelerate in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: ipywidgets in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (8.1.1)\n",
      "Requirement already satisfied: tqdm in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: datetime in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (5.4)\n",
      "Requirement already satisfied: imblearn in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: sentencepiece in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (0.1.99)\n",
      "Requirement already satisfied: papermill in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: xlsxwriter in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (3.1.9)\n",
      "Requirement already satisfied: filelock in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: psutil in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: protobuf in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from transformers) (4.25.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (5.14.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: colorama in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: zope.interface in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from datetime) (6.1)\n",
      "Requirement already satisfied: imbalanced-learn in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from imblearn) (0.11.0)\n",
      "Requirement already satisfied: click in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from papermill) (8.1.7)\n",
      "Requirement already satisfied: nbformat>=5.1.2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from papermill) (5.9.2)\n",
      "Requirement already satisfied: nbclient>=0.2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from papermill) (0.9.0)\n",
      "Requirement already satisfied: entrypoints in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from papermill) (0.4)\n",
      "Requirement already satisfied: tenacity>=5.0.2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from papermill) (8.2.3)\n",
      "Requirement already satisfied: decorator in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from nbclient>=0.2.0->papermill) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from nbclient>=0.2.0->papermill) (5.5.0)\n",
      "Requirement already satisfied: fastjsonschema in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from nbformat>=5.1.2->papermill) (2.19.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from nbformat>=5.1.2->papermill) (4.20.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: setuptools in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from zope.interface->datetime) (68.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.15.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.2 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (6.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill) (4.1.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill) (306)\n",
      "Requirement already satisfied: wcwidth in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in d:\\projects\\school\\csi4900\\fake-news-detection\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy transformers pandas torch scikit-learn pyarrow accelerate transformers[torch] transformers[sentencepiece] ipywidgets tqdm datetime imblearn sentencepiece papermill xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import string\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, DistilBertTokenizerFast, DistilBertForSequenceClassification, AlbertTokenizerFast, AlbertForSequenceClassification, RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = f'logs/log_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log'\n",
    "logging.basicConfig(filename=log_filename, filemode='w', format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_category_data(dataset: str, category: str):\n",
    "    df = pd.read_feather(os.path.join(os.path.realpath('.'), f'../data/{category}/{dataset}.feather'))\n",
    "    # add a column for the category\n",
    "    df['category'] = category\n",
    "    df['dataset'] = dataset\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                text   label  metadata  dataset  title  author\n",
      "category                                                      \n",
      "crime           1119    1119      1119     1119      0       0\n",
      "health         13521   13528     13528    13528   5939       0\n",
      "politics      106071  106110    106110   106110      0    6424\n",
      "science         2907    2907      2907     2907      0       0\n",
      "social_media   67038   67038     67038    67038      0       0\n"
     ]
    }
   ],
   "source": [
    "# load the testing datasets\n",
    "testing_datasets = {\n",
    "  'crime': ['FA-KES-Dataset', 'snope'],\n",
    "  'health': ['covid_claims', 'covid_fake_news_dataset', 'covid_FNIR'],\n",
    "  'politics': ['fake_news_dataset', 'isot_dataset', 'liar_dataset', 'pheme', 'politifact_dataset'],\n",
    "  'science': ['climate_dataset', 'isot_multipurpose_small'],\n",
    "  'social_media': ['gossipcop', 'isot_dataset']\n",
    "}\n",
    "\n",
    "data = pd.concat([load_category_data(dataset, category) for category, datasets in testing_datasets.items() for dataset in datasets])\n",
    "print(data.groupby('category').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                text   label  dataset\n",
      "category                             \n",
      "crime           1119    1119     1119\n",
      "health         13521   13521    13521\n",
      "politics      106071  106071   106071\n",
      "science         2907    2907     2907\n",
      "social_media   67038   67038    67038\n"
     ]
    }
   ],
   "source": [
    "# keep only the text and category columns\n",
    "data = data[['text', 'category', 'label', 'dataset']]\n",
    "data.dropna(inplace=True)\n",
    "print(data.groupby('category').count())\n",
    "logging.info(f'Number of samples: {len(data)}')\n",
    "logging.info(data.groupby('category').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['crime', 'health', 'politics', 'science', 'social_media']\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain-specific Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "classification_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(categories))\n",
    "classification_model_state_dir = 'models/classification/distilbert-base-uncased-2023-12-13_17-42-42.pt'\n",
    "classification_model.load_state_dict(torch.load(classification_model_state_dir, map_location=device))\n",
    "classification_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "detection_models = {\n",
    "  'crime': {\n",
    "    'model': DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2),\n",
    "    'tokenizer': DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased'),\n",
    "    'state_dir': 'models/detection/bert_model_wPwIZtuKPF.pt'\n",
    "  },\n",
    "  'health': {\n",
    "    'model': DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2),\n",
    "    'tokenizer': DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased'),\n",
    "    'state_dir': 'models/detection/bert_model_VTSrphxrbr.pt'\n",
    "  },\n",
    "  'politics': {\n",
    "    'model': BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2),\n",
    "    'tokenizer': BertTokenizerFast.from_pretrained('bert-base-uncased'),\n",
    "    'state_dir': 'models/detection/bert_model_DMaHx8fEE3.pt'\n",
    "  },\n",
    "  'science': {\n",
    "    'model': BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2),\n",
    "    'tokenizer': BertTokenizerFast.from_pretrained('bert-base-cased'),\n",
    "    'state_dir': 'models/detection/bert_model_tV8F6Cjsy0.pt'\n",
    "  },\n",
    "  'social_media': {\n",
    "    'model': DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2),\n",
    "    'tokenizer': DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased'),\n",
    "    'state_dir': 'models/detection/bert_model_mLXTbOJuAN.pt'\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading crime model\n",
      "Loading health model\n",
      "Loading politics model\n",
      "Loading science model\n",
      "Loading social_media model\n"
     ]
    }
   ],
   "source": [
    "# load the detection models\n",
    "for category, model in detection_models.items():\n",
    "  print(f'Loading {category} model')\n",
    "  if model is not None:\n",
    "    model['model'].load_state_dict(torch.load(model['state_dir'], map_location=device))\n",
    "    model['model'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(input: str):\n",
    "    # tokenize the input\n",
    "    input = classification_tokenizer(input, return_tensors='pt', padding=True, truncation=True)\n",
    "    input = input.to(device)\n",
    "    # get the prediction\n",
    "    output = classification_model(**input)\n",
    "    # get the prediction\n",
    "    prediction = torch.argmax(output.logits, dim=1).item()\n",
    "    return categories[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detection(input: str, category: str):\n",
    "    if detection_models[category] is None:\n",
    "        return 0\n",
    "    # tokenize the input\n",
    "    input = detection_models[category]['tokenizer'](input, return_tensors='pt', padding=True, truncation=True)\n",
    "    input = input.to(device)\n",
    "    # get the prediction\n",
    "    output = detection_models[category]['model'](**input)\n",
    "    # get the prediction\n",
    "    prediction = torch.argmax(output.logits, dim=1).item()\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(input: str):\n",
    "    category = get_category(input)\n",
    "    detection = get_detection(input, category)\n",
    "    return category, detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 7406.596935033798\n"
     ]
    }
   ],
   "source": [
    "# get the predictions \n",
    "results = []\n",
    "start_time = time.time()\n",
    "for index, row in data.iterrows():\n",
    "    category, detection = get_prediction(row['text'])\n",
    "    results.append({\n",
    "        'text': row['text'],\n",
    "        'category': row['category'],\n",
    "        'label': row['label'],\n",
    "        'dataset': row['dataset'],\n",
    "        'category_prediction': category,\n",
    "        'detection_prediction': detection\n",
    "    })\n",
    "print(f'Elapsed time: {time.time() - start_time}')\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results to a csv\n",
    "results.to_csv('results.csv', index=False)\n",
    "# drop the text column\n",
    "results.drop(columns=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the baseline detection model\n",
    "baseline_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "baseline_model_state_dir = 'models/detection-baseline/bert-base-uncased-2023-12-21_20-36-38.pt'\n",
    "baseline_model.load_state_dict(torch.load(baseline_model_state_dir, map_location=device))\n",
    "baseline_model.to(device)\n",
    "baseline_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions\n",
    "baseline_results = []\n",
    "start_time = time.time()\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    # tokenize the input\n",
    "    input = baseline_tokenizer(row['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    input = input.to(device)\n",
    "    # get the prediction\n",
    "    output = baseline_model(**input)\n",
    "    # get the prediction\n",
    "    overall_prediction = torch.argmax(output.logits, dim=1).item()\n",
    "    baseline_results.append({\n",
    "        'text': row['text'],\n",
    "        'category': row['category'],\n",
    "        'label': row['label'],\n",
    "        'dataset': row['dataset'],\n",
    "        'prediction': overall_prediction\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results to a csv\n",
    "baseline_results = pd.DataFrame(baseline_results)\n",
    "baseline_results.to_csv('baseline_results.csv', index=False)\n",
    "# drop the text column\n",
    "baseline_results.drop(columns=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-specific detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category precision: 0.7284218697549514\n",
      "Category recall: 0.7284218697549514\n",
      "Category f-score: 0.7284218697549514\n",
      "Category accuracy: 0.7284218697549514\n",
      "Category g-mean: 0.7284218697549514\n"
     ]
    }
   ],
   "source": [
    "# Category detection metrics\n",
    "category_detection_prediction = precision_score(results['category'], results['category_prediction'], average='micro')\n",
    "category_detection_recall = recall_score(results['category'], results['category_prediction'], average='micro')\n",
    "category_detection_f1 = f1_score(results['category'], results['category_prediction'], average='micro')\n",
    "category_detection_accuracy = accuracy_score(results['category'], results['category_prediction'])\n",
    "category_detection_gmean = np.sqrt(category_detection_prediction * category_detection_recall)\n",
    "\n",
    "print(f'Category precision: {category_detection_prediction}')\n",
    "print(f'Category recall: {category_detection_recall}')\n",
    "print(f'Category f-score: {category_detection_f1}')\n",
    "print(f'Category accuracy: {category_detection_accuracy}')\n",
    "print(f'Category g-mean: {category_detection_gmean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Category detection metrics by category\n",
    "category_detection_metrics = defaultdict(dict)\n",
    "for category in categories:\n",
    "    category_results = results[results['category'] == category]\n",
    "    category_detection_metrics[category]['precision'] = precision_score(category_results['category'], category_results['category_prediction'], average='macro')\n",
    "    category_detection_metrics[category]['recall'] = recall_score(category_results['category'], category_results['category_prediction'], average='macro')\n",
    "    category_detection_metrics[category]['f1'] = f1_score(category_results['category'], category_results['category_prediction'], average='macro')\n",
    "    category_detection_metrics[category]['accuracy'] = accuracy_score(category_results['category'], category_results['category_prediction'])\n",
    "    category_detection_metrics[category]['gmean'] = np.sqrt(category_detection_metrics[category]['precision'] * category_detection_metrics[category]['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\School\\CSI4900\\Fake-News-Detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Category detection metrics by dataset\n",
    "dataset_detection_metrics = defaultdict(dict)\n",
    "for dataset in data['dataset'].unique():\n",
    "    dataset_results = results[results['dataset'] == dataset]\n",
    "    dataset_detection_metrics[dataset]['precision'] = precision_score(dataset_results['category'], dataset_results['category_prediction'], average='macro')\n",
    "    dataset_detection_metrics[dataset]['recall'] = recall_score(dataset_results['category'], dataset_results['category_prediction'], average='macro')\n",
    "    dataset_detection_metrics[dataset]['f1'] = f1_score(dataset_results['category'], dataset_results['category_prediction'], average='macro')\n",
    "    dataset_detection_metrics[dataset]['accuracy'] = accuracy_score(dataset_results['category'], dataset_results['category_prediction'])\n",
    "    dataset_detection_metrics[dataset]['gmean'] = np.sqrt(dataset_detection_metrics[dataset]['precision'] * dataset_detection_metrics[dataset]['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'FA-KES-Dataset': {'precision': 1.0,\n",
       "              'recall': 1.0,\n",
       "              'f1': 1.0,\n",
       "              'accuracy': 1.0,\n",
       "              'gmean': 1.0},\n",
       "             'snope': {'precision': 0.3333333333333333,\n",
       "              'recall': 0.326984126984127,\n",
       "              'f1': 0.3301282051282051,\n",
       "              'accuracy': 0.9809523809523809,\n",
       "              'gmean': 0.330143467290675},\n",
       "             'covid_claims': {'precision': 0.2,\n",
       "              'recall': 0.19466950959488274,\n",
       "              'f1': 0.19729875742841707,\n",
       "              'accuracy': 0.9733475479744137,\n",
       "              'gmean': 0.19731675529203432},\n",
       "             'covid_fake_news_dataset': {'precision': 0.3333333333333333,\n",
       "              'recall': 0.3255316875066795,\n",
       "              'f1': 0.3293863206271965,\n",
       "              'accuracy': 0.9765950625200385,\n",
       "              'gmean': 0.3294094147140098},\n",
       "             'covid_FNIR': {'precision': 0.2,\n",
       "              'recall': 0.1908539799683711,\n",
       "              'f1': 0.19531998111807944,\n",
       "              'accuracy': 0.9542698998418555,\n",
       "              'gmean': 0.19537347822484563},\n",
       "             'fake_news_dataset': {'precision': 0.2,\n",
       "              'recall': 0.18013583160734067,\n",
       "              'f1': 0.18954891028890014,\n",
       "              'accuracy': 0.9006791580367034,\n",
       "              'gmean': 0.18980823565237662},\n",
       "             'isot_dataset': {'precision': 0.2,\n",
       "              'recall': 0.19963027306338813,\n",
       "              'f1': 0.1388028306546975,\n",
       "              'accuracy': 0.4990756826584703,\n",
       "              'gmean': 0.1998150510163777},\n",
       "             'liar_dataset': {'precision': 0.2,\n",
       "              'recall': 0.19261452165783732,\n",
       "              'f1': 0.19623779665052782,\n",
       "              'accuracy': 0.9630726082891866,\n",
       "              'gmean': 0.1962725256666543},\n",
       "             'pheme': {'precision': 0.25,\n",
       "              'recall': 0.24381226650062265,\n",
       "              'f1': 0.24686736543462842,\n",
       "              'accuracy': 0.9752490660024906,\n",
       "              'gmean': 0.24688674858152201},\n",
       "             'politifact_dataset': {'precision': 0.2,\n",
       "              'recall': 0.18975983358547655,\n",
       "              'f1': 0.19474539676378544,\n",
       "              'accuracy': 0.9487991679273827,\n",
       "              'gmean': 0.19481264516733843},\n",
       "             'climate_dataset': {'precision': 0.3333333333333333,\n",
       "              'recall': 0.32855567805953695,\n",
       "              'f1': 0.33092726263187117,\n",
       "              'accuracy': 0.9856670341786108,\n",
       "              'gmean': 0.3309358840518492},\n",
       "             'isot_multipurpose_small': {'precision': 0.25,\n",
       "              'recall': 0.000125,\n",
       "              'f1': 0.00024987506246876566,\n",
       "              'accuracy': 0.0005,\n",
       "              'gmean': 0.005590169943749474},\n",
       "             'gossipcop': {'precision': 0.2,\n",
       "              'recall': 0.19542908762420957,\n",
       "              'f1': 0.19768812537122493,\n",
       "              'accuracy': 0.9771454381210479,\n",
       "              'gmean': 0.19770133415038432}})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_detection_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall precision: 0.9093540885577334\n",
      "Overall recall: 0.6408861338120032\n",
      "Overall f-score: 0.7518737001818111\n",
      "Overall accuracy: 0.8010028533064787\n",
      "Overall g-mean: 0.7634084267820888\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, f-score, accuracy, recall, and g-mean\n",
    "overall_prediction = precision_score(results['label'], results['detection_prediction'])\n",
    "overall_recall = recall_score(results['label'], results['detection_prediction'])\n",
    "overall_f1 = f1_score(results['label'], results['detection_prediction'])\n",
    "overall_accuracy = accuracy_score(results['label'], results['detection_prediction'])\n",
    "overall_gmean = np.sqrt(overall_prediction * overall_recall)\n",
    "\n",
    "# print\n",
    "print(f'Overall precision: {overall_prediction}')\n",
    "print(f'Overall recall: {overall_recall}')\n",
    "print(f'Overall f-score: {overall_f1}')\n",
    "print(f'Overall accuracy: {overall_accuracy}')\n",
    "print(f'Overall g-mean: {overall_gmean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: crime\n",
      "Precision: 0.8362068965517241\n",
      "Recall: 0.33856893542757416\n",
      "F-score: 0.4819875776397516\n",
      "Accuracy: 0.6273458445040214\n",
      "G-mean: 0.5320842778758952\n",
      "\n",
      "Category: health\n",
      "Precision: 0.4735125260960334\n",
      "Recall: 0.5631595282433272\n",
      "F-score: 0.5144598809186277\n",
      "Accuracy: 0.49338066711042083\n",
      "G-mean: 0.5163943171778214\n",
      "\n",
      "Category: politics\n",
      "Precision: 0.9692012260423206\n",
      "Recall: 0.4987742070354815\n",
      "F-score: 0.6586112574970514\n",
      "Accuracy: 0.7434925662999312\n",
      "G-mean: 0.6952787735700515\n",
      "\n",
      "Category: science\n",
      "Precision: 0.8969404186795491\n",
      "Recall: 0.8890662410215483\n",
      "F-score: 0.8929859719438877\n",
      "Accuracy: 0.9081527347781218\n",
      "G-mean: 0.8929946508550431\n",
      "\n",
      "Category: social_media\n",
      "Precision: 0.9742554452511483\n",
      "Recall: 0.9131023468962645\n",
      "F-score: 0.9426881720430108\n",
      "Accuracy: 0.9522957128792625\n",
      "G-mean: 0.9431834039757531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the metrics for each category\n",
    "overall_category_metrics = defaultdict(list)\n",
    "for category in categories:\n",
    "    category_results = results[results['category'] == category]\n",
    "    detection_prediction = precision_score(category_results['label'], category_results['detection_prediction'])\n",
    "    detection_recall = recall_score(category_results['label'], category_results['detection_prediction'])\n",
    "    detection_f1 = f1_score(category_results['label'], category_results['detection_prediction'])\n",
    "    detection_accuracy = accuracy_score(category_results['label'], category_results['detection_prediction'])\n",
    "    detection_gmean = np.sqrt(detection_prediction * detection_recall)\n",
    "\n",
    "    overall_category_metrics[category].append({\n",
    "        'precision': detection_prediction,\n",
    "        'recall': detection_recall,\n",
    "        'f1': detection_f1,\n",
    "        'accuracy': detection_accuracy,\n",
    "        'gmean': detection_gmean\n",
    "    })\n",
    "\n",
    "    # print\n",
    "    print(f'Category: {category}')\n",
    "    print(f'Precision: {detection_prediction}')\n",
    "    print(f'Recall: {detection_recall}')\n",
    "    print(f'F-score: {detection_f1}')\n",
    "    print(f'Accuracy: {detection_accuracy}')\n",
    "    print(f'G-mean: {detection_gmean}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: FA-KES-Dataset\n",
      "Precision: 0.75\n",
      "Recall: 0.007936507936507936\n",
      "F-score: 0.015706806282722512\n",
      "Accuracy: 0.5323383084577115\n",
      "G-mean: 0.07715167498104596\n",
      "\n",
      "Dataset: snope\n",
      "Precision: 0.8377192982456141\n",
      "Recall: 0.9794871794871794\n",
      "F-score: 0.9030732860520094\n",
      "Accuracy: 0.8698412698412699\n",
      "G-mean: 0.9058340425489515\n",
      "\n",
      "Dataset: covid_claims\n",
      "Precision: 0.9568699938385705\n",
      "Recall: 0.9761156505342552\n",
      "F-score: 0.9663970130678282\n",
      "Accuracy: 0.9616204690831557\n",
      "G-mean: 0.9664449164398585\n",
      "\n",
      "Dataset: covid_fake_news_dataset\n",
      "Precision: 0.7900232018561485\n",
      "Recall: 0.6436672967863895\n",
      "F-score: 0.7093750000000001\n",
      "Accuracy: 0.8210965052901571\n",
      "G-mean: 0.7131003426848673\n",
      "\n",
      "Dataset: covid_FNIR\n",
      "Precision: 0.2693570187294845\n",
      "Recall: 0.3675889328063241\n",
      "F-score: 0.3108981502117228\n",
      "Accuracy: 0.18502899314707433\n",
      "G-mean: 0.3146627703727663\n",
      "\n",
      "Dataset: fake_news_dataset\n",
      "Precision: 0.8394648829431438\n",
      "Recall: 0.14517061885482938\n",
      "F-score: 0.24753451676528598\n",
      "Accuracy: 0.5589807812725784\n",
      "G-mean: 0.3490925902446987\n",
      "\n",
      "Dataset: isot_dataset\n",
      "Precision: 0.9999121882683526\n",
      "Recall: 0.9698905498062264\n",
      "F-score: 0.9846725900944723\n",
      "Accuracy: 0.9842086507194084\n",
      "G-mean: 0.9847869729223369\n",
      "\n",
      "Dataset: liar_dataset\n",
      "Precision: 0.5033112582781457\n",
      "Recall: 0.026812488975127887\n",
      "F-score: 0.050912744933846926\n",
      "Accuracy: 0.5585073231536304\n",
      "G-mean: 0.11616810045636679\n",
      "\n",
      "Dataset: pheme\n",
      "Precision: 0.8475609756097561\n",
      "Recall: 0.6247191011235955\n",
      "F-score: 0.7192755498059508\n",
      "Accuracy: 0.8986612702366127\n",
      "G-mean: 0.7276589385353309\n",
      "\n",
      "Dataset: politifact_dataset\n",
      "Precision: 0.8004905968928864\n",
      "Recall: 0.0832482993197279\n",
      "F-score: 0.15081260109373798\n",
      "Accuracy: 0.47877269288956126\n",
      "G-mean: 0.2581462391993473\n",
      "\n",
      "Dataset: climate_dataset\n",
      "Precision: 0.5347985347985348\n",
      "Recall: 0.5770750988142292\n",
      "F-score: 0.5551330798479087\n",
      "Accuracy: 0.7420066152149944\n",
      "G-mean: 0.5555348029732876\n",
      "\n",
      "Dataset: isot_multipurpose_small\n",
      "Precision: 0.9989680082559339\n",
      "Recall: 0.968\n",
      "F-score: 0.9832402234636872\n",
      "Accuracy: 0.9835\n",
      "G-mean: 0.9833621062415127\n",
      "\n",
      "Dataset: gossipcop\n",
      "Precision: 0.8357819905213271\n",
      "Recall: 0.6625962802930678\n",
      "F-score: 0.7391805511893536\n",
      "Accuracy: 0.8875790424570912\n",
      "G-mean: 0.7441680173558707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the metrics for each dataset\n",
    "overall_dataset_metrics = defaultdict(list)\n",
    "for dataset in data['dataset'].unique():\n",
    "    dataset_results = results[results['dataset'] == dataset]\n",
    "    detection_prediction = precision_score(dataset_results['label'], dataset_results['detection_prediction'])\n",
    "    detection_recall = recall_score(dataset_results['label'], dataset_results['detection_prediction'])\n",
    "    detection_f1 = f1_score(dataset_results['label'], dataset_results['detection_prediction'])\n",
    "    detection_accuracy = accuracy_score(dataset_results['label'], dataset_results['detection_prediction'])\n",
    "    detection_gmean = np.sqrt(detection_prediction * detection_recall)\n",
    "\n",
    "    overall_dataset_metrics[dataset].append({\n",
    "        'precision': detection_prediction,\n",
    "        'recall': detection_recall,\n",
    "        'f1': detection_f1,\n",
    "        'accuracy': detection_accuracy,\n",
    "        'gmean': detection_gmean\n",
    "    })\n",
    "\n",
    "    print(f'Dataset: {dataset}')\n",
    "    print(f'Precision: {detection_prediction}')\n",
    "    print(f'Recall: {detection_recall}')\n",
    "    print(f'F-score: {detection_f1}')\n",
    "    print(f'Accuracy: {detection_accuracy}')\n",
    "    print(f'G-mean: {detection_gmean}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall precision: 0.5733203679772765\n",
      "Overall recall: 0.4995819071722431\n",
      "Overall f-score: 0.5339171879654453\n",
      "Overall accuracy: 0.5896693521315878\n",
      "Overall g-mean: 0.5351826630738145\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, f-score, accuracy, recall, and g-mean\n",
    "baseline_prediction = precision_score(baseline_results['label'], baseline_results['prediction'])\n",
    "baseline_recall = recall_score(baseline_results['label'], baseline_results['prediction'])\n",
    "baseline_f1 = f1_score(baseline_results['label'], baseline_results['prediction'])\n",
    "baseline_accuracy = accuracy_score(baseline_results['label'], baseline_results['prediction'])\n",
    "baseline_gmean = np.sqrt(baseline_prediction * baseline_recall)\n",
    "\n",
    "# print\n",
    "print(f'Overall precision: {baseline_prediction}')\n",
    "print(f'Overall recall: {baseline_recall}')\n",
    "print(f'Overall f-score: {baseline_f1}')\n",
    "print(f'Overall accuracy: {baseline_accuracy}')\n",
    "print(f'Overall g-mean: {baseline_gmean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: crime\n",
      "Precision: 0.5257985257985258\n",
      "Recall: 0.37347294938917974\n",
      "F-score: 0.43673469387755104\n",
      "Accuracy: 0.5067024128686327\n",
      "G-mean: 0.4431382698599368\n",
      "\n",
      "Category: health\n",
      "Precision: 0.856718082140866\n",
      "Recall: 0.7153941651148356\n",
      "F-score: 0.7797040169133193\n",
      "Accuracy: 0.8073367354485615\n",
      "G-mean: 0.782873627804608\n",
      "\n",
      "Category: politics\n",
      "Precision: 0.5626445325404691\n",
      "Recall: 0.5178737718314678\n",
      "F-score: 0.5393316246251892\n",
      "Accuracy: 0.5611335803376983\n",
      "G-mean: 0.5397951891848294\n",
      "\n",
      "Category: science\n",
      "Precision: 0.5005324813631523\n",
      "Recall: 0.3750997605746209\n",
      "F-score: 0.42883211678832117\n",
      "Accuracy: 0.5693154454764362\n",
      "G-mean: 0.4333008353547675\n",
      "\n",
      "Category: social_media\n",
      "Precision: 0.5333072441081833\n",
      "Recall: 0.4258089154284127\n",
      "F-score: 0.4735338403922629\n",
      "Accuracy: 0.5931859542349115\n",
      "G-mean: 0.4765364405833213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the metrics for each category\n",
    "baseline_category_metrics = defaultdict(list)\n",
    "for category in categories:\n",
    "    category_results = baseline_results[baseline_results['category'] == category]\n",
    "    detection_prediction = precision_score(category_results['label'], category_results['prediction'])\n",
    "    detection_recall = recall_score(category_results['label'], category_results['prediction'])\n",
    "    detection_f1 = f1_score(category_results['label'], category_results['prediction'])\n",
    "    detection_accuracy = accuracy_score(category_results['label'], category_results['prediction'])\n",
    "    detection_gmean = np.sqrt(detection_prediction * detection_recall)\n",
    "\n",
    "    baseline_category_metrics[category].append({\n",
    "        'precision': detection_prediction,\n",
    "        'recall': detection_recall,\n",
    "        'f1': detection_f1,\n",
    "        'accuracy': detection_accuracy,\n",
    "        'gmean': detection_gmean\n",
    "    })\n",
    "\n",
    "    print(f'Category: {category}')\n",
    "    print(f'Precision: {detection_prediction}')\n",
    "    print(f'Recall: {detection_recall}')\n",
    "    print(f'F-score: {detection_f1}')\n",
    "    print(f'Accuracy: {detection_accuracy}')\n",
    "    print(f'G-mean: {detection_gmean}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: FA-KES-Dataset\n",
      "Precision: 0.458128078817734\n",
      "Recall: 0.24603174603174602\n",
      "F-score: 0.3201376936316695\n",
      "Accuracy: 0.5087064676616916\n",
      "G-mean: 0.33572913358494294\n",
      "\n",
      "Dataset: snope\n",
      "Precision: 0.5931372549019608\n",
      "Recall: 0.6205128205128205\n",
      "F-score: 0.606516290726817\n",
      "Accuracy: 0.5015873015873016\n",
      "G-mean: 0.606670644576155\n",
      "\n",
      "Dataset: covid_claims\n",
      "Precision: 0.5694945848375451\n",
      "Recall: 0.3966059082338152\n",
      "F-score: 0.4675805854020007\n",
      "Accuracy: 0.48933901918976547\n",
      "G-mean: 0.4752524771673832\n",
      "\n",
      "Dataset: covid_fake_news_dataset\n",
      "Precision: 0.5175600739371534\n",
      "Recall: 0.2646502835538752\n",
      "F-score: 0.3502188868042526\n",
      "Accuracy: 0.6668804103879449\n",
      "G-mean: 0.3700978523629018\n",
      "\n",
      "Dataset: covid_FNIR\n",
      "Precision: 0.9911575562700965\n",
      "Recall: 0.974703557312253\n",
      "F-score: 0.9828616978876046\n",
      "Accuracy: 0.9829994728518714\n",
      "G-mean: 0.9828961267363824\n",
      "\n",
      "Dataset: fake_news_dataset\n",
      "Precision: 0.5765467625899281\n",
      "Recall: 0.38625409678041256\n",
      "F-score: 0.46259524359270376\n",
      "Accuracy: 0.5515630268291508\n",
      "G-mean: 0.47190417357324105\n",
      "\n",
      "Dataset: isot_dataset\n",
      "Precision: 0.5810313075506446\n",
      "Recall: 0.3762190707380435\n",
      "F-score: 0.4567144888199561\n",
      "Accuracy: 0.5318945164595305\n",
      "G-mean: 0.46754150467783484\n",
      "\n",
      "Dataset: liar_dataset\n",
      "Precision: 0.48271699769559967\n",
      "Recall: 0.7759745986946551\n",
      "F-score: 0.5951833310783385\n",
      "Accuracy: 0.5338111561234029\n",
      "G-mean: 0.6120262482687583\n",
      "\n",
      "Dataset: pheme\n",
      "Precision: 0.3658385093167702\n",
      "Recall: 0.4411985018726592\n",
      "F-score: 0.4\n",
      "Accuracy: 0.7249377334993773\n",
      "G-mean: 0.4017554010064904\n",
      "\n",
      "Dataset: politifact_dataset\n",
      "Precision: 0.6056573449051752\n",
      "Recall: 0.8011054421768707\n",
      "F-score: 0.6898041369211058\n",
      "Accuracy: 0.5994232223903178\n",
      "G-mean: 0.6965596852373312\n",
      "\n",
      "Dataset: climate_dataset\n",
      "Precision: 0.36947791164658633\n",
      "Recall: 0.36363636363636365\n",
      "F-score: 0.3665338645418326\n",
      "Accuracy: 0.649393605292172\n",
      "G-mean: 0.36654550090694377\n",
      "\n",
      "Dataset: isot_multipurpose_small\n",
      "Precision: 0.5478260869565217\n",
      "Recall: 0.378\n",
      "F-score: 0.44733727810650886\n",
      "Accuracy: 0.533\n",
      "G-mean: 0.4550585246642076\n",
      "\n",
      "Dataset: gossipcop\n",
      "Precision: 0.4402104182704645\n",
      "Recall: 0.6445613375915837\n",
      "F-score: 0.5231379126324617\n",
      "Accuracy: 0.717479674796748\n",
      "G-mean: 0.5326749628264511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the metrics for each dataset\n",
    "baseline_dataset_metrics = defaultdict(list)\n",
    "for dataset in data['dataset'].unique():\n",
    "    dataset_results = baseline_results[baseline_results['dataset'] == dataset]\n",
    "    detection_prediction = precision_score(dataset_results['label'], dataset_results['prediction'])\n",
    "    detection_recall = recall_score(dataset_results['label'], dataset_results['prediction'])\n",
    "    detection_f1 = f1_score(dataset_results['label'], dataset_results['prediction'])\n",
    "    detection_accuracy = accuracy_score(dataset_results['label'], dataset_results['prediction'])\n",
    "    detection_gmean = np.sqrt(detection_prediction * detection_recall)\n",
    "\n",
    "    baseline_dataset_metrics[dataset].append({\n",
    "        'precision': detection_prediction,\n",
    "        'recall': detection_recall,\n",
    "        'f1': detection_f1,\n",
    "        'accuracy': detection_accuracy,\n",
    "        'gmean': detection_gmean\n",
    "    })\n",
    "\n",
    "    print(f'Dataset: {dataset}')\n",
    "    print(f'Precision: {detection_prediction}')\n",
    "    print(f'Recall: {detection_recall}')\n",
    "    print(f'F-score: {detection_f1}')\n",
    "    print(f'Accuracy: {detection_accuracy}')\n",
    "    print(f'G-mean: {detection_gmean}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel sheet generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics_worksheet(workbook, sheet, sheet_name, category_metrics, dataset_metrics, \n",
    "                              prediction, recall, f1, accuracy, gmean):\n",
    "    category_rows = len(category_detection_metrics)\n",
    "    dataset_rows = len(dataset_detection_metrics)\n",
    " # add the metrics to the sheet\n",
    "    sheet.write('A1', 'Category')\n",
    "    sheet.write('B1', 'Precision')\n",
    "    sheet.write('C1', 'Recall')\n",
    "    sheet.write('D1', 'F1')\n",
    "    sheet.write('E1', 'Accuracy')\n",
    "    sheet.write('F1', 'G-Mean')\n",
    "\n",
    "    # add the category metrics\n",
    "    row = 1\n",
    "    for category, metrics in category_metrics.items():\n",
    "        metrics = metrics[0]\n",
    "        sheet.write(row, 0, category)\n",
    "        sheet.write(row, 1, metrics['precision'])\n",
    "        sheet.write(row, 2, metrics['recall'])\n",
    "        sheet.write(row, 3, metrics['f1'])\n",
    "        sheet.write(row, 4, metrics['accuracy'])\n",
    "        sheet.write(row, 5, metrics['gmean'])\n",
    "        row += 1\n",
    "\n",
    "    # add the dataset metrics\n",
    "    sheet.write('H1', 'Dataset')\n",
    "    sheet.write('I1', 'Precision')\n",
    "    sheet.write('J1', 'Recall')\n",
    "    sheet.write('K1', 'F1')\n",
    "    sheet.write('L1', 'Accuracy')\n",
    "    sheet.write('M1', 'G-Mean')\n",
    "    row = 1\n",
    "    for dataset, metrics in dataset_metrics.items():\n",
    "        metrics = metrics[0]\n",
    "        sheet.write(row, 7, dataset)\n",
    "        sheet.write(row, 8, metrics['precision'])\n",
    "        sheet.write(row, 9, metrics['recall'])\n",
    "        sheet.write(row, 10, metrics['f1'])\n",
    "        sheet.write(row, 11, metrics['accuracy'])\n",
    "        sheet.write(row, 12, metrics['gmean'])\n",
    "        row += 1\n",
    "\n",
    "    # create a chart for the category metrics\n",
    "    category_metrics_chart = workbook.add_chart({'type': 'column'})\n",
    "    \n",
    "    # add the category metrics to the chart\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Precision',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={sheet_name}!$B$2:$B${category_rows + 1}'\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Recall',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={sheet_name}!$C$2:$C${category_rows + 1}'\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'F1',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={sheet_name}!$D$2:$D${category_rows + 1}'\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Accuracy',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={sheet_name}!$E$2:$E${category_rows + 1}'\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'G-Mean',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={sheet_name}!$F$2:$F${category_rows + 1}'\n",
    "    })\n",
    "\n",
    "    # between 0 and 1\n",
    "    category_metrics_chart.set_y_axis({\n",
    "        'min': 0,\n",
    "        'max': 1\n",
    "    })\n",
    "\n",
    "    # add the chart to the sheet\n",
    "    sheet.insert_chart('A10', category_metrics_chart)\n",
    "\n",
    "    # create a chart for the dataset metrics\n",
    "    dataset_metrics_chart = workbook.add_chart({'type': 'column'})\n",
    "\n",
    "    # add the dataset metrics to the chart\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Precision',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={sheet_name}!$I$2:$I${dataset_rows + 1}'\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Recall',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={sheet_name}!$J$2:$J${dataset_rows + 1}'\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'F1',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={sheet_name}!$K$2:$K${dataset_rows + 1}'\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Accuracy',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={sheet_name}!$L$2:$L${dataset_rows + 1}'\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'G-Mean',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={sheet_name}!$M$2:$M${dataset_rows + 1}'\n",
    "    })\n",
    "\n",
    "    # between 0 and 1\n",
    "    dataset_metrics_chart.set_y_axis({\n",
    "        'min': 0,\n",
    "        'max': 1\n",
    "    })\n",
    "\n",
    "    # add the chart to the sheet\n",
    "    sheet.insert_chart('H16', dataset_metrics_chart)\n",
    "\n",
    "    # add the overall metrics\n",
    "    sheet.write('H32', 'Overall')\n",
    "    sheet.write('I32', prediction)\n",
    "    sheet.write('J32', recall)\n",
    "    sheet.write('K32', f1)\n",
    "    sheet.write('L32', accuracy)\n",
    "    sheet.write('M32', gmean)\n",
    "\n",
    "    # create a chart for the overall metrics\n",
    "    overall_metrics_chart = workbook.add_chart({'type': 'column'})\n",
    "\n",
    "    # add the overall metrics to the chart\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Precision',\n",
    "        'categories': f'={sheet_name}!$H$32',\n",
    "        'values': f'={sheet_name}!$I$32'\n",
    "    })\n",
    "\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Recall',\n",
    "        'categories': f'={sheet_name}!$H$32',\n",
    "        'values': f'={sheet_name}!$J$32'\n",
    "    })\n",
    "\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'F1',\n",
    "        'categories': f'={sheet_name}!$H$32',\n",
    "        'values': f'={sheet_name}!$K$32'\n",
    "    })\n",
    "\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Accuracy',\n",
    "        'categories': f'={sheet_name}!$H$32',\n",
    "        'values': f'={sheet_name}!$L$32'\n",
    "    })\n",
    "\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'G-Mean',\n",
    "        'categories': f'={sheet_name}!$H$32',\n",
    "        'values': f'={sheet_name}!$M$32'\n",
    "    })\n",
    "\n",
    "    # between 0 and 1\n",
    "    overall_metrics_chart.set_y_axis({\n",
    "        'min': 0,\n",
    "        'max': 1\n",
    "    })\n",
    "\n",
    "\n",
    "    # add the chart to the sheet\n",
    "    sheet.insert_chart('A25', overall_metrics_chart)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_metrics_charts(workbook, sheet, metrics_sheet_name, baseline_sheet_name):\n",
    "    category_metrics_charts(workbook, sheet, metrics_sheet_name, baseline_sheet_name)\n",
    "    dataset_metrics_charts(workbook, sheet, metrics_sheet_name, baseline_sheet_name)\n",
    "    overall_metrics_charts(workbook, sheet, metrics_sheet_name, baseline_sheet_name)\n",
    "\n",
    "def category_metrics_charts(workbook, sheet, metrics_sheet_name, baseline_sheet_name):\n",
    "    category_rows = len(category_detection_metrics)\n",
    "    # create a chart for the category metrics\n",
    "    category_metrics_chart = workbook.add_chart({'type': 'column'})\n",
    "    \n",
    "    # add the category metrics to the chart, with series from both the metrics sheet and the baseline sheet. make ones of the same metric the same color (use material design colors)\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Precision',\n",
    "        'categories': f'={metrics_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$B$2:$B${category_rows + 1}',\n",
    "        'fill': {'color': '#2196f3'}\n",
    "    })\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Baseline Precision',\n",
    "        'categories': f'={baseline_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$B$2:$B${category_rows + 1}',\n",
    "        'fill': {'color': '#2196f3', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Recall',\n",
    "        'categories': f'={metrics_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$C$2:$C${category_rows + 1}',\n",
    "        'fill': {'color': '#f44336'}\n",
    "    })\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Baseline Recall',\n",
    "        'categories': f'={baseline_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$C$2:$C${category_rows + 1}',\n",
    "        'fill': {'color': '#f44336', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'F1',\n",
    "        'categories': f'={metrics_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$D$2:$D${category_rows + 1}',\n",
    "        'fill': {'color': '#4caf50'}\n",
    "    })\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Baseline F1',\n",
    "        'categories': f'={baseline_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$D$2:$D${category_rows + 1}',\n",
    "        'fill': {'color': '#4caf50', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Accuracy',\n",
    "        'categories': f'={metrics_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$E$2:$E${category_rows + 1}',\n",
    "        'fill': {'color': '#9c27b0'}\n",
    "    })\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Baseline Accuracy',\n",
    "        'categories': f'={baseline_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$E$2:$E${category_rows + 1}',\n",
    "        'fill': {'color': '#9c27b0', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'G-Mean',\n",
    "        'categories': f'={metrics_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$F$2:$F${category_rows + 1}',\n",
    "        'fill': {'color': '#009688'}\n",
    "    })\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Baseline G-Mean',\n",
    "        'categories': f'={baseline_sheet_name}!$A$2:$A${category_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$F$2:$F${category_rows + 1}',\n",
    "        'fill': {'color': '#009688', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    # make chart bigger\n",
    "    category_metrics_chart.set_size({'width': 900, 'height': 450})\n",
    "\n",
    "    # make range between 0 and 1\n",
    "    category_metrics_chart.set_y_axis({'min': 0, 'max': 1})\n",
    "\n",
    "    # add the chart to the sheet\n",
    "    sheet.insert_chart('B2', category_metrics_chart)\n",
    "\n",
    "    \n",
    "def dataset_metrics_charts(workbook, sheet, metrics_sheet_name, baseline_sheet_name):\n",
    "    dataset_rows = len(dataset_detection_metrics)\n",
    "    # create a chart for the dataset metrics\n",
    "    dataset_metrics_chart = workbook.add_chart({'type': 'column'})\n",
    "\n",
    "    # add the dataset metrics to the chart, with series from both the metrics sheet and the baseline sheet. make ones of the same metric the same color (use material design colors)\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Precision',\n",
    "        'categories': f'={metrics_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$I$2:$I${dataset_rows + 1}',\n",
    "        'fill': {'color': '#2196f3'}\n",
    "    })\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Baseline Precision',\n",
    "        'categories': f'={baseline_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$I$2:$I${dataset_rows + 1}',\n",
    "        'fill': {'color': '#2196f3', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Recall',\n",
    "        'categories': f'={metrics_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$J$2:$J${dataset_rows + 1}',\n",
    "        'fill': {'color': '#f44336'}\n",
    "    })\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Baseline Recall',\n",
    "        'categories': f'={baseline_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$J$2:$J${dataset_rows + 1}',\n",
    "        'fill': {'color': '#f44336', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'F1',\n",
    "        'categories': f'={metrics_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$K$2:$K${dataset_rows + 1}',\n",
    "        'fill': {'color': '#4caf50'}\n",
    "    })\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Baseline F1',\n",
    "        'categories': f'={baseline_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$K$2:$K${dataset_rows + 1}',\n",
    "        'fill': {'color': '#4caf50', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Accuracy',\n",
    "        'categories': f'={metrics_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$L$2:$L${dataset_rows + 1}',\n",
    "        'fill': {'color': '#9c27b0'}\n",
    "    })\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Baseline Accuracy',\n",
    "        'categories': f'={baseline_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$L$2:$L${dataset_rows + 1}',\n",
    "        'fill': {'color': '#9c27b0', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'G-Mean',\n",
    "        'categories': f'={metrics_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={metrics_sheet_name}!$M$2:$M${dataset_rows + 1}',\n",
    "        'fill': {'color': '#009688'}\n",
    "    })\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Baseline G-Mean',\n",
    "        'categories': f'={baseline_sheet_name}!$H$2:$H${dataset_rows + 1}',\n",
    "        'values': f'={baseline_sheet_name}!$M$2:$M${dataset_rows + 1}',\n",
    "        'fill': {'color': '#009688', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    # make chart bigger\n",
    "    dataset_metrics_chart.set_size({'width': 900, 'height': 450})\n",
    "\n",
    "    # make range between 0 and 1\n",
    "    dataset_metrics_chart.set_y_axis({'min': 0, 'max': 1})\n",
    "\n",
    "    # add the chart to the sheet\n",
    "    sheet.insert_chart('B25', dataset_metrics_chart)\n",
    "\n",
    "\n",
    "def overall_metrics_charts(workbook, sheet, metrics_sheet_name, baseline_sheet_name):\n",
    "    # create a chart for the overall metrics\n",
    "    overall_metrics_chart = workbook.add_chart({'type': 'column'})\n",
    "\n",
    "    # add the overall metrics to the chart, with series from both the metrics sheet and the baseline sheet. make ones of the same metric the same color (use material design colors)\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Precision',\n",
    "        'categories': f'={metrics_sheet_name}!$H$32',\n",
    "        'values': f'={metrics_sheet_name}!$I$32',\n",
    "        'fill': {'color': '#2196f3'}\n",
    "    })\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Baseline Precision',\n",
    "        'categories': f'={baseline_sheet_name}!$H$32',\n",
    "        'values': f'={baseline_sheet_name}!$I$32',\n",
    "        'fill': {'color': '#2196f3', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Recall',\n",
    "        'categories': f'={metrics_sheet_name}!$H$32',\n",
    "        'values': f'={metrics_sheet_name}!$J$32',\n",
    "        'fill': {'color': '#f44336'}\n",
    "    })\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Baseline Recall',\n",
    "        'categories': f'={baseline_sheet_name}!$H$32',\n",
    "        'values': f'={baseline_sheet_name}!$J$32',\n",
    "        'fill': {'color': '#f44336', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'F1',\n",
    "        'categories': f'={metrics_sheet_name}!$H$32',\n",
    "        'values': f'={metrics_sheet_name}!$K$32',\n",
    "        'fill': {'color': '#4caf50'}\n",
    "    })\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Baseline F1',\n",
    "        'categories': f'={baseline_sheet_name}!$H$32',\n",
    "        'values': f'={baseline_sheet_name}!$K$32',\n",
    "        'fill': {'color': '#4caf50', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Accuracy',\n",
    "        'categories': f'={metrics_sheet_name}!$H$32',\n",
    "        'values': f'={metrics_sheet_name}!$L$32',\n",
    "        'fill': {'color': '#9c27b0'}\n",
    "    })\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Baseline Accuracy',\n",
    "        'categories': f'={baseline_sheet_name}!$H$32',\n",
    "        'values': f'={baseline_sheet_name}!$L$32',\n",
    "        'fill': {'color': '#9c27b0', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'G-Mean',\n",
    "        'categories': f'={metrics_sheet_name}!$H$32',\n",
    "        'values': f'={metrics_sheet_name}!$M$32',\n",
    "        'fill': {'color': '#009688'}\n",
    "    })\n",
    "    overall_metrics_chart.add_series({\n",
    "        'name': 'Baseline G-Mean',\n",
    "        'categories': f'={baseline_sheet_name}!$H$32',\n",
    "        'values': f'={baseline_sheet_name}!$M$32',\n",
    "        'fill': {'color': '#009688', 'transparency': 50}\n",
    "    })\n",
    "\n",
    "    # make chart bigger\n",
    "    overall_metrics_chart.set_size({'width': 900, 'height': 450})\n",
    "\n",
    "    # make range between 0 and 1\n",
    "    overall_metrics_chart.set_y_axis({'min': 0, 'max': 1})\n",
    "\n",
    "    # add the chart to the sheet\n",
    "    sheet.insert_chart('Q2', overall_metrics_chart)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_column_widths(sheet):\n",
    "    sheet.set_column('A:A', 20)\n",
    "    sheet.set_column('B:B', 12)\n",
    "    sheet.set_column('C:C', 12)\n",
    "    sheet.set_column('D:D', 12)\n",
    "    sheet.set_column('E:E', 12)\n",
    "    sheet.set_column('F:F', 12)\n",
    "    sheet.set_column('H:H', 20)\n",
    "    sheet.set_column('I:I', 12)\n",
    "    sheet.set_column('J:J', 12)\n",
    "    sheet.set_column('K:K', 12)\n",
    "    sheet.set_column('L:L', 12)\n",
    "    sheet.set_column('M:M', 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_detection_charts(workbook, sheet, sheet_name, category_detection_metrics, dataset_detection_metrics, prediction, recall, f1, accuracy, gmean):\n",
    "    category_rows = len(category_detection_metrics)\n",
    "    dataset_rows = len(dataset_detection_metrics)\n",
    "    # add the metrics to the sheet\n",
    "    sheet.write('A1', 'Category')\n",
    "    sheet.write('B1', 'Precision')\n",
    "    sheet.write('C1', 'Recall')\n",
    "    sheet.write('D1', 'F1')\n",
    "    sheet.write('E1', 'Accuracy')\n",
    "    sheet.write('F1', 'G-Mean')\n",
    "\n",
    "    # add the category metrics\n",
    "    row = 1\n",
    "    for category, metrics in category_detection_metrics.items():\n",
    "        sheet.write(row, 0, category)\n",
    "        sheet.write(row, 1, metrics['precision'])\n",
    "        sheet.write(row, 2, metrics['recall'])\n",
    "        sheet.write(row, 3, metrics['f1'])\n",
    "        sheet.write(row, 4, metrics['accuracy'])\n",
    "        sheet.write(row, 5, metrics['gmean'])\n",
    "        row += 1\n",
    "\n",
    "    # add the dataset metrics\n",
    "    sheet.write('H1', 'Dataset')\n",
    "    sheet.write('I1', 'Precision')\n",
    "    sheet.write('J1', 'Recall')\n",
    "    sheet.write('K1', 'F1')\n",
    "    sheet.write('L1', 'Accuracy')\n",
    "    sheet.write('M1', 'G-Mean')\n",
    "    row = 1\n",
    "    for dataset, metrics in dataset_detection_metrics.items():\n",
    "        sheet.write(row, 7, dataset)\n",
    "        sheet.write(row, 8, metrics['precision'])\n",
    "        sheet.write(row, 9, metrics['recall'])\n",
    "        sheet.write(row, 10, metrics['f1'])\n",
    "        sheet.write(row, 11, metrics['accuracy'])\n",
    "        sheet.write(row, 12, metrics['gmean'])\n",
    "        row += 1\n",
    "\n",
    "    # add the overall metrics\n",
    "    sheet.write('A8', 'Overall')\n",
    "    sheet.write('B8', prediction)\n",
    "    sheet.write('C8', recall)\n",
    "    sheet.write('D8', f1)\n",
    "    sheet.write('E8', accuracy)\n",
    "    sheet.write('F8', gmean)\n",
    "\n",
    "    # create a chart for the category metrics\n",
    "    category_metrics_chart = workbook.add_chart({'type': 'column'})\n",
    "    \n",
    "    # add the category metrics to the chart\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Precision',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows+1}',\n",
    "        'values': f'={sheet_name}!$B$2:$B${category_rows+1}'\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Recall',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows+1}',\n",
    "        'values': f'={sheet_name}!$C$2:$C${category_rows+1}'\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'F1',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows+1}',\n",
    "        'values': f'={sheet_name}!$D$2:$D${category_rows+1}'\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'Accuracy',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows+1}',\n",
    "        'values': f'={sheet_name}!$E$2:$E${category_rows+1}'\n",
    "    })\n",
    "\n",
    "    category_metrics_chart.add_series({\n",
    "        'name': 'G-Mean',\n",
    "        'categories': f'={sheet_name}!$A$2:$A${category_rows+1}',\n",
    "        'values': f'={sheet_name}!$F$2:$F${category_rows+1}'\n",
    "    })\n",
    "\n",
    "    # make chart bigger\n",
    "    category_metrics_chart.set_size({'width': 600, 'height': 450})\n",
    "\n",
    "    # make the sheet between 0 and 1\n",
    "    category_metrics_chart.set_y_axis({'min': 0, 'max': 1})\n",
    "\n",
    "    # add the chart to the sheet\n",
    "    sheet.insert_chart('A10', category_metrics_chart)\n",
    "\n",
    "    # create a chart for the dataset metrics\n",
    "    dataset_metrics_chart = workbook.add_chart({'type': 'column'})\n",
    "\n",
    "    # add the dataset metrics to the chart\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Precision',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows+1}',\n",
    "        'values': f'={sheet_name}!$I$2:$I${dataset_rows+1}'\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Recall',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows+1}',\n",
    "        'values': f'={sheet_name}!$J$2:$J${dataset_rows+1}'\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'F1',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows+1}',\n",
    "        'values': f'={sheet_name}!$K$2:$K${dataset_rows+1}'\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'Accuracy',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows+1}',\n",
    "        'values': f'={sheet_name}!$L$2:$L${dataset_rows+1}'\n",
    "    })\n",
    "\n",
    "    dataset_metrics_chart.add_series({\n",
    "        'name': 'G-Mean',\n",
    "        'categories': f'={sheet_name}!$H$2:$H${dataset_rows+1}',\n",
    "        'values': f'={sheet_name}!$M$2:$M${dataset_rows+1}'\n",
    "    })\n",
    "\n",
    "    # make chart bigger\n",
    "    dataset_metrics_chart.set_size({'width': 600, 'height': 450})\n",
    "\n",
    "    # make the sheet between 0 and 1\n",
    "    dataset_metrics_chart.set_y_axis({'min': 0, 'max': 1})\n",
    "\n",
    "    # add the chart to the sheet\n",
    "    sheet.insert_chart('A33', dataset_metrics_chart)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results to sheets in an excel file\n",
    "with pd.ExcelWriter('results.xlsx', engine='xlsxwriter') as writer:\n",
    "    results.to_excel(writer, index=False, sheet_name='Domain specific results')\n",
    "    baseline_results.to_excel(writer, index=False, sheet_name='Baseline results')\n",
    "\n",
    "    workbook = writer.book\n",
    "\n",
    "    # create a sheet for the metrics graphs\n",
    "    metrics_sheet = workbook.add_worksheet('Domain_Specific_Metrics')\n",
    "\n",
    "    generate_metrics_worksheet(workbook, metrics_sheet, 'Domain_Specific_Metrics', overall_category_metrics, overall_dataset_metrics, \n",
    "                              overall_prediction, overall_recall, overall_f1, overall_accuracy, overall_gmean)\n",
    "    set_column_widths(metrics_sheet)\n",
    "\n",
    "    \n",
    "    # create a sheet for the baseline metrics graphs\n",
    "    baseline_metrics_sheet = workbook.add_worksheet('Baseline_metrics')\n",
    "\n",
    "    generate_metrics_worksheet(workbook, baseline_metrics_sheet, 'Baseline_metrics', baseline_category_metrics, baseline_dataset_metrics,\n",
    "                                baseline_prediction, baseline_recall, baseline_f1, baseline_accuracy, baseline_gmean)\n",
    "    set_column_widths(baseline_metrics_sheet)\n",
    "\n",
    "    # create a sheet for comparing the metrics\n",
    "    compare_metrics_sheet = workbook.add_worksheet('Compare_metrics')\n",
    "\n",
    "    # add charts for the category metrics\n",
    "    comparison_metrics_charts(workbook, compare_metrics_sheet, 'Domain_Specific_Metrics', 'Baseline_metrics')\n",
    "\n",
    "    # create a sheet for the category detection metrics\n",
    "    category_detection_sheet = workbook.add_worksheet('Category_detection')\n",
    "\n",
    "    category_detection_charts(workbook, category_detection_sheet, 'Category_detection', category_detection_metrics, dataset_detection_metrics,\n",
    "                                category_detection_prediction, category_detection_recall, category_detection_f1, category_detection_accuracy, category_detection_gmean)\n",
    "    \n",
    "    set_column_widths(category_detection_sheet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
